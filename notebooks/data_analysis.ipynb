{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 9886468,
          "sourceType": "datasetVersion",
          "datasetId": 6071248
        }
      ],
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!ls\n",
        "# Change to the cloned repo directory (optional)\n",
        "!git clone \"https://github.com/ezragershman/spotify-listening-data\"\n",
        "%cd '/content/spotify-listening-data'\n",
        "\n",
        "for dirname, _, filenames in os.walk('/content/spotify-listening-data'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T16:37:44.049622Z",
          "iopub.execute_input": "2024-11-12T16:37:44.050110Z",
          "iopub.status.idle": "2024-11-12T16:37:44.082224Z",
          "shell.execute_reply.started": "2024-11-12T16:37:44.050063Z",
          "shell.execute_reply": "2024-11-12T16:37:44.080893Z"
        },
        "id": "tR7kAO397UKR",
        "outputId": "ae81859b-282d-42ec-b419-10d75d2c12de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  notebooks  README.md\n",
            "Cloning into 'spotify-listening-data'...\n",
            "remote: Enumerating objects: 150, done.\u001b[K\n",
            "remote: Counting objects: 100% (150/150), done.\u001b[K\n",
            "remote: Compressing objects: 100% (121/121), done.\u001b[K\n",
            "remote: Total 150 (delta 43), reused 37 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (150/150), 11.04 MiB | 12.99 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n",
            "/content/spotify-listening-data\n",
            "/content/spotify-listening-data/README.md\n",
            "/content/spotify-listening-data/notebooks/data_analysis.ipynb\n",
            "/content/spotify-listening-data/data/raw/READMEplaceholder.md\n",
            "/content/spotify-listening-data/data/raw/Streaming_History_Audio_2013-2024.json\n",
            "/content/spotify-listening-data/data/processed/spotify_data.csv\n",
            "/content/spotify-listening-data/data/processed/ipads processing/geolocated_ips.csv\n",
            "/content/spotify-listening-data/data/processed/ipads processing/unique_ips.csv\n",
            "/content/spotify-listening-data/data/processed/ipads processing/deduplicated_unique_ips.csv\n",
            "/content/spotify-listening-data/.git/HEAD\n",
            "/content/spotify-listening-data/.git/description\n",
            "/content/spotify-listening-data/.git/FETCH_HEAD\n",
            "/content/spotify-listening-data/.git/packed-refs\n",
            "/content/spotify-listening-data/.git/index\n",
            "/content/spotify-listening-data/.git/ORIG_HEAD\n",
            "/content/spotify-listening-data/.git/config\n",
            "/content/spotify-listening-data/.git/COMMIT_EDITMSG\n",
            "/content/spotify-listening-data/.git/objects/5f/43b0fd2e989e4256e81860092eac3b14418397\n",
            "/content/spotify-listening-data/.git/objects/pack/pack-8e7f307334daaa5161ba7d8a2047d395eb3fa66c.pack\n",
            "/content/spotify-listening-data/.git/objects/pack/pack-8e7f307334daaa5161ba7d8a2047d395eb3fa66c.idx\n",
            "/content/spotify-listening-data/.git/objects/e6/648160b0d1d9cc2510028c0be976cb9685cdad\n",
            "/content/spotify-listening-data/.git/objects/31/6c9d833bf1315d73c6a1c31224902910519316\n",
            "/content/spotify-listening-data/.git/objects/cc/1946125dcb90856d1980e107f4fc15b4169d88\n",
            "/content/spotify-listening-data/.git/objects/3a/2323715a7d6ca0866c5fff53354790ef44afc1\n",
            "/content/spotify-listening-data/.git/refs/remotes/origin/HEAD\n",
            "/content/spotify-listening-data/.git/refs/remotes/origin/main\n",
            "/content/spotify-listening-data/.git/refs/heads/main\n",
            "/content/spotify-listening-data/.git/logs/HEAD\n",
            "/content/spotify-listening-data/.git/logs/refs/remotes/origin/HEAD\n",
            "/content/spotify-listening-data/.git/logs/refs/remotes/origin/main\n",
            "/content/spotify-listening-data/.git/logs/refs/heads/main\n",
            "/content/spotify-listening-data/.git/hooks/fsmonitor-watchman.sample\n",
            "/content/spotify-listening-data/.git/hooks/push-to-checkout.sample\n",
            "/content/spotify-listening-data/.git/hooks/update.sample\n",
            "/content/spotify-listening-data/.git/hooks/pre-push.sample\n",
            "/content/spotify-listening-data/.git/hooks/commit-msg.sample\n",
            "/content/spotify-listening-data/.git/hooks/pre-receive.sample\n",
            "/content/spotify-listening-data/.git/hooks/pre-applypatch.sample\n",
            "/content/spotify-listening-data/.git/hooks/pre-merge-commit.sample\n",
            "/content/spotify-listening-data/.git/hooks/applypatch-msg.sample\n",
            "/content/spotify-listening-data/.git/hooks/prepare-commit-msg.sample\n",
            "/content/spotify-listening-data/.git/hooks/pre-rebase.sample\n",
            "/content/spotify-listening-data/.git/hooks/post-update.sample\n",
            "/content/spotify-listening-data/.git/hooks/pre-commit.sample\n",
            "/content/spotify-listening-data/.git/info/exclude\n",
            "/content/spotify-listening-data/spotify-listening-data/README.md\n",
            "/content/spotify-listening-data/spotify-listening-data/notebooks/data_analysis.ipynb\n",
            "/content/spotify-listening-data/spotify-listening-data/data/raw/READMEplaceholder.md\n",
            "/content/spotify-listening-data/spotify-listening-data/data/raw/Streaming_History_Audio_2013-2024.json\n",
            "/content/spotify-listening-data/spotify-listening-data/data/processed/geolocated_ips.csv\n",
            "/content/spotify-listening-data/spotify-listening-data/data/processed/unique_ips.csv\n",
            "/content/spotify-listening-data/spotify-listening-data/data/processed/spotify_data.csv\n",
            "/content/spotify-listening-data/spotify-listening-data/data/processed/deduplicated_unique_ips.csv\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/HEAD\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/description\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/packed-refs\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/index\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/config\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/objects/pack/pack-2986eac50826586e54ba31f6e385eb682f88c010.pack\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/objects/pack/pack-2986eac50826586e54ba31f6e385eb682f88c010.idx\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/refs/remotes/origin/HEAD\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/refs/heads/main\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/logs/HEAD\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/logs/refs/remotes/origin/HEAD\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/logs/refs/heads/main\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/hooks/fsmonitor-watchman.sample\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/hooks/push-to-checkout.sample\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/hooks/update.sample\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/hooks/pre-push.sample\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/hooks/commit-msg.sample\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/hooks/pre-receive.sample\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/hooks/pre-applypatch.sample\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/hooks/pre-merge-commit.sample\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/hooks/applypatch-msg.sample\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/hooks/prepare-commit-msg.sample\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/hooks/pre-rebase.sample\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/hooks/post-update.sample\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/hooks/pre-commit.sample\n",
            "/content/spotify-listening-data/spotify-listening-data/.git/info/exclude\n"
          ]
        }
      ],
      "execution_count": 27
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull --rebase"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocT3SHlA7F3R",
        "outputId": "9f47d1fc-11e3-4075-c48e-c2f9727da55c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/spotify-listening-data'\n",
        "from google.colab import userdata\n",
        "github_token = userdata.get('github_token')\n",
        "\n",
        "!git config --global user.email \"ezragersh@gmail.com\"\n",
        "!git config --global user.name \"ezragershman\"\n",
        "# Set up Git to use the token for authentication\n",
        "if github_token:\n",
        "    repo_url = f\"https://{github_token}@github.com/ezragershman/spotify-listening-data.git\"  # Replace 'YourGitHubUsername' and 'YourRepository' with your actual GitHub username and repo\n",
        "    !git remote set-url origin {repo_url}\n",
        "    !git add *\n",
        "    !git commit -m \";\"\n",
        "    !git push origin main\n",
        "else:\n",
        "    print(\"GitHub token is not available.\")"
      ],
      "metadata": {
        "id": "HfdwU_o685D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin main"
      ],
      "metadata": {
        "id": "nNs61EY-g2fb",
        "outputId": "0bc36224-78c9-47da-efb4-dd35912b28d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enumerating objects: 3, done.\n",
            "Counting objects:  33% (1/3)\rCounting objects:  66% (2/3)\rCounting objects: 100% (3/3)\rCounting objects: 100% (3/3), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects:  50% (1/2)\rCompressing objects: 100% (2/2)\rCompressing objects: 100% (2/2), done.\n",
            "Writing objects:  50% (1/2)\rWriting objects: 100% (2/2)\rWriting objects: 100% (2/2), 240 bytes | 240.00 KiB/s, done.\n",
            "Total 2 (delta 1), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas:   0% (0/1)\u001b[K\rremote: Resolving deltas: 100% (1/1)\u001b[K\rremote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "To https://github.com/ezragershman/spotify-listening-data.git\n",
            "   ae6658d..0160046  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAIN FUNCTION TO UPDATE & SAVE MUSIC LIBRARY DATABASE**\n"
      ],
      "metadata": {
        "id": "mDDlLipM7UKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from base64 import b64encode\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Step 1: Set up Spotify credentials as environment variables in Google Colab\n",
        "CLIENT_ID = userdata.get('spotify_client_id')\n",
        "CLIENT_SECRET = userdata.get('spotify_client_sec')\n",
        "\n",
        "# Step 2: Get Access Token from Spotify API using Client Credentials Flow\n",
        "def get_access_token(client_id, client_secret):\n",
        "    auth_header = b64encode(f\"{client_id}:{client_secret}\".encode('utf-8')).decode('utf-8')\n",
        "    headers = {\n",
        "        'Authorization': f'Basic {auth_header}',\n",
        "        'Content-Type': 'application/x-www-form-urlencoded'\n",
        "    }\n",
        "    data = {'grant_type': 'client_credentials'}\n",
        "\n",
        "    response = requests.post('https://accounts.spotify.com/api/token', headers=headers, data=data)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()['access_token']\n",
        "    else:\n",
        "        print(\"Failed to get access token\")\n",
        "        print(response.json())  # Added for better debugging\n",
        "        return None\n",
        "\n",
        "# Step 3: Search for a track by name and artist\n",
        "def search_track(track_name, artist_name, access_token):\n",
        "    headers = {'Authorization': f'Bearer {access_token}'}\n",
        "    query = f\"track:{track_name} artist:{artist_name}\"\n",
        "    params = {'q': query, 'type': 'track', 'limit': 1}\n",
        "\n",
        "    response = requests.get('https://api.spotify.com/v1/search', headers=headers, params=params)\n",
        "    if response.status_code == 200:\n",
        "          track = response.json()\n",
        "          return {\n",
        "              'album_type': track['album']['album_type'],\n",
        "              'track_uri': track['uri'],\n",
        "              'explicit': track['explicit'],\n",
        "              'available_markets': track['available_markets']\n",
        "          }\n",
        "        else:\n",
        "            print(\"No track found\")\n",
        "            return None\n",
        "    else:\n",
        "        print(\"Failed to search for track\")\n",
        "        print(response.json())  # Added for better debugging\n",
        "        return None\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "GKJ1oCcy7UKX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Example usage - search for \"Never Gonna Give You Up\" by Rick Astley\n",
        "access_token = get_access_token(CLIENT_ID, CLIENT_SECRET)\n",
        "if access_token:\n",
        "    track_info = search_track(\"Never Gonna Give You Up\", \"Rick Astley\", access_token)\n",
        "    if track_info:\n",
        "        print(track_info)"
      ],
      "metadata": {
        "id": "a1g8MXq29ptb",
        "outputId": "40b81b31-af28-41cc-8e43-0d086364d204",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Track Name': 'Never Gonna Give You Up', 'Artist': 'Rick Astley', 'Album': 'Whenever You Need Somebody', 'Release Date': '1987-11-12', 'Duration (ms)': 213573, 'Popularity': 78, 'URI': 'spotify:track:4PTG3Z6ehGkBFwjybzWkR8'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAIN FUNCTION TO UPDATE & SAVE TO DATABASE**\n",
        "Set json_file_path to the name of the file you want to process."
      ],
      "metadata": {
        "id": "gjfLJz8F7UKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_file_path = \"/content/spotify-listening-data/data/processed/spotify_data.csv\"\n",
        "json_to_csv(json_file_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T15:40:07.289030Z",
          "iopub.execute_input": "2024-11-12T15:40:07.289607Z",
          "iopub.status.idle": "2024-11-12T15:40:07.580940Z",
          "shell.execute_reply.started": "2024-11-12T15:40:07.289557Z",
          "shell.execute_reply": "2024-11-12T15:40:07.579471Z"
        },
        "id": "DF0iBnKW7UKb",
        "outputId": "f57987d6-adbb-4334-8bb6-c03c80c2fca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected object or value",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-3a7b1cbf6623>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mjson_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/spotify-listening-data/data/processed/spotify_data.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjson_to_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-be1bbdf0d3cd>\u001b[0m in \u001b[0;36mjson_to_csv\u001b[0;34m(json_file_path, csv_output_path, geolocation_file)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mjson_to_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_output_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/spotify-listening-data/data/processed/spotify_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeolocation_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/spotify-listening-data/data/processed/geolocated_ips.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Load the new JSON data into a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Rename columns for consistency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m                         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m                     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype_backend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m                     return obj.convert_dtypes(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1403\u001b[0;31m                 \u001b[0mujson_loads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m             )\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected object or value"
          ]
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import uuid\n",
        "import os\n",
        "import requests\n",
        "\n",
        "def json_to_csv(json_file_path, csv_output_path='/content/spotify-listening-data/data/processed/spotify_data.csv', geolocation_file='/content/spotify-listening-data/data/processed/geolocated_ips.csv'):\n",
        "    # Load the new JSON data into a DataFrame\n",
        "    new_data = pd.read_json(json_file_path)\n",
        "\n",
        "    # Rename columns for consistency\n",
        "    new_data.rename(columns={\n",
        "        'ts': 'end_time_UTC',\n",
        "        'platform': 'platform',\n",
        "        'ms_played': 'ms_played',\n",
        "        'conn_country': 'connection_country',\n",
        "        'ip_addr_decrypted': 'ip_address',\n",
        "        'master_metadata_track_name': 'track_name',\n",
        "        'master_metadata_album_artist_name': 'album_artist_name',\n",
        "        'master_metadata_album_album_name': 'album_name',\n",
        "        'spotify_track_uri': 'track_uri',\n",
        "        'episode_name': 'episode_name',\n",
        "        'episode_show_name': 'episode_show_name',\n",
        "        'spotify_episode_uri': 'episode_uri',\n",
        "        'reason_start': 'start_reason',\n",
        "        'reason_end': 'end_reason',\n",
        "        'shuffle': 'shuffle',\n",
        "        'skipped': 'skipped',\n",
        "    }, inplace=True)\n",
        "\n",
        "    # Load the geolocation data into a DataFrame (assuming it contains the relevant information)\n",
        "    geolocated_ips_df = pd.read_csv(geolocation_file)\n",
        "\n",
        "    # Merge the new data with the geolocation data based on the 'ip_address'\n",
        "    # This will add columns like 'city', 'region', 'country', 'latitude', 'longitude', 'local_time_zone', and 'utc_offset'\n",
        "    new_data = new_data.merge(\n",
        "       geolocated_ips_df[['unique_ip_address', 'city', 'region', 'country', 'latitude', 'longitude', 'local_time_zone', 'utc_offset']],\n",
        "       left_on='ip_address',\n",
        "       right_on='unique_ip_address',\n",
        "       how='left'\n",
        "       )\n",
        "\n",
        "    # Generate a unique ID for each row\n",
        "    new_data['unique_id'] = new_data['end_time_UTC'].apply(lambda x: f\"{x}_{uuid.uuid4().hex[:8]}\")\n",
        "\n",
        "    # Keep only the relevant columns for the CSV\n",
        "    new_data = new_data[['unique_id', 'end_time_UTC', 'platform', 'ms_played', 'connection_country',\n",
        "                        'ip_address', 'track_name', 'album_artist_name', 'album_name',\n",
        "                        'track_uri', 'episode_name', 'episode_show_name', 'episode_uri',\n",
        "                        'start_reason', 'end_reason', 'shuffle', 'skipped',\n",
        "                        'city', 'region', 'country', 'latitude', 'longitude',\n",
        "                        'local_time_zone', 'utc_offset']]  # Include geolocation and timezone columns\n",
        "\n",
        "    # If the output CSV already exists, read it to check for duplicates\n",
        "    if os.path.exists(csv_output_path):\n",
        "        existing_data = pd.read_csv(csv_output_path)\n",
        "\n",
        "        # Identify and keep only new rows that are not already in the CSV\n",
        "        merged_data = pd.concat([existing_data, new_data]).drop_duplicates(subset=['unique_id'], keep='first')\n",
        "    else:\n",
        "        # If no existing CSV, just use new data\n",
        "        merged_data = new_data\n",
        "\n",
        "    # Ensure the 'processed' directory exists (create it if not)\n",
        "    os.makedirs(os.path.dirname(csv_output_path), exist_ok=True)\n",
        "\n",
        "    # Save updated data to CSV in the specified path\n",
        "    merged_data.to_csv(csv_output_path, index=False)\n",
        "\n",
        "    print(f\"Data successfully updated and saved to {csv_output_path}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T16:16:52.328701Z",
          "iopub.execute_input": "2024-11-12T16:16:52.329098Z",
          "iopub.status.idle": "2024-11-12T16:16:52.340326Z",
          "shell.execute_reply.started": "2024-11-12T16:16:52.329059Z",
          "shell.execute_reply": "2024-11-12T16:16:52.339023Z"
        },
        "id": "Ihlqqxdk7UKc"
      },
      "outputs": [],
      "execution_count": 23
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IP Address Information Scraper**"
      ],
      "metadata": {
        "id": "2ErJcgHNSx67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Create table for unique ID information.\n",
        "\n",
        "def extract_unique_ips(json_file_path, output_csv_path='/content/spotify-listening-data/data/processed/ipads processing/unique_ips.csv'):\n",
        "    # Load the JSON data into a DataFrame\n",
        "    data = pd.read_json(json_file_path)\n",
        "\n",
        "    # Extract the IP address column (adjust column name based on your data)\n",
        "    ip_addresses = data['ip_addr_decrypted']  # Ensure 'ip_addr_decrypted' is the correct column name\n",
        "\n",
        "    # Get the unique IP addresses\n",
        "    unique_ips = ip_addresses.dropna().unique()\n",
        "\n",
        "    # Create a DataFrame from the unique IP addresses\n",
        "    unique_ips_df = pd.DataFrame(unique_ips, columns=['unique_ip_address'])\n",
        "\n",
        "    # Save the unique IPs to a CSV file\n",
        "    unique_ips_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "    print(f\"Unique IP addresses extracted and saved to {output_csv_path}\")\n",
        "\n",
        "# Example usage\n",
        "json_file_path = '/content/spotify-listening-data/data/raw/Streaming_History_Audio_2013-2024.json'\n",
        "extract_unique_ips(json_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MYkRh1V73qy",
        "outputId": "04bc4596-c796-4eef-bbff-64844f944c14"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique IP addresses extracted and saved to /content/spotify-listening-data/data/processed/unique_ips.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from ipaddress import ip_network, ip_address\n",
        "\n",
        "# Load IP data into DataFrame (assuming your list is saved as 'unique_ips.csv')\n",
        "df = pd.read_csv('/content/spotify-listening-data/data/processed/ipads processing/unique_ips.csv')\n",
        "\n",
        "def group_by_subnet(ip):\n",
        "    try:\n",
        "        # If it's an IPv4 address\n",
        "        ip_obj = ip_address(ip)\n",
        "        if ip_obj.version == 4:\n",
        "            return f\"{ip_obj.exploded.split('.')[0]}.{ip_obj.exploded.split('.')[1]}.{ip_obj.exploded.split('.')[2]}.0/24\"\n",
        "        else:\n",
        "            # For IPv6, return the first few blocks (customizable)\n",
        "            return ':'.join(ip.split(':')[:4]) + '::/64'\n",
        "    except ValueError:\n",
        "        return 'Invalid IP'\n",
        "\n",
        "# Apply subnet grouping\n",
        "df['subnet'] = df['unique_ip_address'].apply(group_by_subnet)\n",
        "\n",
        "# Drop duplicates based on subnets, keeping one representative IP\n",
        "deduplicated_ip = df.drop_duplicates(subset='subnet')\n",
        "\n",
        "# Save the deduplicated DataFrame as a CSV file\n",
        "deduplicated_ip.to_csv('/content/spotify-listening-data/data/processed/ipads processing/deduplicated_unique_ips.csv', index=False)"
      ],
      "metadata": {
        "id": "g3pEI2SBA-lr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from ip2geotools.databases.noncommercial import DbIpCity\n",
        "\n",
        "# Load previously processed IPs from CSV if it exists\n",
        "def load_processed_ips(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Loaded {len(df)} previously processed IPs from {file_path}.\")\n",
        "        return set(df['unique_ip_address'].tolist())  # Assuming the CSV has a column named 'unique_ip_address'\n",
        "    except FileNotFoundError:\n",
        "        print(f\"No previous geolocated IPs found. Starting fresh.\")\n",
        "        return set()\n",
        "\n",
        "# Save new processed IPs to the CSV\n",
        "def save_processed_ips(file_path, processed_ips):\n",
        "    df = pd.DataFrame(list(processed_ips), columns=['unique_ip_address'])\n",
        "    df.to_csv(file_path, index=False)\n",
        "    print(f\"Saved {len(processed_ips)} processed IPs to {file_path}.\")\n",
        "\n",
        "# Path to the CSV files\n",
        "deduplicated_ips_file = '/content/spotify-listening-data/data/processed/ipads processing/deduplicated_unique_ips.csv'\n",
        "geolocated_ips_file = '/content/spotify-listening-data/data/processed/ipads processing/geolocated_ips.csv'\n",
        "\n",
        "# Load the IPs\n",
        "print(\"Loading deduplicated IPs...\")\n",
        "deduplicated_df = pd.read_csv(deduplicated_ips_file)\n",
        "print(f\"Loaded {len(deduplicated_df)} deduplicated IPs from {deduplicated_ips_file}.\")\n",
        "\n",
        "processed_ips = load_processed_ips(geolocated_ips_file)\n",
        "\n",
        "# Function to get location details from IP address\n",
        "def get_location_from_ip(ip_address):\n",
        "    try:\n",
        "        # Query the ip2geotools database with the given IP address\n",
        "        res = DbIpCity.get(ip_address, api_key=\"free\")\n",
        "\n",
        "        # Structure the location data\n",
        "        location = {\n",
        "            \"city\": res.city,\n",
        "            \"region\": res.region,\n",
        "            \"country\": res.country,\n",
        "            \"latitude\": res.latitude,\n",
        "            \"longitude\": res.longitude\n",
        "        }\n",
        "        return location\n",
        "    except Exception as e:\n",
        "        # Handle any potential errors\n",
        "        return {\n",
        "            \"city\": None,\n",
        "            \"region\": None,\n",
        "            \"country\": None,\n",
        "            \"latitude\": None,\n",
        "            \"longitude\": None,\n",
        "            \"error\": f\"Error: {str(e)}\"\n",
        "        }\n",
        "\n",
        "# Add columns to store location data if not already present\n",
        "if 'city' not in deduplicated_df.columns:\n",
        "    print(\"Adding location columns to deduplicated IP data...\")\n",
        "    deduplicated_df['city'] = None\n",
        "    deduplicated_df['region'] = None\n",
        "    deduplicated_df['country'] = None\n",
        "    deduplicated_df['latitude'] = None\n",
        "    deduplicated_df['longitude'] = None\n",
        "\n",
        "# Start processing IPs\n",
        "print(\"Starting IP geolocation processing...\")\n",
        "\n",
        "new_geolocated_data = []  # To store new geolocated IP data\n",
        "\n",
        "for idx, ip in enumerate(deduplicated_df['unique_ip_address']):\n",
        "    print(ip)\n",
        "    # Check if this IP has already been processed\n",
        "    if ip not in processed_ips:\n",
        "        # Get location data for unprocessed IPs\n",
        "        print(f\"Processing IP {idx + 1}/{len(deduplicated_df)}: {ip}\")\n",
        "        location_data = get_location_from_ip(ip)\n",
        "        deduplicated_df.at[idx, 'city'] = location_data['city']\n",
        "        deduplicated_df.at[idx, 'region'] = location_data['region']\n",
        "        deduplicated_df.at[idx, 'country'] = location_data['country']\n",
        "        deduplicated_df.at[idx, 'latitude'] = location_data['latitude']\n",
        "        deduplicated_df.at[idx, 'longitude'] = location_data['longitude']\n",
        "\n",
        "        # Add the IP to the processed list\n",
        "        processed_ips.add(ip)\n",
        "\n",
        "        # Store new geolocated data\n",
        "        new_geolocated_data.append({\n",
        "            \"unique_ip_address\": ip,\n",
        "            \"city\": location_data['city'],\n",
        "            \"region\": location_data['region'],\n",
        "            \"country\": location_data['country'],\n",
        "            \"latitude\": location_data['latitude'],\n",
        "            \"longitude\": location_data['longitude']\n",
        "        })\n",
        "\n",
        "        # Log the geolocation\n",
        "        print(f\"Processed {ip}: {location_data['city']}, {location_data['region']}, {location_data['country']}\")\n",
        "\n",
        "    # Wait 10 seconds to avoid hitting rate limits\n",
        "    time.sleep(10)\n",
        "\n",
        "# Save the updated deduplicated DataFrame back to the original CSV\n",
        "print(\"Saving updated deduplicated IPs to CSV...\")\n",
        "deduplicated_df.to_csv(deduplicated_ips_file, index=False)\n",
        "\n",
        "# Save the processed IPs to the geolocated file\n",
        "if new_geolocated_data:\n",
        "    new_geolocated_df = pd.DataFrame(new_geolocated_data)\n",
        "    new_geolocated_df.to_csv(geolocated_ips_file, index=False)\n",
        "\n",
        "# Save the processed IPs to the geolocated file\n",
        "save_processed_ips(geolocated_ips_file, processed_ips)\n",
        "\n",
        "print(\"Geolocation data added and saved successfully.\")\n"
      ],
      "metadata": {
        "id": "t4mBrzp4CbnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To remove the time zone and UTC offset columns, if needed\n",
        "geolocated_ips.drop(columns=['local_time_zone', 'utc_offset'], inplace=True)"
      ],
      "metadata": {
        "id": "5YJG1LgNZ081"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from geopy.geocoders import Nominatim\n",
        "from timezonefinder import TimezoneFinder\n",
        "from pytz import timezone\n",
        "from datetime import datetime\n",
        "\n",
        "# Function to get timezone and UTC offset for a given city and country\n",
        "def get_timezone_and_offset(city, country, region=None):\n",
        "    geolocator = Nominatim(user_agent=\"timezone_lookup\")\n",
        "\n",
        "    # First attempt: geocode using city and country\n",
        "    location = geolocator.geocode(f\"{city}, {country}\", timeout=10)\n",
        "\n",
        "    if location:\n",
        "        latitude = location.latitude\n",
        "        longitude = location.longitude\n",
        "        tz_finder = TimezoneFinder()\n",
        "        tz_name = tz_finder.timezone_at(lng=longitude, lat=latitude)\n",
        "\n",
        "        if tz_name:\n",
        "            tz = timezone(tz_name)\n",
        "            utc_offset = tz.utcoffset(datetime.now()).total_seconds() / 3600  # in hours\n",
        "            return tz_name, utc_offset\n",
        "        else:\n",
        "            return 'Unknown', None\n",
        "\n",
        "    else:\n",
        "        # Second attempt: geocode using region if city-based lookup fails\n",
        "        if region:\n",
        "            location = geolocator.geocode(f\"{region}, {country}\", timeout=10)\n",
        "\n",
        "            if location:\n",
        "                latitude = location.latitude\n",
        "                longitude = location.longitude\n",
        "                tz_finder = TimezoneFinder()\n",
        "                tz_name = tz_finder.timezone_at(lng=longitude, lat=latitude)\n",
        "\n",
        "                if tz_name:\n",
        "                    tz = timezone(tz_name)\n",
        "                    utc_offset = tz.utcoffset(datetime.now()).total_seconds() / 3600  # in hours\n",
        "                    return tz_name, utc_offset\n",
        "                else:\n",
        "                    return 'Unknown', None\n",
        "        return 'Unknown', None  # Return 'Unknown' if both lookups fail\n",
        "\n",
        "# Apply the function to your DataFrame\n",
        "def add_time_zone_info(df):\n",
        "    df[['local_time_zone', 'utc_offset']] = df.apply(\n",
        "        lambda row: pd.Series(get_timezone_and_offset(row['city'], row['country'], row['region'])),\n",
        "        axis=1\n",
        "    )\n",
        "    return df\n",
        "\n",
        "# Load the data\n",
        "geolocated_ips = pd.read_csv('/content/spotify-listening-data/data/processed/ipads processing/geolocated_ips.csv')\n",
        "\n",
        "# Apply the function to add time zone information\n",
        "geolocated_ips = add_time_zone_info(geolocated_ips)\n",
        "\n",
        "# Export the updated file\n",
        "geolocated_ips.to_csv('/content/spotify-listening-data/data/processed/ipads processing/geolocated_ips.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "7iVw3f_XcsXM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA ANALYSIS AND MANIPULATION**:"
      ],
      "metadata": {
        "id": "8uVMsLCyCxTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Spotify listening data\n",
        "spotify_data_path = '/content/spotify-listening-data/data/processed/spotify_data.csv'\n",
        "spotify_df = pd.read_csv(spotify_data_path)\n",
        "\n",
        "# Display the first few rows and summary of the data\n",
        "print(\"First few rows of the data:\")\n",
        "print(spotify_df.head())\n",
        "\n",
        "print(\"\\nData structure (columns and data types):\")\n",
        "print(spotify_df.info())\n",
        "\n",
        "print(\"\\nSummary statistics:\")\n",
        "print(spotify_df.describe(include='all'))"
      ],
      "metadata": {
        "id": "OId5bfZtC-Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Listening Trends Over Time:**\n",
        "Analyze Spotify listening trends over different time periods (monthly, daily, and weekly).\n",
        "\n",
        "## Data Preparation:\n",
        "### Step 1: Load and Parse Date-Time Data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dc47RgjjFsKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopy"
      ],
      "metadata": {
        "id": "ArSYZeU9TmWE",
        "outputId": "0f0dccc6-dca8-4bd0-abd5-952410c978a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: geopy in /usr/local/lib/python3.10/dist-packages (2.4.1)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.10/dist-packages (from geopy) (2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "ipads_df = pd.read_csv('/content/spotify-listening-data/data/processed/ipads processing/geolocated_ips.csv')\n",
        "\n",
        "# Check the structure of the DataFrame\n",
        "print(ipads_df.info())  # Shows the column names, non-null counts, and data types\n",
        "\n",
        "# Display the first few rows to understand the data\n",
        "print(ipads_df.head())\n"
      ],
      "metadata": {
        "id": "2vsgcBueGneK",
        "outputId": "5f1a5e4f-7078-4e04-a9c2-03710a68bbc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 83 entries, 0 to 82\n",
            "Data columns (total 6 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   unique_ip_address  83 non-null     object \n",
            " 1   city               83 non-null     object \n",
            " 2   region             83 non-null     object \n",
            " 3   country            83 non-null     object \n",
            " 4   latitude           25 non-null     float64\n",
            " 5   longitude          25 non-null     float64\n",
            "dtypes: float64(2), object(4)\n",
            "memory usage: 4.0+ KB\n",
            "None\n",
            "  unique_ip_address                city                region country  \\\n",
            "0      173.79.66.78     Washington D.C.  District of Columbia      US   \n",
            "1     70.110.22.156     Washington D.C.  District of Columbia      US   \n",
            "2    108.18.101.195     Washington D.C.  District of Columbia      US   \n",
            "3    71.246.200.163     Washington D.C.  District of Columbia      US   \n",
            "4     129.64.121.98  Waltham (Brandeis)         Massachusetts      US   \n",
            "\n",
            "    latitude  longitude  \n",
            "0        NaN        NaN  \n",
            "1        NaN        NaN  \n",
            "2        NaN        NaN  \n",
            "3  38.895037 -77.036543  \n",
            "4        NaN        NaN  \n"
          ]
        }
      ]
    }
  ]
}