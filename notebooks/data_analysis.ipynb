{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 9886468,
          "sourceType": "datasetVersion",
          "datasetId": 6071248
        }
      ],
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!ls\n",
        "# Change to the cloned repo directory (optional)\n",
        "!git clone \"https://github.com/ezragershman/spotify-listening-data\"\n",
        "%cd '/content/spotify-listening-data'\n",
        "\n",
        "for dirname, _, filenames in os.walk('/content/spotify-listening-data'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T16:37:44.049622Z",
          "iopub.execute_input": "2024-11-12T16:37:44.050110Z",
          "iopub.status.idle": "2024-11-12T16:37:44.082224Z",
          "shell.execute_reply.started": "2024-11-12T16:37:44.050063Z",
          "shell.execute_reply": "2024-11-12T16:37:44.080893Z"
        },
        "id": "tR7kAO397UKR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull --rebase"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocT3SHlA7F3R",
        "outputId": "b87e7914-4e2a-4ba9-d41e-d4a8bde93dfa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/spotify-listening-data'\n",
        "from google.colab import userdata\n",
        "github_token = userdata.get('github_token')\n",
        "\n",
        "!git config --global user.email \"ezragersh@gmail.com\"\n",
        "!git config --global user.name \"ezragershman\"\n",
        "# Set up Git to use the token for authentication\n",
        "if github_token:\n",
        "    repo_url = f\"https://{github_token}@github.com/ezragershman/spotify-listening-data.git\"  # Replace 'YourGitHubUsername' and 'YourRepository' with your actual GitHub username and repo\n",
        "    !git remote set-url origin {repo_url}\n",
        "    !git add *\n",
        "    !git commit -m \"Spotify Data Updated;\"\n",
        "    !git push origin main\n",
        "else:\n",
        "    print(\"GitHub token is not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfdwU_o685D0",
        "outputId": "56e078eb-705f-4e34-d32c-3d94615481ff"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spotify-listening-data\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Everything up-to-date\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAIN FUNCTION TO UPDATE & SAVE MUSIC LIBRARY DATABASE**\n"
      ],
      "metadata": {
        "id": "mDDlLipM7UKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from base64 import b64encode\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Step 1: Set up Spotify credentials as environment variables in Google Colab\n",
        "CLIENT_ID = userdata.get('spotify_client_id')\n",
        "CLIENT_SECRET = userdata.get('spotify_client_sec')\n",
        "\n",
        "# Step 2: Get Access Token from Spotify API using Client Credentials Flow\n",
        "def get_access_token(client_id, client_secret):\n",
        "    auth_header = b64encode(f\"{client_id}:{client_secret}\".encode('utf-8')).decode('utf-8')\n",
        "    headers = {\n",
        "        'Authorization': f'Basic {auth_header}',\n",
        "        'Content-Type': 'application/x-www-form-urlencoded'\n",
        "    }\n",
        "    data = {'grant_type': 'client_credentials'}\n",
        "\n",
        "    response = requests.post('https://accounts.spotify.com/api/token', headers=headers, data=data)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()['access_token']\n",
        "    else:\n",
        "        print(\"Failed to get access token\")\n",
        "        print(response.json())  # Added for better debugging\n",
        "        return None\n",
        "\n",
        "# Step 3: Search for a track by name and artist\n",
        "def search_track(track_name, artist_name, access_token):\n",
        "    headers = {'Authorization': f'Bearer {access_token}'}\n",
        "    query = f\"track:{track_name} artist:{artist_name}\"\n",
        "    params = {'q': query, 'type': 'track', 'limit': 1}\n",
        "\n",
        "    response = requests.get('https://api.spotify.com/v1/search', headers=headers, params=params)\n",
        "    if response.status_code == 200:\n",
        "          track = response.json()\n",
        "          return {\n",
        "              'album_type': track['album']['album_type'],\n",
        "              'track_uri': track['uri'],\n",
        "              'explicit': track['explicit'],\n",
        "              'available_markets': track['available_markets']\n",
        "          }\n",
        "        else:\n",
        "            print(\"No track found\")\n",
        "            return None\n",
        "    else:\n",
        "        print(\"Failed to search for track\")\n",
        "        print(response.json())  # Added for better debugging\n",
        "        return None\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "GKJ1oCcy7UKX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Example usage - search for \"Never Gonna Give You Up\" by Rick Astley\n",
        "access_token = get_access_token(CLIENT_ID, CLIENT_SECRET)\n",
        "if access_token:\n",
        "    track_info = search_track(\"Never Gonna Give You Up\", \"Rick Astley\", access_token)\n",
        "    if track_info:\n",
        "        print(track_info)"
      ],
      "metadata": {
        "id": "a1g8MXq29ptb",
        "outputId": "40b81b31-af28-41cc-8e43-0d086364d204",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Track Name': 'Never Gonna Give You Up', 'Artist': 'Rick Astley', 'Album': 'Whenever You Need Somebody', 'Release Date': '1987-11-12', 'Duration (ms)': 213573, 'Popularity': 78, 'URI': 'spotify:track:4PTG3Z6ehGkBFwjybzWkR8'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAIN FUNCTION TO UPDATE & SAVE TO DATABASE**\n",
        "Set json_file_path to the name of the file you want to process."
      ],
      "metadata": {
        "id": "gjfLJz8F7UKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_file_path = \"/content/spotify-listening-data/data/raw/Streaming_History_Audio_2013-2024.json\"\n",
        "json_to_csv(json_file_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T15:40:07.289030Z",
          "iopub.execute_input": "2024-11-12T15:40:07.289607Z",
          "iopub.status.idle": "2024-11-12T15:40:07.580940Z",
          "shell.execute_reply.started": "2024-11-12T15:40:07.289557Z",
          "shell.execute_reply": "2024-11-12T15:40:07.579471Z"
        },
        "id": "DF0iBnKW7UKb",
        "outputId": "ee7dc901-ad23-43d1-8fa5-428252224b7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully updated and saved to /content/spotify-listening-data/data/processed/spotify_data.csv\n"
          ]
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import uuid\n",
        "import os\n",
        "import requests\n",
        "\n",
        "def json_to_csv(json_file_path, csv_output_path='/content/spotify-listening-data/data/processed/spotify_data.csv', geolocation_file='/content/spotify-listening-data/data/processed/ipads processing/geolocated_ips.csv'):\n",
        "    # Load the new JSON data into a DataFrame\n",
        "    new_data = pd.read_json(json_file_path)\n",
        "\n",
        "    # Rename columns for consistency\n",
        "    new_data.rename(columns={\n",
        "        'ts': 'end_time_UTC',\n",
        "        'platform': 'platform',\n",
        "        'ms_played': 'ms_played',\n",
        "        'conn_country': 'connection_country',\n",
        "        'ip_addr_decrypted': 'ip_address',\n",
        "        'master_metadata_track_name': 'track_name',\n",
        "        'master_metadata_album_artist_name': 'album_artist_name',\n",
        "        'master_metadata_album_album_name': 'album_name',\n",
        "        'spotify_track_uri': 'track_uri',\n",
        "        'episode_name': 'episode_name',\n",
        "        'episode_show_name': 'episode_show_name',\n",
        "        'spotify_episode_uri': 'episode_uri',\n",
        "        'reason_start': 'start_reason',\n",
        "        'reason_end': 'end_reason',\n",
        "        'shuffle': 'shuffle',\n",
        "        'skipped': 'skipped',\n",
        "    }, inplace=True)\n",
        "\n",
        "    # Load the geolocation data into a DataFrame (assuming it contains the relevant information)\n",
        "    geolocated_ips_df = pd.read_csv(geolocation_file)\n",
        "\n",
        "    # Merge the new data with the geolocation data based on the 'ip_address'\n",
        "    # This will add columns like 'city', 'region', 'country', 'latitude', 'longitude', 'local_time_zone', and 'utc_offset'\n",
        "    new_data = new_data.merge(\n",
        "       geolocated_ips_df[['unique_ip_address', 'city', 'region', 'country', 'latitude', 'longitude', 'local_time_zone', 'utc_offset']],\n",
        "       left_on='ip_address',\n",
        "       right_on='unique_ip_address',\n",
        "       how='left'\n",
        "       )\n",
        "\n",
        "    # Generate a unique ID for each row\n",
        "    new_data['unique_id'] = new_data['end_time_UTC'].apply(lambda x: f\"{x}_{uuid.uuid4().hex[:8]}\")\n",
        "\n",
        "    # Keep only the relevant columns for the CSV\n",
        "    new_data = new_data[['unique_id', 'end_time_UTC', 'platform', 'ms_played', 'connection_country',\n",
        "                        'ip_address', 'track_name', 'album_artist_name', 'album_name',\n",
        "                        'track_uri', 'episode_name', 'episode_show_name', 'episode_uri',\n",
        "                        'start_reason', 'end_reason', 'shuffle', 'skipped',\n",
        "                        'city', 'region', 'country', 'latitude', 'longitude',\n",
        "                        'local_time_zone', 'utc_offset']]  # Include geolocation and timezone columns\n",
        "\n",
        "    # If the output CSV already exists, read it to check for duplicates\n",
        "    if os.path.exists(csv_output_path):\n",
        "        existing_data = pd.read_csv(csv_output_path)\n",
        "\n",
        "        # Identify and keep only new rows that are not already in the CSV\n",
        "        merged_data = pd.concat([existing_data, new_data]).drop_duplicates(subset=['unique_id'], keep='first')\n",
        "    else:\n",
        "        # If no existing CSV, just use new data\n",
        "        merged_data = new_data\n",
        "\n",
        "    # Ensure the 'processed' directory exists (create it if not)\n",
        "    os.makedirs(os.path.dirname(csv_output_path), exist_ok=True)\n",
        "\n",
        "    # Save updated data to CSV in the specified path\n",
        "    merged_data.to_csv(csv_output_path, index=False)\n",
        "\n",
        "    print(f\"Data successfully updated and saved to {csv_output_path}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T16:16:52.328701Z",
          "iopub.execute_input": "2024-11-12T16:16:52.329098Z",
          "iopub.status.idle": "2024-11-12T16:16:52.340326Z",
          "shell.execute_reply.started": "2024-11-12T16:16:52.329059Z",
          "shell.execute_reply": "2024-11-12T16:16:52.339023Z"
        },
        "id": "Ihlqqxdk7UKc"
      },
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "from base64 import b64encode\n",
        "\n",
        "\n",
        "# Step 1: Set up Spotify credentials as environment variables in Google Colab\n",
        "CLIENT_ID = userdata.get('spotify_client_id')\n",
        "CLIENT_SECRET = userdata.get('spotify_client_sec')\n",
        "\n",
        "# Step 1: Get access token from Spotify API\n",
        "def get_access_token(client_id, client_secret):\n",
        "    \"\"\"Fetches an access token from the Spotify API using Client Credentials Flow.\"\"\"\n",
        "    print(\"Requesting access token...\")\n",
        "    auth_header = b64encode(f\"{client_id}:{client_secret}\".encode('utf-8')).decode('utf-8')\n",
        "    headers = {\n",
        "        'Authorization': f'Basic {auth_header}',\n",
        "        'Content-Type': 'application/x-www-form-urlencoded'\n",
        "    }\n",
        "    data = {'grant_type': 'client_credentials'}\n",
        "\n",
        "    response = requests.post('https://accounts.spotify.com/api/token', headers=headers, data=data)\n",
        "    if response.status_code == 200:\n",
        "        print(\"Access token obtained successfully.\")\n",
        "        return response.json()['access_token']\n",
        "    else:\n",
        "        print(\"Failed to get access token\")\n",
        "        print(response.json())  # Debugging output for errors\n",
        "        return None\n",
        "\n",
        "# Step 2: Fetch song details from Spotify API\n",
        "def get_song_details(track_uri, access_token):\n",
        "    \"\"\"Fetches song details for a given track URI using the Spotify API.\"\"\"\n",
        "    print(f\"Fetching details for track URI: {track_uri}...\")\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {access_token}',\n",
        "    }\n",
        "    track_id = track_uri.split(':')[-1]  # Extract track ID from URI\n",
        "    url = f'https://api.spotify.com/v1/tracks/{track_id}'\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        track_info = response.json()\n",
        "        print(f\"Details for {track_uri} fetched successfully.\")\n",
        "        return {\n",
        "            'song_duration_ms': track_info['duration_ms'],\n",
        "            'album_type': track_info['album']['album_type'],\n",
        "            'explicit': track_info['explicit'],\n",
        "            'available_markets': track_info['available_markets']\n",
        "        }\n",
        "    elif response.status_code == 429:\n",
        "        # Handle rate limits\n",
        "        retry_after = int(response.headers.get('Retry-After', 1))\n",
        "        print(f\"Rate limited. Waiting for {retry_after} seconds...\")\n",
        "        time.sleep(retry_after)\n",
        "        return None\n",
        "    else:\n",
        "        print(f\"Error fetching data for {track_uri}: {response.status_code}\")\n",
        "        print(response.text)  # Detailed error logging\n",
        "        return None\n",
        "\n",
        "# Step 3: Load or create a lookup table\n",
        "def load_lookup_table(file_path):\n",
        "    \"\"\"Loads an existing lookup table or creates a new one if not found.\"\"\"\n",
        "    try:\n",
        "        print(f\"Loading lookup table from {file_path}...\")\n",
        "        return pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Lookup table not found, creating a new one.\")\n",
        "        return pd.DataFrame(columns=['track_uri', 'song_duration_ms', 'album_type', 'explicit', 'available_markets'])\n",
        "\n",
        "# Step 4: Update the lookup table\n",
        "def update_lookup_table(lookup_table, track_uri, access_token):\n",
        "    \"\"\"Fetches and updates song details if not already present in the lookup table.\"\"\"\n",
        "    if track_uri in lookup_table['track_uri'].values:\n",
        "        print(f\"Track {track_uri} found in lookup table.\")\n",
        "        song_details = lookup_table[lookup_table['track_uri'] == track_uri].iloc[0].to_dict()\n",
        "    else:\n",
        "        print(f\"Track {track_uri} not found in lookup table, fetching details...\")\n",
        "        song_details = get_song_details(track_uri, access_token)\n",
        "        if song_details:\n",
        "            song_details['track_uri'] = track_uri\n",
        "            lookup_table = pd.concat([lookup_table, pd.DataFrame([song_details])], ignore_index=True)\n",
        "        else:\n",
        "            song_details = None\n",
        "    return song_details, lookup_table\n",
        "\n",
        "# Step 5: Fetch song details for all URIs in the data\n",
        "def fetch_song_details(data, lookup_table, save_interval=10):\n",
        "    \"\"\"Fetches song details for all URIs in the provided data.\"\"\"\n",
        "    print(\"Fetching song details for all URIs in the data...\")\n",
        "    access_token = get_access_token(CLIENT_ID, CLIENT_SECRET)\n",
        "    if not access_token:\n",
        "        print(\"No access token obtained, returning original data.\")\n",
        "        return data, lookup_table\n",
        "\n",
        "    song_details = []\n",
        "    total_rows = len(data['track_uri'].dropna())\n",
        "\n",
        "    for idx, track_uri in enumerate(data['track_uri'].dropna()):\n",
        "        print(f\"Processing track {idx+1}/{total_rows} with URI: {track_uri}\")\n",
        "        details, lookup_table = update_lookup_table(lookup_table, track_uri, access_token)\n",
        "        song_details.append(details)\n",
        "\n",
        "        if (idx + 1) % save_interval == 0:\n",
        "            print(f\"Saving progress after processing {idx+1} tracks...\")\n",
        "            for i, details in enumerate(song_details):\n",
        "                if details is not None:\n",
        "                    keys = ['song_duration_ms', 'album_type', 'explicit', 'available_markets']\n",
        "                    for key in keys:\n",
        "                        if key in details:\n",
        "                            data.loc[i, key] = details[key]\n",
        "\n",
        "            data.to_csv(file_path, index=False)\n",
        "            lookup_table.to_csv(lookup_file_path, index=False)\n",
        "            print(f\"Progress saved after {idx+1} tracks.\")\n",
        "            song_details = []  # Clear list after saving\n",
        "\n",
        "        time.sleep(1)  # Slow down to prevent rate limiting\n",
        "\n",
        "    for i, details in enumerate(song_details):\n",
        "        if details is not None:\n",
        "            keys = ['song_duration_ms', 'album_type', 'explicit', 'available_markets']\n",
        "            for key in keys:\n",
        "                if key in details:\n",
        "                    data.loc[i, key] = details[key]\n",
        "\n",
        "    print(\"All song details added successfully.\")\n",
        "    return data, lookup_table\n",
        "\n",
        "# Step 6: Load existing data and lookup table\n",
        "file_path = '/content/spotify-listening-data/data/processed/spotify_data.csv'\n",
        "lookup_file_path = '/content/spotify-listening-data/data/processed/song_lookup_table.csv'\n",
        "\n",
        "print(f\"Loading data from {file_path}...\")\n",
        "spotify_data = pd.read_csv(file_path)\n",
        "print(f\"Loaded {len(spotify_data)} rows of data.\")\n",
        "\n",
        "lookup_table = load_lookup_table(lookup_file_path)\n",
        "\n",
        "# Step 7: Fetch and update song details\n",
        "spotify_data, lookup_table = fetch_song_details(spotify_data, lookup_table, save_interval=10)\n",
        "\n",
        "# Step 8: Save final results\n",
        "spotify_data.to_csv(file_path, index=False)\n",
        "lookup_table.to_csv(lookup_file_path, index=False)\n",
        "print(f\"Updated data saved to {file_path}.\")\n",
        "print(f\"Updated lookup table saved to {lookup_file_path}.\")\n",
        "print(\"Process completed successfully!\")\n"
      ],
      "metadata": {
        "id": "MAqTCxH4qPyb",
        "outputId": "1af368ce-1d2e-452f-b1d5-329c69b3ef58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from /content/spotify-listening-data/data/processed/spotify_data.csv...\n",
            "Loaded 11213 rows of data.\n",
            "Loading lookup table from /content/spotify-listening-data/data/processed/song_lookup_table.csv...\n",
            "Lookup table not found, creating a new one.\n",
            "Fetching song details for all URIs in the data...\n",
            "Requesting access token...\n",
            "Access token obtained successfully.\n",
            "Processing track 1/10275 with URI: spotify:track:7p7chfq7OqmGmpwWWnPz0p\n",
            "Track spotify:track:7p7chfq7OqmGmpwWWnPz0p not found in lookup table, fetching details...\n",
            "Fetching details for track URI: spotify:track:7p7chfq7OqmGmpwWWnPz0p...\n",
            "Rate limited. Waiting for 77600 seconds...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-c1360c8beec2>\u001b[0m in \u001b[0;36m<cell line: 143>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;31m# Step 7: Fetch and update song details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m \u001b[0mspotify_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_song_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspotify_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;31m# Step 8: Save final results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-c1360c8beec2>\u001b[0m in \u001b[0;36mfetch_song_details\u001b[0;34m(data, lookup_table, save_interval)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack_uri\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'track_uri'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processing track {idx+1}/{total_rows} with URI: {track_uri}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mdetails\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_lookup_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlookup_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccess_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0msong_details\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetails\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-c1360c8beec2>\u001b[0m in \u001b[0;36mupdate_lookup_table\u001b[0;34m(lookup_table, track_uri, access_token)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Track {track_uri} not found in lookup table, fetching details...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0msong_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_song_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccess_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msong_details\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0msong_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'track_uri'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrack_uri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-c1360c8beec2>\u001b[0m in \u001b[0;36mget_song_details\u001b[0;34m(track_uri, access_token)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mretry_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Retry-After'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Rate limited. Waiting for {retry_after} seconds...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_after\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IP Address Information Scraper**"
      ],
      "metadata": {
        "id": "2ErJcgHNSx67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Create table for unique ID information.\n",
        "\n",
        "def extract_unique_ips(json_file_path, output_csv_path='/content/spotify-listening-data/data/processed/ipads processing/unique_ips.csv'):\n",
        "    # Load the JSON data into a DataFrame\n",
        "    data = pd.read_json(json_file_path)\n",
        "\n",
        "    # Extract the IP address column (adjust column name based on your data)\n",
        "    ip_addresses = data['ip_addr_decrypted']  # Ensure 'ip_addr_decrypted' is the correct column name\n",
        "\n",
        "    # Get the unique IP addresses\n",
        "    unique_ips = ip_addresses.dropna().unique()\n",
        "\n",
        "    # Create a DataFrame from the unique IP addresses\n",
        "    unique_ips_df = pd.DataFrame(unique_ips, columns=['unique_ip_address'])\n",
        "\n",
        "    # Save the unique IPs to a CSV file\n",
        "    unique_ips_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "    print(f\"Unique IP addresses extracted and saved to {output_csv_path}\")\n",
        "\n",
        "# Example usage\n",
        "json_file_path = '/content/spotify-listening-data/data/raw/Streaming_History_Audio_2013-2024.json'\n",
        "extract_unique_ips(json_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MYkRh1V73qy",
        "outputId": "04bc4596-c796-4eef-bbff-64844f944c14"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique IP addresses extracted and saved to /content/spotify-listening-data/data/processed/unique_ips.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from ipaddress import ip_network, ip_address\n",
        "\n",
        "# Load IP data into DataFrame (assuming your list is saved as 'unique_ips.csv')\n",
        "df = pd.read_csv('/content/spotify-listening-data/data/processed/ipads processing/unique_ips.csv')\n",
        "\n",
        "def group_by_subnet(ip):\n",
        "    try:\n",
        "        # If it's an IPv4 address\n",
        "        ip_obj = ip_address(ip)\n",
        "        if ip_obj.version == 4:\n",
        "            return f\"{ip_obj.exploded.split('.')[0]}.{ip_obj.exploded.split('.')[1]}.{ip_obj.exploded.split('.')[2]}.0/24\"\n",
        "        else:\n",
        "            # For IPv6, return the first few blocks (customizable)\n",
        "            return ':'.join(ip.split(':')[:4]) + '::/64'\n",
        "    except ValueError:\n",
        "        return 'Invalid IP'\n",
        "\n",
        "# Apply subnet grouping\n",
        "df['subnet'] = df['unique_ip_address'].apply(group_by_subnet)\n",
        "\n",
        "# Drop duplicates based on subnets, keeping one representative IP\n",
        "deduplicated_ip = df.drop_duplicates(subset='subnet')\n",
        "\n",
        "# Save the deduplicated DataFrame as a CSV file\n",
        "deduplicated_ip.to_csv('/content/spotify-listening-data/data/processed/ipads processing/deduplicated_unique_ips.csv', index=False)"
      ],
      "metadata": {
        "id": "g3pEI2SBA-lr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from ip2geotools.databases.noncommercial import DbIpCity\n",
        "\n",
        "# Load previously processed IPs from CSV if it exists\n",
        "def load_processed_ips(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Loaded {len(df)} previously processed IPs from {file_path}.\")\n",
        "        return set(df['unique_ip_address'].tolist())  # Assuming the CSV has a column named 'unique_ip_address'\n",
        "    except FileNotFoundError:\n",
        "        print(f\"No previous geolocated IPs found. Starting fresh.\")\n",
        "        return set()\n",
        "\n",
        "# Save new processed IPs to the CSV\n",
        "def save_processed_ips(file_path, processed_ips):\n",
        "    df = pd.DataFrame(list(processed_ips), columns=['unique_ip_address'])\n",
        "    df.to_csv(file_path, index=False)\n",
        "    print(f\"Saved {len(processed_ips)} processed IPs to {file_path}.\")\n",
        "\n",
        "# Path to the CSV files\n",
        "deduplicated_ips_file = '/content/spotify-listening-data/data/processed/ipads processing/deduplicated_unique_ips.csv'\n",
        "geolocated_ips_file = '/content/spotify-listening-data/data/processed/ipads processing/geolocated_ips.csv'\n",
        "\n",
        "# Load the IPs\n",
        "print(\"Loading deduplicated IPs...\")\n",
        "deduplicated_df = pd.read_csv(deduplicated_ips_file)\n",
        "print(f\"Loaded {len(deduplicated_df)} deduplicated IPs from {deduplicated_ips_file}.\")\n",
        "\n",
        "processed_ips = load_processed_ips(geolocated_ips_file)\n",
        "\n",
        "# Function to get location details from IP address\n",
        "def get_location_from_ip(ip_address):\n",
        "    try:\n",
        "        # Query the ip2geotools database with the given IP address\n",
        "        res = DbIpCity.get(ip_address, api_key=\"free\")\n",
        "\n",
        "        # Structure the location data\n",
        "        location = {\n",
        "            \"city\": res.city,\n",
        "            \"region\": res.region,\n",
        "            \"country\": res.country,\n",
        "            \"latitude\": res.latitude,\n",
        "            \"longitude\": res.longitude\n",
        "        }\n",
        "        return location\n",
        "    except Exception as e:\n",
        "        # Handle any potential errors\n",
        "        return {\n",
        "            \"city\": None,\n",
        "            \"region\": None,\n",
        "            \"country\": None,\n",
        "            \"latitude\": None,\n",
        "            \"longitude\": None,\n",
        "            \"error\": f\"Error: {str(e)}\"\n",
        "        }\n",
        "\n",
        "# Add columns to store location data if not already present\n",
        "if 'city' not in deduplicated_df.columns:\n",
        "    print(\"Adding location columns to deduplicated IP data...\")\n",
        "    deduplicated_df['city'] = None\n",
        "    deduplicated_df['region'] = None\n",
        "    deduplicated_df['country'] = None\n",
        "    deduplicated_df['latitude'] = None\n",
        "    deduplicated_df['longitude'] = None\n",
        "\n",
        "# Start processing IPs\n",
        "print(\"Starting IP geolocation processing...\")\n",
        "\n",
        "new_geolocated_data = []  # To store new geolocated IP data\n",
        "\n",
        "for idx, ip in enumerate(deduplicated_df['unique_ip_address']):\n",
        "    print(ip)\n",
        "    # Check if this IP has already been processed\n",
        "    if ip not in processed_ips:\n",
        "        # Get location data for unprocessed IPs\n",
        "        print(f\"Processing IP {idx + 1}/{len(deduplicated_df)}: {ip}\")\n",
        "        location_data = get_location_from_ip(ip)\n",
        "        deduplicated_df.at[idx, 'city'] = location_data['city']\n",
        "        deduplicated_df.at[idx, 'region'] = location_data['region']\n",
        "        deduplicated_df.at[idx, 'country'] = location_data['country']\n",
        "        deduplicated_df.at[idx, 'latitude'] = location_data['latitude']\n",
        "        deduplicated_df.at[idx, 'longitude'] = location_data['longitude']\n",
        "\n",
        "        # Add the IP to the processed list\n",
        "        processed_ips.add(ip)\n",
        "\n",
        "        # Store new geolocated data\n",
        "        new_geolocated_data.append({\n",
        "            \"unique_ip_address\": ip,\n",
        "            \"city\": location_data['city'],\n",
        "            \"region\": location_data['region'],\n",
        "            \"country\": location_data['country'],\n",
        "            \"latitude\": location_data['latitude'],\n",
        "            \"longitude\": location_data['longitude']\n",
        "        })\n",
        "\n",
        "        # Log the geolocation\n",
        "        print(f\"Processed {ip}: {location_data['city']}, {location_data['region']}, {location_data['country']}\")\n",
        "\n",
        "    # Wait 10 seconds to avoid hitting rate limits\n",
        "    time.sleep(10)\n",
        "\n",
        "# Save the updated deduplicated DataFrame back to the original CSV\n",
        "print(\"Saving updated deduplicated IPs to CSV...\")\n",
        "deduplicated_df.to_csv(deduplicated_ips_file, index=False)\n",
        "\n",
        "# Save the processed IPs to the geolocated file\n",
        "if new_geolocated_data:\n",
        "    new_geolocated_df = pd.DataFrame(new_geolocated_data)\n",
        "    new_geolocated_df.to_csv(geolocated_ips_file, index=False)\n",
        "\n",
        "# Save the processed IPs to the geolocated file\n",
        "save_processed_ips(geolocated_ips_file, processed_ips)\n",
        "\n",
        "print(\"Geolocation data added and saved successfully.\")\n"
      ],
      "metadata": {
        "id": "t4mBrzp4CbnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To remove the time zone and UTC offset columns, if needed\n",
        "geolocated_ips.drop(columns=['local_time_zone', 'utc_offset'], inplace=True)"
      ],
      "metadata": {
        "id": "5YJG1LgNZ081"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from geopy.geocoders import Nominatim\n",
        "from timezonefinder import TimezoneFinder\n",
        "from pytz import timezone\n",
        "from datetime import datetime\n",
        "\n",
        "# Function to get timezone and UTC offset for a given city and country\n",
        "def get_timezone_and_offset(city, country, region=None):\n",
        "    geolocator = Nominatim(user_agent=\"timezone_lookup\")\n",
        "\n",
        "    # First attempt: geocode using city and country\n",
        "    location = geolocator.geocode(f\"{city}, {country}\", timeout=10)\n",
        "\n",
        "    if location:\n",
        "        latitude = location.latitude\n",
        "        longitude = location.longitude\n",
        "        tz_finder = TimezoneFinder()\n",
        "        tz_name = tz_finder.timezone_at(lng=longitude, lat=latitude)\n",
        "\n",
        "        if tz_name:\n",
        "            tz = timezone(tz_name)\n",
        "            utc_offset = tz.utcoffset(datetime.now()).total_seconds() / 3600  # in hours\n",
        "            return tz_name, utc_offset\n",
        "        else:\n",
        "            return 'Unknown', None\n",
        "\n",
        "    else:\n",
        "        # Second attempt: geocode using region if city-based lookup fails\n",
        "        if region:\n",
        "            location = geolocator.geocode(f\"{region}, {country}\", timeout=10)\n",
        "\n",
        "            if location:\n",
        "                latitude = location.latitude\n",
        "                longitude = location.longitude\n",
        "                tz_finder = TimezoneFinder()\n",
        "                tz_name = tz_finder.timezone_at(lng=longitude, lat=latitude)\n",
        "\n",
        "                if tz_name:\n",
        "                    tz = timezone(tz_name)\n",
        "                    utc_offset = tz.utcoffset(datetime.now()).total_seconds() / 3600  # in hours\n",
        "                    return tz_name, utc_offset\n",
        "                else:\n",
        "                    return 'Unknown', None\n",
        "        return 'Unknown', None  # Return 'Unknown' if both lookups fail\n",
        "\n",
        "# Apply the function to your DataFrame\n",
        "def add_time_zone_info(df):\n",
        "    df[['local_time_zone', 'utc_offset']] = df.apply(\n",
        "        lambda row: pd.Series(get_timezone_and_offset(row['city'], row['country'], row['region'])),\n",
        "        axis=1\n",
        "    )\n",
        "    return df\n",
        "\n",
        "# Load the data\n",
        "geolocated_ips = pd.read_csv('/content/spotify-listening-data/data/processed/ipads processing/geolocated_ips.csv')\n",
        "\n",
        "# Apply the function to add time zone information\n",
        "geolocated_ips = add_time_zone_info(geolocated_ips)\n",
        "\n",
        "# Export the updated file\n",
        "geolocated_ips.to_csv('/content/spotify-listening-data/data/processed/ipads processing/geolocated_ips.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "7iVw3f_XcsXM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA ANALYSIS AND MANIPULATION**:"
      ],
      "metadata": {
        "id": "8uVMsLCyCxTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Spotify listening data\n",
        "spotify_data_path = '/content/spotify-listening-data/data/processed/spotify_data.csv'\n",
        "spotify_df = pd.read_csv(spotify_data_path)\n",
        "\n",
        "# Display the first few rows and summary of the data\n",
        "print(\"First few rows of the data:\")\n",
        "print(spotify_df.head())\n",
        "\n",
        "print(\"\\nData structure (columns and data types):\")\n",
        "print(spotify_df.info())\n",
        "\n",
        "print(\"\\nSummary statistics:\")\n",
        "print(spotify_df.describe(include='all'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OId5bfZtC-Mf",
        "outputId": "1a812846-729d-4f94-e660-6e22d3da4be3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the data:\n",
            "                       unique_id          end_time_UTC             platform  \\\n",
            "0  2013-10-13T15:15:10Z_6cdee161  2013-10-13T15:15:10Z  OS X 10.8.4 [x86 4]   \n",
            "1  2013-10-13T15:15:26Z_082a86db  2013-10-13T15:15:26Z  OS X 10.8.4 [x86 4]   \n",
            "2  2013-10-13T15:15:28Z_b922befa  2013-10-13T15:15:28Z  OS X 10.8.4 [x86 4]   \n",
            "3  2013-10-13T15:15:29Z_955600b9  2013-10-13T15:15:29Z  OS X 10.8.4 [x86 4]   \n",
            "4  2013-10-13T15:15:35Z_88ef0788  2013-10-13T15:15:35Z  OS X 10.8.4 [x86 4]   \n",
            "\n",
            "   ms_played connection_country    ip_address      track_name  \\\n",
            "0      39381                 US  173.79.66.78             NaN   \n",
            "1      16427                 US  173.79.66.78  Lost and Found   \n",
            "2       3204                 US  173.79.66.78        Koukasen   \n",
            "3       2136                 US  173.79.66.78            ARIA   \n",
            "4       5313                 US  173.79.66.78    Pray for you   \n",
            "\n",
            "          album_artist_name                      album_name  \\\n",
            "0                       NaN                             NaN   \n",
            "1               sasakure.UK  The Fantastic Reality of Aesop   \n",
            "2                ELLEGARDEN            Eleven Fire Crackers   \n",
            "3                  Kalafina                  Seventh Heaven   \n",
            "4  SOSOSO (Tsukui Kazuhito)         GROW feat. Hatsune Miku   \n",
            "\n",
            "                              track_uri  ... end_reason shuffle skipped  \\\n",
            "0                                   NaN  ...    unknown   False     1.0   \n",
            "1  spotify:track:7p7chfq7OqmGmpwWWnPz0p  ...     fwdbtn   False     1.0   \n",
            "2  spotify:track:5cwAwML79FCaUyPEpEmrIh  ...     fwdbtn   False     1.0   \n",
            "3  spotify:track:2XjZtWBjcvYtVxcXPVoEHi  ...     fwdbtn   False     1.0   \n",
            "4  spotify:track:4AJY2aMfSaJip3o1JJCRCs  ...     fwdbtn   False     1.0   \n",
            "\n",
            "              city                region  country  latitude longitude  \\\n",
            "0  Washington D.C.  District of Columbia       US       NaN       NaN   \n",
            "1  Washington D.C.  District of Columbia       US       NaN       NaN   \n",
            "2  Washington D.C.  District of Columbia       US       NaN       NaN   \n",
            "3  Washington D.C.  District of Columbia       US       NaN       NaN   \n",
            "4  Washington D.C.  District of Columbia       US       NaN       NaN   \n",
            "\n",
            "    local_time_zone utc_offset  \n",
            "0  America/New_York       -5.0  \n",
            "1  America/New_York       -5.0  \n",
            "2  America/New_York       -5.0  \n",
            "3  America/New_York       -5.0  \n",
            "4  America/New_York       -5.0  \n",
            "\n",
            "[5 rows x 24 columns]\n",
            "\n",
            "Data structure (columns and data types):\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11213 entries, 0 to 11212\n",
            "Data columns (total 24 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   unique_id           11213 non-null  object \n",
            " 1   end_time_UTC        11213 non-null  object \n",
            " 2   platform            11213 non-null  object \n",
            " 3   ms_played           11213 non-null  int64  \n",
            " 4   connection_country  11213 non-null  object \n",
            " 5   ip_address          11213 non-null  object \n",
            " 6   track_name          10275 non-null  object \n",
            " 7   album_artist_name   10275 non-null  object \n",
            " 8   album_name          10275 non-null  object \n",
            " 9   track_uri           10275 non-null  object \n",
            " 10  episode_name        20 non-null     object \n",
            " 11  episode_show_name   20 non-null     object \n",
            " 12  episode_uri         20 non-null     object \n",
            " 13  start_reason        11212 non-null  object \n",
            " 14  end_reason          11213 non-null  object \n",
            " 15  shuffle             11213 non-null  bool   \n",
            " 16  skipped             11193 non-null  float64\n",
            " 17  city                8165 non-null   object \n",
            " 18  region              8165 non-null   object \n",
            " 19  country             8165 non-null   object \n",
            " 20  latitude            3744 non-null   float64\n",
            " 21  longitude           3744 non-null   float64\n",
            " 22  local_time_zone     8165 non-null   object \n",
            " 23  utc_offset          8165 non-null   float64\n",
            "dtypes: bool(1), float64(4), int64(1), object(18)\n",
            "memory usage: 2.0+ MB\n",
            "None\n",
            "\n",
            "Summary statistics:\n",
            "                            unique_id          end_time_UTC platform  \\\n",
            "count                           11213                 11213    11213   \n",
            "unique                          11213                 10902       13   \n",
            "top     2013-10-13T15:15:10Z_6cdee161  2014-04-20T01:28:38Z      ios   \n",
            "freq                                1                    14     9944   \n",
            "mean                              NaN                   NaN      NaN   \n",
            "std                               NaN                   NaN      NaN   \n",
            "min                               NaN                   NaN      NaN   \n",
            "25%                               NaN                   NaN      NaN   \n",
            "50%                               NaN                   NaN      NaN   \n",
            "75%                               NaN                   NaN      NaN   \n",
            "max                               NaN                   NaN      NaN   \n",
            "\n",
            "           ms_played connection_country       ip_address         track_name  \\\n",
            "count   1.121300e+04              11213            11213              10275   \n",
            "unique           NaN                  2              188               4686   \n",
            "top              NaN                 US  199.195.178.243  Genie In a Bottle   \n",
            "freq             NaN              11210             2455                150   \n",
            "mean    8.727164e+04                NaN              NaN                NaN   \n",
            "std     1.063838e+05                NaN              NaN                NaN   \n",
            "min     0.000000e+00                NaN              NaN                NaN   \n",
            "25%     3.342000e+03                NaN              NaN                NaN   \n",
            "50%     3.065100e+04                NaN              NaN                NaN   \n",
            "75%     1.754590e+05                NaN              NaN                NaN   \n",
            "max     3.150141e+06                NaN              NaN                NaN   \n",
            "\n",
            "               album_artist_name                             album_name  \\\n",
            "count                      10275                                  10275   \n",
            "unique                      1805                                   2682   \n",
            "top     Crazy Ex-Girlfriend Cast  Christina Aguilera (Expanded Edition)   \n",
            "freq                         441                                    150   \n",
            "mean                         NaN                                    NaN   \n",
            "std                          NaN                                    NaN   \n",
            "min                          NaN                                    NaN   \n",
            "25%                          NaN                                    NaN   \n",
            "50%                          NaN                                    NaN   \n",
            "75%                          NaN                                    NaN   \n",
            "max                          NaN                                    NaN   \n",
            "\n",
            "                                   track_uri  ... end_reason shuffle  \\\n",
            "count                                  10275  ...      11213   11213   \n",
            "unique                                  5105  ...         13       2   \n",
            "top     spotify:track:11mwFrKvLXCbcVGNxffGyP  ...  trackdone   False   \n",
            "freq                                     150  ...       4827    5825   \n",
            "mean                                     NaN  ...        NaN     NaN   \n",
            "std                                      NaN  ...        NaN     NaN   \n",
            "min                                      NaN  ...        NaN     NaN   \n",
            "25%                                      NaN  ...        NaN     NaN   \n",
            "50%                                      NaN  ...        NaN     NaN   \n",
            "75%                                      NaN  ...        NaN     NaN   \n",
            "max                                      NaN  ...        NaN     NaN   \n",
            "\n",
            "             skipped   city    region country     latitude    longitude  \\\n",
            "count   11193.000000   8165      8165    8165  3744.000000  3744.000000   \n",
            "unique           NaN     44        17       1          NaN          NaN   \n",
            "top              NaN  Keyes  Illinois      US          NaN          NaN   \n",
            "freq             NaN   2455      3473    8165          NaN          NaN   \n",
            "mean        0.528098    NaN       NaN     NaN    38.469788  -108.557077   \n",
            "std         0.499232    NaN       NaN     NaN     1.897776    18.054898   \n",
            "min         0.000000    NaN       NaN     NaN    32.678109  -120.907233   \n",
            "25%         0.000000    NaN       NaN     NaN    37.560463  -120.907233   \n",
            "50%         1.000000    NaN       NaN     NaN    37.560463  -120.907233   \n",
            "75%         1.000000    NaN       NaN     NaN    39.043719   -87.788762   \n",
            "max         1.000000    NaN       NaN     NaN    42.733825   -74.006015   \n",
            "\n",
            "        local_time_zone   utc_offset  \n",
            "count              8165  8165.000000  \n",
            "unique                5          NaN  \n",
            "top     America/Chicago          NaN  \n",
            "freq               3585          NaN  \n",
            "mean                NaN    -6.373423  \n",
            "std                 NaN     1.164477  \n",
            "min                 NaN    -8.000000  \n",
            "25%                 NaN    -8.000000  \n",
            "50%                 NaN    -6.000000  \n",
            "75%                 NaN    -6.000000  \n",
            "max                 NaN    -5.000000  \n",
            "\n",
            "[11 rows x 24 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Listening Trends Over Time:**\n",
        "Analyze Spotify listening trends over different time periods (monthly, daily, and weekly).\n",
        "\n",
        "## Data Preparation:\n",
        "### Step 1: Load and Parse Date-Time Data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dc47RgjjFsKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArSYZeU9TmWE",
        "outputId": "0f0dccc6-dca8-4bd0-abd5-952410c978a9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: geopy in /usr/local/lib/python3.10/dist-packages (2.4.1)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.10/dist-packages (from geopy) (2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pytz\n",
        "from datetime import timedelta\n",
        "\n",
        "# Step 1: Load the data (replace with actual data loading process)\n",
        "# Assuming the data is loaded from a CSV or another source\n",
        "new_data = pd.read_csv('/content/spotify-listening-data/data/processed/spotify_data.csv')\n",
        "\n",
        "# Step 2: Convert 'end_time_UTC' to datetime\n",
        "new_data['end_time_UTC'] = pd.to_datetime(new_data['end_time_UTC'])\n",
        "\n",
        "# Step 3: Convert UTC to Local Time\n",
        "# We'll adjust the 'end_time_UTC' column to local time using the 'utc_offset' column.\n",
        "\n",
        "# Function to convert from UTC to local time\n",
        "def convert_to_local_time(row):\n",
        "    # If 'utc_offset' is NaN, return the original UTC time (or handle as appropriate)\n",
        "    if pd.isna(row['utc_offset']):\n",
        "        return row['end_time_UTC']\n",
        "\n",
        "    utc_time = row['end_time_UTC']\n",
        "    utc_offset = row['utc_offset']\n",
        "\n",
        "    # Apply UTC offset to convert to local time\n",
        "    local_time = utc_time + timedelta(hours=utc_offset)\n",
        "    return local_time\n",
        "\n",
        "# Apply the function to create a new 'end_time_local' column\n",
        "new_data['end_time_local'] = new_data.apply(convert_to_local_time, axis=1)\n",
        "\n",
        "# Step 4: Apply the function to create a new 'end_time_local' column\n",
        "new_data['end_time_local'] = new_data.apply(convert_to_local_time, axis=1)\n",
        "\n",
        "# Step 5: Extract Time Features from 'end_time_local' (if not NaN)\n",
        "# Now that we have 'end_time_local', we'll extract day of the week, hour, and month.\n",
        "\n",
        "# Make sure to handle cases where 'end_time_local' might be NaN (if there were missing UTC offsets)\n",
        "# We can use `.dt` accessor to extract day of the week, hour, and month\n",
        "\n",
        "# First, we need to check for non-NaT values in 'end_time_local'\n",
        "new_data = new_data[new_data['end_time_local'].notna()]\n",
        "\n",
        "# Extract 'day_of_week', 'hour', and 'month' from 'end_time_local'\n",
        "new_data['day_of_week'] = new_data['end_time_local'].dt.weekday  # 0=Monday, 6=Sunday\n",
        "new_data['hour'] = new_data['end_time_local'].dt.hour  # Extract the hour\n",
        "new_data['month'] = new_data['end_time_local'].dt.month  # Extract the month\n",
        "new_data['year'] = new_data['end_time_local'].dt.year  # Extract the year\n",
        "\n",
        "# Step 6: Check the new DataFrame to see the extracted time features\n",
        "new_data"
      ],
      "metadata": {
        "id": "2vsgcBueGneK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}