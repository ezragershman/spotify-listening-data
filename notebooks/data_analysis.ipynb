{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9885841,"sourceType":"datasetVersion","datasetId":6070744}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:29:01.383990Z","iopub.execute_input":"2024-11-12T15:29:01.384562Z","iopub.status.idle":"2024-11-12T15:29:01.882367Z","shell.execute_reply.started":"2024-11-12T15:29:01.384509Z","shell.execute_reply":"2024-11-12T15:29:01.880804Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/music-data/README.md\n/kaggle/input/music-data/notebooks/data_analysis.ipynb\n/kaggle/input/music-data/data/processed/placeholder.md\n/kaggle/input/music-data/data/raw/StreamingHistory_music_1.json\n/kaggle/input/music-data/data/raw/READMEplaceholder.md\n/kaggle/input/music-data/data/raw/StreamingHistory_podcast_0.json\n/kaggle/input/music-data/data/raw/StreamingHistory_music_0.json\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# **MAIN FUNCTION TO UPDATE & SAVE TO DATABASE**\nSet json_file_path to the name of the file you want to process. ","metadata":{}},{"cell_type":"code","source":"json_file_path = \"/kaggle/input/music-data/data/raw/StreamingHistory_music_0.json\"\njson_to_csv(json_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:40:07.289030Z","iopub.execute_input":"2024-11-12T15:40:07.289607Z","iopub.status.idle":"2024-11-12T15:40:07.580940Z","shell.execute_reply.started":"2024-11-12T15:40:07.289557Z","shell.execute_reply":"2024-11-12T15:40:07.579471Z"}},"outputs":[{"name":"stdout","text":"Data successfully updated and saved to data/processed/spotify_data.csv\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\nimport uuid\nimport os\n\ndef json_to_csv(json_file_path, csv_output_path='data/processed/spotify_data.csv'):\n    # Load new JSON data into a DataFrame\n    new_data = pd.read_json(json_file_path)\n    \n    # Rename columns for consistency\n    new_data.rename(columns={\n        'endTime': 'end_time',\n        'artistName': 'artist_name',\n        'trackName': 'track_name',\n        'msPlayed': 'ms_played'\n    }, inplace=True)\n\n    # Generate a unique ID for each row based on 'end_time' + uuid for extra uniqueness\n    new_data['unique_id'] = new_data['end_time'].apply(lambda x: f\"{x.replace(' ', '_')}_{uuid.uuid4().hex[:8]}\")\n\n    # Keep only relevant columns\n    new_data = new_data[['unique_id', 'end_time', 'artist_name', 'track_name', 'ms_played']]\n\n    # If the output CSV already exists, read it to check for duplicates\n    if os.path.exists(csv_output_path):\n        existing_data = pd.read_csv(csv_output_path)\n\n        # Identify and keep only new rows that are not already in the CSV\n        merged_data = pd.concat([existing_data, new_data]).drop_duplicates(subset=['unique_id'], keep='first')\n    else:\n        # If no existing CSV, just use new data\n        merged_data = new_data\n\n    # Ensure the 'processed' directory exists (create it if not)\n    os.makedirs(os.path.dirname(csv_output_path), exist_ok=True)\n\n    # Save updated data to CSV in the specified path\n    merged_data.to_csv(csv_output_path, index=False)\n\n    print(f\"Data successfully updated and saved to {csv_output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:38:45.434063Z","iopub.execute_input":"2024-11-12T15:38:45.434548Z","iopub.status.idle":"2024-11-12T15:38:45.446571Z","shell.execute_reply.started":"2024-11-12T15:38:45.434501Z","shell.execute_reply":"2024-11-12T15:38:45.444845Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"data/processed/spotify_data.csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T15:40:35.478950Z","iopub.execute_input":"2024-11-12T15:40:35.479469Z","iopub.status.idle":"2024-11-12T15:40:35.515571Z","shell.execute_reply.started":"2024-11-12T15:40:35.479396Z","shell.execute_reply":"2024-11-12T15:40:35.513886Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241m/\u001b[39mprocessed\u001b[38;5;241m/\u001b[39mspotify_data\u001b[38;5;241m.\u001b[39mcsv\n","\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"],"ename":"NameError","evalue":"name 'data' is not defined","output_type":"error"}],"execution_count":9}]}