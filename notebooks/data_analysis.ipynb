{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 9886468,
          "sourceType": "datasetVersion",
          "datasetId": 6071248
        }
      ],
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!ls\n",
        "# Change to the cloned repo directory (optional)\n",
        "!git clone \"https://github.com/ezragershman/spotify-listening-data\"\n",
        "%cd '/content/spotify-listening-data'\n",
        "\n",
        "for dirname, _, filenames in os.walk('/content/spotify-listening-data'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T16:37:44.049622Z",
          "iopub.execute_input": "2024-11-12T16:37:44.050110Z",
          "iopub.status.idle": "2024-11-12T16:37:44.082224Z",
          "shell.execute_reply.started": "2024-11-12T16:37:44.050063Z",
          "shell.execute_reply": "2024-11-12T16:37:44.080893Z"
        },
        "id": "tR7kAO397UKR",
        "outputId": "e483d8e8-defc-4570-ae74-ade251edd2bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n",
            "Cloning into 'spotify-listening-data'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 97 (delta 29), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (97/97), 2.10 MiB | 3.59 MiB/s, done.\n",
            "Resolving deltas: 100% (29/29), done.\n",
            "/content/spotify-listening-data\n",
            "/content/spotify-listening-data/README.md\n",
            "/content/spotify-listening-data/notebooks/data_analysis.ipynb\n",
            "/content/spotify-listening-data/data/raw/READMEplaceholder.md\n",
            "/content/spotify-listening-data/data/raw/Streaming_History_Audio_2013-2024.json\n",
            "/content/spotify-listening-data/data/processed/spotify_data.csv\n",
            "/content/spotify-listening-data/.git/HEAD\n",
            "/content/spotify-listening-data/.git/description\n",
            "/content/spotify-listening-data/.git/packed-refs\n",
            "/content/spotify-listening-data/.git/index\n",
            "/content/spotify-listening-data/.git/config\n",
            "/content/spotify-listening-data/.git/objects/pack/pack-b4a4f34cedcba5e3a6c0d267a99dbb82480dc031.pack\n",
            "/content/spotify-listening-data/.git/objects/pack/pack-b4a4f34cedcba5e3a6c0d267a99dbb82480dc031.idx\n",
            "/content/spotify-listening-data/.git/refs/remotes/origin/HEAD\n",
            "/content/spotify-listening-data/.git/refs/heads/main\n",
            "/content/spotify-listening-data/.git/logs/HEAD\n",
            "/content/spotify-listening-data/.git/logs/refs/remotes/origin/HEAD\n",
            "/content/spotify-listening-data/.git/logs/refs/heads/main\n",
            "/content/spotify-listening-data/.git/hooks/fsmonitor-watchman.sample\n",
            "/content/spotify-listening-data/.git/hooks/push-to-checkout.sample\n",
            "/content/spotify-listening-data/.git/hooks/update.sample\n",
            "/content/spotify-listening-data/.git/hooks/pre-push.sample\n",
            "/content/spotify-listening-data/.git/hooks/commit-msg.sample\n",
            "/content/spotify-listening-data/.git/hooks/pre-receive.sample\n",
            "/content/spotify-listening-data/.git/hooks/pre-applypatch.sample\n",
            "/content/spotify-listening-data/.git/hooks/pre-merge-commit.sample\n",
            "/content/spotify-listening-data/.git/hooks/applypatch-msg.sample\n",
            "/content/spotify-listening-data/.git/hooks/prepare-commit-msg.sample\n",
            "/content/spotify-listening-data/.git/hooks/pre-rebase.sample\n",
            "/content/spotify-listening-data/.git/hooks/post-update.sample\n",
            "/content/spotify-listening-data/.git/hooks/pre-commit.sample\n",
            "/content/spotify-listening-data/.git/info/exclude\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull --rebase"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocT3SHlA7F3R",
        "outputId": "0bb2cddc-ce1b-41d7-d0a7-02f1142417b6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rebasing (1/1)\r\r\u001b[KSuccessfully rebased and updated refs/heads/main.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/spotify-listening-data'\n",
        "from google.colab import userdata\n",
        "github_token = userdata.get('github_token')\n",
        "\n",
        "!git config --global user.email \"ezragersh@gmail.com\"\n",
        "!git config --global user.name \"ezragershman\"\n",
        "# Set up Git to use the token for authentication\n",
        "if github_token:\n",
        "    repo_url = f\"https://{github_token}@github.com/ezragershman/spotify-listening-data.git\"  # Replace 'YourGitHubUsername' and 'YourRepository' with your actual GitHub username and repo\n",
        "    !git remote set-url origin {repo_url}\n",
        "    !git add *\n",
        "    !git commit -m \"Added geolocated ip data pt2\"\n",
        "    !git push origin main\n",
        "else:\n",
        "    print(\"GitHub token is not available.\")"
      ],
      "metadata": {
        "id": "HfdwU_o685D0",
        "outputId": "6ef6333e-e5fe-4719-9aa4-329372b4341c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spotify-listening-data\n",
            "[main 8fa0d21] Added geolocated ip data pt2\n",
            " 2 files changed, 168 insertions(+), 168 deletions(-)\n",
            " rewrite data/processed/deduplicated_unique_ips.csv (97%)\n",
            " rewrite data/processed/geolocated_ips.csv (100%)\n",
            "Enumerating objects: 11, done.\n",
            "Counting objects: 100% (11/11), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (6/6), done.\n",
            "Writing objects: 100% (6/6), 1.52 KiB | 1.52 MiB/s, done.\n",
            "Total 6 (delta 1), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "To https://github.com/ezragershman/spotify-listening-data.git\n",
            "   c868ce6..8fa0d21  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PrifwzUb7GlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAIN FUNCTION TO UPDATE & SAVE MUSIC LIBRARY DATABASE**\n"
      ],
      "metadata": {
        "id": "mDDlLipM7UKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from base64 import b64encode\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Step 1: Set up Spotify credentials as environment variables in Google Colab\n",
        "CLIENT_ID = userdata.get('spotify_client_id')\n",
        "CLIENT_SECRET = userdata.get('spotify_client_sec')\n",
        "\n",
        "# Step 2: Get Access Token from Spotify API using Client Credentials Flow\n",
        "def get_access_token(client_id, client_secret):\n",
        "    auth_header = b64encode(f\"{client_id}:{client_secret}\".encode('utf-8')).decode('utf-8')\n",
        "    headers = {\n",
        "        'Authorization': f'Basic {auth_header}',\n",
        "        'Content-Type': 'application/x-www-form-urlencoded'\n",
        "    }\n",
        "    data = {'grant_type': 'client_credentials'}\n",
        "\n",
        "    response = requests.post('https://accounts.spotify.com/api/token', headers=headers, data=data)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()['access_token']\n",
        "    else:\n",
        "        print(\"Failed to get access token\")\n",
        "        print(response.json())  # Added for better debugging\n",
        "        return None\n",
        "\n",
        "# Step 3: Search for a track by name and artist\n",
        "def search_track(track_name, artist_name, access_token):\n",
        "    headers = {'Authorization': f'Bearer {access_token}'}\n",
        "    query = f\"track:{track_name} artist:{artist_name}\"\n",
        "    params = {'q': query, 'type': 'track', 'limit': 1}\n",
        "\n",
        "    response = requests.get('https://api.spotify.com/v1/search', headers=headers, params=params)\n",
        "    if response.status_code == 200:\n",
        "          track = response.json()\n",
        "          return {\n",
        "              'album_type': track['album']['album_type'],\n",
        "              'track_uri': track['uri'],\n",
        "              'explicit': track['explicit'],\n",
        "              'available_markets': track['available_markets']\n",
        "          }\n",
        "        else:\n",
        "            print(\"No track found\")\n",
        "            return None\n",
        "    else:\n",
        "        print(\"Failed to search for track\")\n",
        "        print(response.json())  # Added for better debugging\n",
        "        return None\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "GKJ1oCcy7UKX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Example usage - search for \"Never Gonna Give You Up\" by Rick Astley\n",
        "access_token = get_access_token(CLIENT_ID, CLIENT_SECRET)\n",
        "if access_token:\n",
        "    track_info = search_track(\"Never Gonna Give You Up\", \"Rick Astley\", access_token)\n",
        "    if track_info:\n",
        "        print(track_info)"
      ],
      "metadata": {
        "id": "a1g8MXq29ptb",
        "outputId": "40b81b31-af28-41cc-8e43-0d086364d204",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Track Name': 'Never Gonna Give You Up', 'Artist': 'Rick Astley', 'Album': 'Whenever You Need Somebody', 'Release Date': '1987-11-12', 'Duration (ms)': 213573, 'Popularity': 78, 'URI': 'spotify:track:4PTG3Z6ehGkBFwjybzWkR8'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAIN FUNCTION TO UPDATE & SAVE TO DATABASE**\n",
        "Set json_file_path to the name of the file you want to process."
      ],
      "metadata": {
        "id": "gjfLJz8F7UKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_file_path = \"/content/spotify-listening-data/data/raw/Streaming_History_Audio_2013-2024.json\"\n",
        "json_to_csv(json_file_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T15:40:07.289030Z",
          "iopub.execute_input": "2024-11-12T15:40:07.289607Z",
          "iopub.status.idle": "2024-11-12T15:40:07.580940Z",
          "shell.execute_reply.started": "2024-11-12T15:40:07.289557Z",
          "shell.execute_reply": "2024-11-12T15:40:07.579471Z"
        },
        "id": "DF0iBnKW7UKb",
        "outputId": "6cf4cb3f-727c-4cd1-f9b5-bd9acb27192d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully updated and saved to /content/spotify-listening-data/data/processed/spotify_data.csv\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import uuid\n",
        "import os\n",
        "import requests\n",
        "\n",
        "def json_to_csv(json_file_path, csv_output_path='/content/spotify-listening-data/data/processed/spotify_data.csv'):\n",
        "    # Load the new JSON data into a DataFrame\n",
        "    new_data = pd.read_json(json_file_path)\n",
        "\n",
        "    # Rename columns for consistency\n",
        "    new_data.rename(columns={\n",
        "        'ts': 'end_time_UTC',\n",
        "        'platform': 'platform',\n",
        "        'ms_played': 'ms_played',\n",
        "        'conn_country': 'connection_country',\n",
        "        'ip_addr_decrypted': 'ip_address',\n",
        "        'master_metadata_track_name': 'track_name',\n",
        "        'master_metadata_album_artist_name': 'album_artist_name',\n",
        "        'master_metadata_album_album_name': 'album_name',\n",
        "        'spotify_track_uri': 'track_uri',\n",
        "        'episode_name': 'episode_name',\n",
        "        'episode_show_name': 'episode_show_name',\n",
        "        'spotify_episode_uri': 'episode_uri',\n",
        "        'reason_start': 'start_reason',\n",
        "        'reason_end': 'end_reason',\n",
        "        'shuffle': 'shuffle',\n",
        "        'skipped': 'skipped',\n",
        "    }, inplace=True)\n",
        "\n",
        "    # # Get the location data and normalize it into separate columns\n",
        "    # location_data = new_data['ip_address'].apply(lambda ip: get_location_from_ip(ip))\n",
        "\n",
        "    # # Normalize the location data into separate columns\n",
        "    # location_df = json_normalize(location_data)\n",
        "\n",
        "    # # Merge the location data with the original DataFrame\n",
        "    # new_data = pd.concat([new_data, location_df], axis=1)\n",
        "\n",
        "    # Generate a unique ID for each row\n",
        "    new_data['unique_id'] = new_data['end_time_UTC'].apply(lambda x: f\"{x}_{uuid.uuid4().hex[:8]}\")\n",
        "\n",
        "    # Keep only the relevant columns for the CSV\n",
        "    new_data = new_data[['unique_id', 'end_time_UTC', 'platform', 'ms_played', 'connection_country',\n",
        "                         'ip_address', 'track_name', 'album_artist_name', 'album_name',\n",
        "                         'track_uri', 'episode_name', 'episode_show_name', 'episode_uri',\n",
        "                         'start_reason', 'end_reason', 'shuffle', 'skipped',\n",
        "                         # 'country', 'region', 'city', 'latitude', 'longitude'\n",
        "                         ]]\n",
        "\n",
        "    # If the output CSV already exists, read it to check for duplicates\n",
        "    if os.path.exists(csv_output_path):\n",
        "        existing_data = pd.read_csv(csv_output_path)\n",
        "\n",
        "        # Identify and keep only new rows that are not already in the CSV\n",
        "        merged_data = pd.concat([existing_data, new_data]).drop_duplicates(subset=['unique_id'], keep='first')\n",
        "    else:\n",
        "        # If no existing CSV, just use new data\n",
        "        merged_data = new_data\n",
        "\n",
        "    # Ensure the 'processed' directory exists (create it if not)\n",
        "    os.makedirs(os.path.dirname(csv_output_path), exist_ok=True)\n",
        "\n",
        "    # Save updated data to CSV in the specified path\n",
        "    merged_data.to_csv(csv_output_path, index=False)\n",
        "\n",
        "    print(f\"Data successfully updated and saved to {csv_output_path}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T16:16:52.328701Z",
          "iopub.execute_input": "2024-11-12T16:16:52.329098Z",
          "iopub.status.idle": "2024-11-12T16:16:52.340326Z",
          "shell.execute_reply.started": "2024-11-12T16:16:52.329059Z",
          "shell.execute_reply": "2024-11-12T16:16:52.339023Z"
        },
        "id": "Ihlqqxdk7UKc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "031634f9-5075-4264-b3a3-bcf5a70d6f1f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ip2geotools'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-199158e29dcb>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mip2geotools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoncommercial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDbIpCity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ip2geotools'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Create table for unique ID information.\n",
        "\n",
        "def extract_unique_ips(json_file_path, output_csv_path='/content/spotify-listening-data/data/processed/unique_ips.csv'):\n",
        "    # Load the JSON data into a DataFrame\n",
        "    data = pd.read_json(json_file_path)\n",
        "\n",
        "    # Extract the IP address column (adjust column name based on your data)\n",
        "    ip_addresses = data['ip_addr_decrypted']  # Ensure 'ip_addr_decrypted' is the correct column name\n",
        "\n",
        "    # Get the unique IP addresses\n",
        "    unique_ips = ip_addresses.dropna().unique()\n",
        "\n",
        "    # Create a DataFrame from the unique IP addresses\n",
        "    unique_ips_df = pd.DataFrame(unique_ips, columns=['unique_ip_address'])\n",
        "\n",
        "    # Save the unique IPs to a CSV file\n",
        "    unique_ips_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "    print(f\"Unique IP addresses extracted and saved to {output_csv_path}\")\n",
        "\n",
        "# Example usage\n",
        "json_file_path = '/content/spotify-listening-data/data/raw/Streaming_History_Audio_2013-2024.json'\n",
        "extract_unique_ips(json_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MYkRh1V73qy",
        "outputId": "04bc4596-c796-4eef-bbff-64844f944c14"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique IP addresses extracted and saved to /content/spotify-listening-data/data/processed/unique_ips.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from ipaddress import ip_network, ip_address\n",
        "\n",
        "# Load IP data into DataFrame (assuming your list is saved as 'unique_ips.csv')\n",
        "df = pd.read_csv('data/processed/unique_ips.csv')\n",
        "\n",
        "def group_by_subnet(ip):\n",
        "    try:\n",
        "        # If it's an IPv4 address\n",
        "        ip_obj = ip_address(ip)\n",
        "        if ip_obj.version == 4:\n",
        "            return f\"{ip_obj.exploded.split('.')[0]}.{ip_obj.exploded.split('.')[1]}.{ip_obj.exploded.split('.')[2]}.0/24\"\n",
        "        else:\n",
        "            # For IPv6, return the first few blocks (customizable)\n",
        "            return ':'.join(ip.split(':')[:4]) + '::/64'\n",
        "    except ValueError:\n",
        "        return 'Invalid IP'\n",
        "\n",
        "# Apply subnet grouping\n",
        "df['subnet'] = df['unique_ip_address'].apply(group_by_subnet)\n",
        "\n",
        "# Drop duplicates based on subnets, keeping one representative IP\n",
        "deduplicated_ip = df.drop_duplicates(subset='subnet')\n",
        "\n",
        "# Save the deduplicated DataFrame as a CSV file\n",
        "deduplicated_ip.to_csv('data/processed/deduplicated_unique_ips.csv', index=False)"
      ],
      "metadata": {
        "id": "g3pEI2SBA-lr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from ip2geotools.databases.noncommercial import DbIpCity\n",
        "\n",
        "# Load previously processed IPs from CSV if it exists\n",
        "def load_processed_ips(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Loaded {len(df)} previously processed IPs from {file_path}.\")\n",
        "        return set(df['unique_ip_address'].tolist())  # Assuming the CSV has a column named 'unique_ip_address'\n",
        "    except FileNotFoundError:\n",
        "        print(f\"No previous geolocated IPs found. Starting fresh.\")\n",
        "        return set()\n",
        "\n",
        "# Save new processed IPs to the CSV\n",
        "def save_processed_ips(file_path, processed_ips):\n",
        "    df = pd.DataFrame(list(processed_ips), columns=['unique_ip_address'])\n",
        "    df.to_csv(file_path, index=False)\n",
        "    print(f\"Saved {len(processed_ips)} processed IPs to {file_path}.\")\n",
        "\n",
        "# Path to the CSV files\n",
        "deduplicated_ips_file = '/content/spotify-listening-data/data/processed/deduplicated_unique_ips.csv'\n",
        "geolocated_ips_file = '/content/spotify-listening-data/data/processed/geolocated_ips.csv'\n",
        "\n",
        "# Load the IPs\n",
        "print(\"Loading deduplicated IPs...\")\n",
        "deduplicated_df = pd.read_csv(deduplicated_ips_file)\n",
        "print(f\"Loaded {len(deduplicated_df)} deduplicated IPs from {deduplicated_ips_file}.\")\n",
        "\n",
        "processed_ips = load_processed_ips(geolocated_ips_file)\n",
        "\n",
        "# Function to get location details from IP address\n",
        "def get_location_from_ip(ip_address):\n",
        "    try:\n",
        "        # Query the ip2geotools database with the given IP address\n",
        "        res = DbIpCity.get(ip_address, api_key=\"free\")\n",
        "\n",
        "        # Structure the location data\n",
        "        location = {\n",
        "            \"city\": res.city,\n",
        "            \"region\": res.region,\n",
        "            \"country\": res.country,\n",
        "            \"latitude\": res.latitude,\n",
        "            \"longitude\": res.longitude\n",
        "        }\n",
        "        return location\n",
        "    except Exception as e:\n",
        "        # Handle any potential errors\n",
        "        return {\n",
        "            \"city\": None,\n",
        "            \"region\": None,\n",
        "            \"country\": None,\n",
        "            \"latitude\": None,\n",
        "            \"longitude\": None,\n",
        "            \"error\": f\"Error: {str(e)}\"\n",
        "        }\n",
        "\n",
        "# Add columns to store location data if not already present\n",
        "if 'city' not in deduplicated_df.columns:\n",
        "    print(\"Adding location columns to deduplicated IP data...\")\n",
        "    deduplicated_df['city'] = None\n",
        "    deduplicated_df['region'] = None\n",
        "    deduplicated_df['country'] = None\n",
        "    deduplicated_df['latitude'] = None\n",
        "    deduplicated_df['longitude'] = None\n",
        "\n",
        "# Start processing IPs\n",
        "print(\"Starting IP geolocation processing...\")\n",
        "\n",
        "for idx, ip in enumerate(deduplicated_df['unique_ip_address']):\n",
        "    print(ip)\n",
        "    # Check if this IP has already been processed\n",
        "    if ip not in processed_ips:\n",
        "        # Get location data for unprocessed IPs\n",
        "        print(f\"Processing IP {idx + 1}/{len(deduplicated_df)}: {ip}\")\n",
        "        location_data = get_location_from_ip(ip)\n",
        "        deduplicated_df.at[idx, 'city'] = location_data['city']\n",
        "        deduplicated_df.at[idx, 'region'] = location_data['region']\n",
        "        deduplicated_df.at[idx, 'country'] = location_data['country']\n",
        "        deduplicated_df.at[idx, 'latitude'] = location_data['latitude']\n",
        "        deduplicated_df.at[idx, 'longitude'] = location_data['longitude']\n",
        "\n",
        "        # Add the IP to the processed list\n",
        "        processed_ips.add(ip)\n",
        "\n",
        "        # Log the geolocation\n",
        "        print(f\"Processed {ip}: {location_data['city']}, {location_data['region']}, {location_data['country']}\")\n",
        "\n",
        "    # Wait 10 seconds to avoid hitting rate limits\n",
        "    time.sleep(10)\n",
        "\n",
        "# Save the updated DataFrame back to the original CSV\n",
        "print(\"Saving updated deduplicated IPs to CSV...\")\n",
        "deduplicated_df.to_csv(deduplicated_ips_file, index=False)\n",
        "\n",
        "# Save the processed IPs to the geolocated file\n",
        "save_processed_ips(geolocated_ips_file, processed_ips)\n",
        "\n",
        "print(\"Geolocation data added and saved successfully.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4mBrzp4CbnO",
        "outputId": "6d6b565e-b22b-4227-f584-6b09d187240e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading deduplicated IPs...\n",
            "Loaded 83 deduplicated IPs from /content/spotify-listening-data/data/processed/deduplicated_unique_ips.csv.\n",
            "Loaded 83 previously processed IPs from /content/spotify-listening-data/data/processed/geolocated_ips.csv.\n",
            "Adding location columns to deduplicated IP data...\n",
            "Starting IP geolocation processing...\n",
            "173.79.66.78\n",
            "70.110.22.156\n",
            "108.18.101.195\n",
            "71.246.200.163\n",
            "129.64.121.98\n",
            "129.64.166.4\n",
            "129.64.123.114\n",
            "129.64.155.124\n",
            "129.64.153.87\n",
            "107.127.45.66\n",
            "24.148.79.81\n",
            "107.119.44.1\n",
            "166.194.135.178\n",
            "166.205.140.99\n",
            "71.114.105.10\n",
            "199.195.178.243\n",
            "108.147.92.132\n",
            "2600:387:f:5b17::2\n",
            "2600:387:f:7619::9\n",
            "2600:387:f:6a16::b\n",
            "166.199.152.13\n",
            "96.95.91.5\n",
            "2600:387:f:5b14::4\n",
            "166.194.132.52\n",
            "2600:387:15:2c1b::5\n",
            "2600:387:f:6a17::7\n",
            "2600:387:f:5b13::8\n",
            "2600:387:f:7618::c\n",
            "2600:387:f:6a1b::7\n",
            "107.77.206.136\n",
            "2600:387:f:6a10::4\n",
            "12.74.213.82\n",
            "2600:387:15:2c10::a\n",
            "2601:249:8680:3250:78e4:194:60c:7ff2\n",
            "24.13.173.197\n",
            "2600:387:f:5b19::7\n",
            "2600:387:f:5b10::5\n",
            "2600:387:f:6a18::3\n",
            "2600:387:f:5b15::2\n",
            "2600:387:15:2c11::3\n",
            "2600:387:f:7616::7\n",
            "2600:387:f:7614::2\n",
            "2600:387:f:7612::9\n",
            "2600:387:f:5b11::9\n",
            "184.169.45.4\n",
            "2600:387:f:7813::7\n",
            "2601:19e:4581:b10:f039:690:9ec9:9bce\n",
            "2600:387:f:7611::3\n",
            "2600:387:15:2c18::1\n",
            "2600:387:f:6a11::5\n",
            "2600:387:f:6a1a::4\n",
            "2600:387:f:7615::4\n",
            "2600:387:f:7610::5\n",
            "2600:387:f:5b1b::1\n",
            "2600:387:f:5b1a::8\n",
            "2600:387:f:5b18::3\n",
            "2600:387:15:2c15::2\n",
            "2600:387:15:2c12::a\n",
            "2600:387:f:761a::9\n",
            "2600:387:f:7c12::6\n",
            "2600:6c44:27f:1aea:3004:dde5:bedb:5485\n",
            "2600:387:15:271a::c\n",
            "2600:387:f:5b16::a\n",
            "2600:387:15:2c19::6\n",
            "168.91.221.120\n",
            "2600:387:f:6a13::7\n",
            "2600:387:15:2c14::5\n",
            "2600:387:f:5b12::2\n",
            "50.233.222.124\n",
            "2600:387:f:5a15::4\n",
            "96.241.118.169\n",
            "2600:387:15:2c17::2\n",
            "2600:387:15:2d11::b\n",
            "2600:387:15:2d13::8\n",
            "12.74.238.44\n",
            "2600:387:15:2d14::2\n",
            "2600:387:f:6a12::3\n",
            "2600:387:f:6a14::a\n",
            "2600:387:f:913::6\n",
            "207.104.43.41\n",
            "2600:1700:24f4:7000:15dc:2c67:6667:6d56\n",
            "2600:387:f:d10::8\n",
            "75.104.93.146\n",
            "Saving updated deduplicated IPs to CSV...\n",
            "Saved 83 processed IPs to /content/spotify-listening-data/data/processed/geolocated_ips.csv.\n",
            "Geolocation data added and saved successfully.\n"
          ]
        }
      ]
    }
  ]
}