{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 9886468,
          "sourceType": "datasetVersion",
          "datasetId": 6071248
        }
      ],
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!ls\n",
        "# Change to the cloned repo directory (optional)\n",
        "!git clone \"https://github.com/ezragershman/spotify-listening-data\"\n",
        "%cd '/content/spotify-listening-data'\n",
        "\n",
        "for dirname, _, filenames in os.walk('/content/spotify-listening-data'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T16:37:44.049622Z",
          "iopub.execute_input": "2024-11-12T16:37:44.050110Z",
          "iopub.status.idle": "2024-11-12T16:37:44.082224Z",
          "shell.execute_reply.started": "2024-11-12T16:37:44.050063Z",
          "shell.execute_reply": "2024-11-12T16:37:44.080893Z"
        },
        "id": "tR7kAO397UKR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull --rebase"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocT3SHlA7F3R",
        "outputId": "6ec53c42-1d37-4b8b-f082-bb3501c75a2a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/spotify-listening-data'\n",
        "from google.colab import userdata\n",
        "github_token = userdata.get('github_token')\n",
        "\n",
        "!git config --global user.email \"ezragersh@gmail.com\"\n",
        "!git config --global user.name \"ezragershman\"\n",
        "# Set up Git to use the token for authentication\n",
        "if github_token:\n",
        "    repo_url = f\"https://{github_token}@github.com/ezragershman/spotify-listening-data.git\"  # Replace 'YourGitHubUsername' and 'YourRepository' with your actual GitHub username and repo\n",
        "    !git remote set-url origin {repo_url}\n",
        "    !git add *\n",
        "    !git commit -m \"Spotify_data updated w/song_lookup_table info;\"\n",
        "    !git push origin main\n",
        "else:\n",
        "    print(\"GitHub token is not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfdwU_o685D0",
        "outputId": "ec2506a3-8d3e-4f4a-ef96-7918c486c2b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spotify-listening-data\n",
            "[main 87abb2c] Spotify_data updated w/song_lookup_table info;\n",
            " 1 file changed, 5107 insertions(+), 5107 deletions(-)\n",
            "Enumerating objects: 9, done.\n",
            "Counting objects: 100% (9/9), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (5/5), done.\n",
            "Writing objects: 100% (5/5), 488.29 KiB | 3.34 MiB/s, done.\n",
            "Total 5 (delta 1), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "To https://github.com/ezragershman/spotify-listening-data.git\n",
            "   d9057bd..87abb2c  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAIN FUNCTION TO UPDATE & SAVE MUSIC LIBRARY DATABASE**\n"
      ],
      "metadata": {
        "id": "mDDlLipM7UKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_file_path = \"/content/spotify-listening-data/data/raw/Streaming_History_Audio_2013-2024.json\"\n",
        "json_to_csv(json_file_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T15:40:07.289030Z",
          "iopub.execute_input": "2024-11-12T15:40:07.289607Z",
          "iopub.status.idle": "2024-11-12T15:40:07.580940Z",
          "shell.execute_reply.started": "2024-11-12T15:40:07.289557Z",
          "shell.execute_reply": "2024-11-12T15:40:07.579471Z"
        },
        "id": "DF0iBnKW7UKb",
        "outputId": "ee7dc901-ad23-43d1-8fa5-428252224b7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully updated and saved to /content/spotify-listening-data/data/processed/spotify_data.csv\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import uuid\n",
        "import os\n",
        "import requests\n",
        "\n",
        "def json_to_csv(json_file_path, csv_output_path='/content/spotify-listening-data/data/processed/spotify_data.csv', geolocation_file='/content/spotify-listening-data/data/processed/ipads processing/geolocated_ips.csv'):\n",
        "    # Load the new JSON data into a DataFrame\n",
        "    new_data = pd.read_json(json_file_path)\n",
        "\n",
        "    # Rename columns for consistency\n",
        "    new_data.rename(columns={\n",
        "        'ts': 'end_time_UTC',\n",
        "        'platform': 'platform',\n",
        "        'ms_played': 'ms_played',\n",
        "        'conn_country': 'connection_country',\n",
        "        'ip_addr_decrypted': 'ip_address',\n",
        "        'master_metadata_track_name': 'track_name',\n",
        "        'master_metadata_album_artist_name': 'album_artist_name',\n",
        "        'master_metadata_album_album_name': 'album_name',\n",
        "        'spotify_track_uri': 'track_uri',\n",
        "        'episode_name': 'episode_name',\n",
        "        'episode_show_name': 'episode_show_name',\n",
        "        'spotify_episode_uri': 'episode_uri',\n",
        "        'reason_start': 'start_reason',\n",
        "        'reason_end': 'end_reason',\n",
        "        'shuffle': 'shuffle',\n",
        "        'skipped': 'skipped',\n",
        "    }, inplace=True)\n",
        "\n",
        "    # Load the geolocation data into a DataFrame (assuming it contains the relevant information)\n",
        "    geolocated_ips_df = pd.read_csv(geolocation_file)\n",
        "\n",
        "    # Merge the new data with the geolocation data based on the 'ip_address'\n",
        "    # This will add columns like 'city', 'region', 'country', 'latitude', 'longitude', 'local_time_zone', and 'utc_offset'\n",
        "    new_data = new_data.merge(\n",
        "       geolocated_ips_df[['unique_ip_address', 'city', 'region', 'country', 'latitude', 'longitude', 'local_time_zone', 'utc_offset']],\n",
        "       left_on='ip_address',\n",
        "       right_on='unique_ip_address',\n",
        "       how='left'\n",
        "       )\n",
        "\n",
        "    # Generate a unique ID for each row\n",
        "    new_data['unique_id'] = new_data['end_time_UTC'].apply(lambda x: f\"{x}_{uuid.uuid4().hex[:8]}\")\n",
        "\n",
        "    # Keep only the relevant columns for the CSV\n",
        "    new_data = new_data[['unique_id', 'end_time_UTC', 'platform', 'ms_played', 'connection_country',\n",
        "                        'ip_address', 'track_name', 'album_artist_name', 'album_name',\n",
        "                        'track_uri', 'episode_name', 'episode_show_name', 'episode_uri',\n",
        "                        'start_reason', 'end_reason', 'shuffle', 'skipped',\n",
        "                        'city', 'region', 'country', 'latitude', 'longitude',\n",
        "                        'local_time_zone', 'utc_offset']]  # Include geolocation and timezone columns\n",
        "\n",
        "    # If the output CSV already exists, read it to check for duplicates\n",
        "    if os.path.exists(csv_output_path):\n",
        "        existing_data = pd.read_csv(csv_output_path)\n",
        "\n",
        "        # Identify and keep only new rows that are not already in the CSV\n",
        "        merged_data = pd.concat([existing_data, new_data]).drop_duplicates(subset=['unique_id'], keep='first')\n",
        "    else:\n",
        "        # If no existing CSV, just use new data\n",
        "        merged_data = new_data\n",
        "\n",
        "    # Ensure the 'processed' directory exists (create it if not)\n",
        "    os.makedirs(os.path.dirname(csv_output_path), exist_ok=True)\n",
        "\n",
        "    # Save updated data to CSV in the specified path\n",
        "    merged_data.to_csv(csv_output_path, index=False)\n",
        "\n",
        "    print(f\"Data successfully updated and saved to {csv_output_path}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-12T16:16:52.328701Z",
          "iopub.execute_input": "2024-11-12T16:16:52.329098Z",
          "iopub.status.idle": "2024-11-12T16:16:52.340326Z",
          "shell.execute_reply.started": "2024-11-12T16:16:52.329059Z",
          "shell.execute_reply": "2024-11-12T16:16:52.339023Z"
        },
        "id": "Ihlqqxdk7UKc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "from google.colab import userdata\n",
        "from base64 import b64encode\n",
        "\n",
        "# Step 1: Set up Spotify credentials as environment variables in Google Colab\n",
        "CLIENT_ID = userdata.get('spotify_client_id')\n",
        "CLIENT_SECRET = userdata.get('spotify_client_sec')\n",
        "\n",
        "# Step 1: Get access token from Spotify API\n",
        "def get_access_token(client_id, client_secret):\n",
        "    \"\"\"Fetches an access token from the Spotify API using Client Credentials Flow.\"\"\"\n",
        "    print(\"Requesting access token...\")\n",
        "    auth_header = b64encode(f\"{client_id}:{client_secret}\".encode('utf-8')).decode('utf-8')\n",
        "    headers = {\n",
        "        'Authorization': f'Basic {auth_header}',\n",
        "        'Content-Type': 'application/x-www-form-urlencoded'\n",
        "    }\n",
        "    data = {'grant_type': 'client_credentials'}\n",
        "\n",
        "    response = requests.post('https://accounts.spotify.com/api/token', headers=headers, data=data)\n",
        "    if response.status_code == 200:\n",
        "        print(\"Access token obtained successfully.\")\n",
        "        return response.json()['access_token']\n",
        "    else:\n",
        "        print(\"Failed to get access token\")\n",
        "        print(response.json())  # Debugging output for errors\n",
        "        return None\n",
        "\n",
        "# Step 2: Fetch song details from Spotify API\n",
        "def get_song_details(track_uri, access_token):\n",
        "    \"\"\"Fetches song details for a given track URI using the Spotify API.\"\"\"\n",
        "    print(f\"Fetching details for track URI: {track_uri}...\")\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {access_token}',\n",
        "    }\n",
        "    track_id = track_uri.split(':')[-1]  # Extract track ID from URI\n",
        "    url = f'https://api.spotify.com/v1/tracks/{track_id}'\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        track_info = response.json()\n",
        "        print(f\"Details for {track_uri} fetched successfully.\")\n",
        "        return {\n",
        "            'song_duration_ms': track_info['duration_ms'],\n",
        "            'album_type': track_info['album']['album_type'],\n",
        "            'explicit': track_info['explicit'],\n",
        "            'available_markets': track_info['available_markets']\n",
        "        }\n",
        "    elif response.status_code == 429:\n",
        "        # Handle rate limits\n",
        "        retry_after = int(response.headers.get('Retry-After', 1))\n",
        "        print(f\"Rate limited. Waiting for {retry_after} seconds...\")\n",
        "        time.sleep(retry_after)\n",
        "        return None\n",
        "    else:\n",
        "        print(f\"Error fetching data for {track_uri}: {response.status_code}\")\n",
        "        print(response.text)  # Detailed error logging\n",
        "        return None\n",
        "\n",
        "# Step 3: Load or create a lookup table\n",
        "def load_lookup_table(file_path):\n",
        "    \"\"\"Loads an existing lookup table or creates a new one if not found.\"\"\"\n",
        "    try:\n",
        "        print(f\"Loading lookup table from {file_path}...\")\n",
        "        return pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Lookup table not found, creating a new one.\")\n",
        "        return pd.DataFrame(columns=['track_uri', 'song_duration_ms', 'album_type', 'explicit', 'available_markets'])\n",
        "\n",
        "# Step 4: Update the lookup table\n",
        "def update_lookup_table(lookup_table, track_uri, access_token):\n",
        "    \"\"\"Fetches and updates song details if not already present in the lookup table.\"\"\"\n",
        "    if track_uri in lookup_table['track_uri'].values:\n",
        "        print(f\"Track {track_uri} found in lookup table.\")\n",
        "        song_details = lookup_table[lookup_table['track_uri'] == track_uri].iloc[0].to_dict()\n",
        "    else:\n",
        "        print(f\"Track {track_uri} not found in lookup table, fetching details...\")\n",
        "        song_details = get_song_details(track_uri, access_token)\n",
        "        if song_details:\n",
        "            song_details['track_uri'] = track_uri\n",
        "            lookup_table = pd.concat([lookup_table, pd.DataFrame([song_details])], ignore_index=True)\n",
        "        else:\n",
        "            song_details = None\n",
        "    return song_details, lookup_table\n",
        "\n",
        "# Step 5: Fetch song details for only missing URIs\n",
        "def fetch_song_details(data, lookup_table, save_interval=10):\n",
        "    \"\"\"Fetches song details for all URIs in the provided data.\"\"\"\n",
        "    print(\"Fetching song details for all URIs in the data...\")\n",
        "    access_token = get_access_token(CLIENT_ID, CLIENT_SECRET)\n",
        "    if not access_token:\n",
        "        print(\"No access token obtained, returning original data.\")\n",
        "        return data, lookup_table\n",
        "\n",
        "    song_details = []\n",
        "    total_rows = len(data['track_uri'].dropna())\n",
        "\n",
        "    # Get track URIs that are in data but not in the lookup table\n",
        "    existing_uris = lookup_table['track_uri'].dropna().unique()  # Track URIs already in lookup table\n",
        "    missing_uris = data['track_uri'].dropna().unique()  # All track URIs in the data\n",
        "    missing_uris = [uri for uri in missing_uris if uri not in existing_uris]  # Find missing ones\n",
        "\n",
        "    print(f\"Found {len(missing_uris)} missing tracks to fetch details for.\")\n",
        "\n",
        "    for idx, track_uri in enumerate(missing_uris):  # Only iterate over missing URIs\n",
        "        print(f\"Processing track {idx+1}/{len(missing_uris)} with URI: {track_uri}\")\n",
        "        details, lookup_table = update_lookup_table(lookup_table, track_uri, access_token)\n",
        "        song_details.append(details)\n",
        "\n",
        "        if (idx + 1) % save_interval == 0:\n",
        "            print(f\"Saving progress after processing {idx+1} tracks...\")\n",
        "            valid_song_details = [details for details in song_details if details is not None]  # Filter out None values\n",
        "\n",
        "            if valid_song_details:\n",
        "                details_df = pd.DataFrame(valid_song_details).drop_duplicates(subset='track_uri')  # Remove any duplicates\n",
        "                details_df = details_df.set_index('track_uri')  # Set index for merging\n",
        "\n",
        "                # Update the main DataFrame, dropping duplicates before setting index\n",
        "                data = data.drop_duplicates(subset=['track_uri']).set_index('track_uri')\n",
        "                data.update(details_df)\n",
        "                data = data.reset_index()  # Restore the index\n",
        "\n",
        "            # Save intermediate results to CSV\n",
        "            data.to_csv(file_path, index=False)\n",
        "            lookup_table.to_csv(lookup_file_path, index=False)\n",
        "            print(f\"Progress saved after {idx+1} tracks.\")\n",
        "\n",
        "            song_details = []  # Clear list after saving\n",
        "            time.sleep(1)  # Slow down to prevent rate limiting\n",
        "\n",
        "    print(\"All song details added successfully.\")\n",
        "    return data, lookup_table\n",
        "\n",
        "\n",
        "\n",
        "# Step 6: Load existing data and lookup table\n",
        "file_path = '/content/spotify-listening-data/data/processed/spotify_data.csv'\n",
        "lookup_file_path = '/content/spotify-listening-data/data/processed/song_lookup_table.csv'\n",
        "\n",
        "print(f\"Loading data from {file_path}...\")\n",
        "spotify_data = pd.read_csv(file_path)\n",
        "print(f\"Loaded {len(spotify_data)} rows of data.\")\n",
        "\n",
        "lookup_table = load_lookup_table(lookup_file_path)\n",
        "\n",
        "# Step 7: Fetch and update song details\n",
        "spotify_data, lookup_table = fetch_song_details(spotify_data, lookup_table, save_interval=10)\n",
        "\n",
        "# Step 8: Save final results\n",
        "spotify_data.to_csv(file_path, index=False)\n",
        "lookup_table.to_csv(lookup_file_path, index=False)\n",
        "print(f\"Updated data saved to {file_path}.\")\n",
        "print(f\"Updated lookup table saved to {lookup_file_path}.\")\n",
        "print(\"Process completed successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "MAqTCxH4qPyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load the CSV files and inspect their columns\n",
        "lookup_file_path = '/content/spotify-listening-data/data/processed/song_lookup_table.csv'\n",
        "lookup_table = pd.read_csv(lookup_file_path)\n",
        "\n",
        "spotify_data_file_path = '/content/spotify-listening-data/data/processed/spotify_data.csv'\n",
        "spotify_data = pd.read_csv(spotify_data_file_path)\n",
        "\n",
        "# Step 2: Check columns before merging (for verification)\n",
        "print(\"Columns in spotify_data:\")\n",
        "print(spotify_data.columns)\n",
        "print(\"\\nColumns in lookup_table:\")\n",
        "print(lookup_table.columns)\n",
        "\n",
        "# Step 3: Perform the merge on 'track_uri' without dropping the key column\n",
        "merged_data = pd.merge(spotify_data, lookup_table, on='track_uri', how='left')\n",
        "\n",
        "# Step 4: Print a preview of the merged data\n",
        "print(\"\\nPreview of merged data (after merging):\")\n",
        "print(merged_data.head())\n",
        "\n",
        "# Step 5: Save the merged data back to the original CSV file\n",
        "merged_data.to_csv(spotify_data_file_path, index=False)\n",
        "print(f\"Merged data has been saved to {spotify_data_file_path}.\")"
      ],
      "metadata": {
        "id": "qiY1mwJomyUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spotify_data_path = '/content/spotify-listening-data/data/processed/spotify_data.csv'\n",
        "spotify_df = pd.read_csv(spotify_data_path)\n",
        "\n",
        "spotify_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88HWpya3ou6n",
        "outputId": "cf16f1eb-536a-4505-c9b9-c0b7160a4f15"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['track_uri', 'unique_id', 'end_time_UTC', 'platform', 'ms_played',\n",
              "       'connection_country', 'ip_address', 'track_name', 'album_artist_name',\n",
              "       'album_name', 'episode_name', 'episode_show_name', 'episode_uri',\n",
              "       'start_reason', 'end_reason', 'shuffle', 'skipped', 'city', 'region',\n",
              "       'country', 'latitude', 'longitude', 'local_time_zone', 'utc_offset',\n",
              "       'song_duration_ms_x', 'album_type_x', 'explicit_x',\n",
              "       'available_markets_x', 'song_duration_ms_y', 'album_type_y',\n",
              "       'explicit_y', 'available_markets_y'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IP Address Information Scraper**"
      ],
      "metadata": {
        "id": "2ErJcgHNSx67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Create table for unique ID information.\n",
        "\n",
        "def extract_unique_ips(json_file_path, output_csv_path='/content/spotify-listening-data/data/processed/ipads processing/unique_ips.csv'):\n",
        "    # Load the JSON data into a DataFrame\n",
        "    data = pd.read_json(json_file_path)\n",
        "\n",
        "    # Extract the IP address column (adjust column name based on your data)\n",
        "    ip_addresses = data['ip_addr_decrypted']  # Ensure 'ip_addr_decrypted' is the correct column name\n",
        "\n",
        "    # Get the unique IP addresses\n",
        "    unique_ips = ip_addresses.dropna().unique()\n",
        "\n",
        "    # Create a DataFrame from the unique IP addresses\n",
        "    unique_ips_df = pd.DataFrame(unique_ips, columns=['unique_ip_address'])\n",
        "\n",
        "    # Save the unique IPs to a CSV file\n",
        "    unique_ips_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "    print(f\"Unique IP addresses extracted and saved to {output_csv_path}\")\n",
        "\n",
        "# Example usage\n",
        "json_file_path = '/content/spotify-listening-data/data/raw/Streaming_History_Audio_2013-2024.json'\n",
        "extract_unique_ips(json_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MYkRh1V73qy",
        "outputId": "04bc4596-c796-4eef-bbff-64844f944c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique IP addresses extracted and saved to /content/spotify-listening-data/data/processed/unique_ips.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from ipaddress import ip_network, ip_address\n",
        "\n",
        "# Load IP data into DataFrame (assuming your list is saved as 'unique_ips.csv')\n",
        "df = pd.read_csv('/content/spotify-listening-data/data/processed/ipads processing/unique_ips.csv')\n",
        "\n",
        "def group_by_subnet(ip):\n",
        "    try:\n",
        "        # If it's an IPv4 address\n",
        "        ip_obj = ip_address(ip)\n",
        "        if ip_obj.version == 4:\n",
        "            return f\"{ip_obj.exploded.split('.')[0]}.{ip_obj.exploded.split('.')[1]}.{ip_obj.exploded.split('.')[2]}.0/24\"\n",
        "        else:\n",
        "            # For IPv6, return the first few blocks (customizable)\n",
        "            return ':'.join(ip.split(':')[:4]) + '::/64'\n",
        "    except ValueError:\n",
        "        return 'Invalid IP'\n",
        "\n",
        "# Apply subnet grouping\n",
        "df['subnet'] = df['unique_ip_address'].apply(group_by_subnet)\n",
        "\n",
        "# Drop duplicates based on subnets, keeping one representative IP\n",
        "deduplicated_ip = df.drop_duplicates(subset='subnet')\n",
        "\n",
        "# Save the deduplicated DataFrame as a CSV file\n",
        "deduplicated_ip.to_csv('/content/spotify-listening-data/data/processed/ipads processing/deduplicated_unique_ips.csv', index=False)"
      ],
      "metadata": {
        "id": "g3pEI2SBA-lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from ip2geotools.databases.noncommercial import DbIpCity\n",
        "\n",
        "# Load previously processed IPs from CSV if it exists\n",
        "def load_processed_ips(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Loaded {len(df)} previously processed IPs from {file_path}.\")\n",
        "        return set(df['unique_ip_address'].tolist())  # Assuming the CSV has a column named 'unique_ip_address'\n",
        "    except FileNotFoundError:\n",
        "        print(f\"No previous geolocated IPs found. Starting fresh.\")\n",
        "        return set()\n",
        "\n",
        "# Save new processed IPs to the CSV\n",
        "def save_processed_ips(file_path, processed_ips):\n",
        "    df = pd.DataFrame(list(processed_ips), columns=['unique_ip_address'])\n",
        "    df.to_csv(file_path, index=False)\n",
        "    print(f\"Saved {len(processed_ips)} processed IPs to {file_path}.\")\n",
        "\n",
        "# Path to the CSV files\n",
        "deduplicated_ips_file = '/content/spotify-listening-data/data/processed/ipads processing/deduplicated_unique_ips.csv'\n",
        "geolocated_ips_file = '/content/spotify-listening-data/data/processed/ipads processing/geolocated_ips.csv'\n",
        "\n",
        "# Load the IPs\n",
        "print(\"Loading deduplicated IPs...\")\n",
        "deduplicated_df = pd.read_csv(deduplicated_ips_file)\n",
        "print(f\"Loaded {len(deduplicated_df)} deduplicated IPs from {deduplicated_ips_file}.\")\n",
        "\n",
        "processed_ips = load_processed_ips(geolocated_ips_file)\n",
        "\n",
        "# Function to get location details from IP address\n",
        "def get_location_from_ip(ip_address):\n",
        "    try:\n",
        "        # Query the ip2geotools database with the given IP address\n",
        "        res = DbIpCity.get(ip_address, api_key=\"free\")\n",
        "\n",
        "        # Structure the location data\n",
        "        location = {\n",
        "            \"city\": res.city,\n",
        "            \"region\": res.region,\n",
        "            \"country\": res.country,\n",
        "            \"latitude\": res.latitude,\n",
        "            \"longitude\": res.longitude\n",
        "        }\n",
        "        return location\n",
        "    except Exception as e:\n",
        "        # Handle any potential errors\n",
        "        return {\n",
        "            \"city\": None,\n",
        "            \"region\": None,\n",
        "            \"country\": None,\n",
        "            \"latitude\": None,\n",
        "            \"longitude\": None,\n",
        "            \"error\": f\"Error: {str(e)}\"\n",
        "        }\n",
        "\n",
        "# Add columns to store location data if not already present\n",
        "if 'city' not in deduplicated_df.columns:\n",
        "    print(\"Adding location columns to deduplicated IP data...\")\n",
        "    deduplicated_df['city'] = None\n",
        "    deduplicated_df['region'] = None\n",
        "    deduplicated_df['country'] = None\n",
        "    deduplicated_df['latitude'] = None\n",
        "    deduplicated_df['longitude'] = None\n",
        "\n",
        "# Start processing IPs\n",
        "print(\"Starting IP geolocation processing...\")\n",
        "\n",
        "new_geolocated_data = []  # To store new geolocated IP data\n",
        "\n",
        "for idx, ip in enumerate(deduplicated_df['unique_ip_address']):\n",
        "    print(ip)\n",
        "    # Check if this IP has already been processed\n",
        "    if ip not in processed_ips:\n",
        "        # Get location data for unprocessed IPs\n",
        "        print(f\"Processing IP {idx + 1}/{len(deduplicated_df)}: {ip}\")\n",
        "        location_data = get_location_from_ip(ip)\n",
        "        deduplicated_df.at[idx, 'city'] = location_data['city']\n",
        "        deduplicated_df.at[idx, 'region'] = location_data['region']\n",
        "        deduplicated_df.at[idx, 'country'] = location_data['country']\n",
        "        deduplicated_df.at[idx, 'latitude'] = location_data['latitude']\n",
        "        deduplicated_df.at[idx, 'longitude'] = location_data['longitude']\n",
        "\n",
        "        # Add the IP to the processed list\n",
        "        processed_ips.add(ip)\n",
        "\n",
        "        # Store new geolocated data\n",
        "        new_geolocated_data.append({\n",
        "            \"unique_ip_address\": ip,\n",
        "            \"city\": location_data['city'],\n",
        "            \"region\": location_data['region'],\n",
        "            \"country\": location_data['country'],\n",
        "            \"latitude\": location_data['latitude'],\n",
        "            \"longitude\": location_data['longitude']\n",
        "        })\n",
        "\n",
        "        # Log the geolocation\n",
        "        print(f\"Processed {ip}: {location_data['city']}, {location_data['region']}, {location_data['country']}\")\n",
        "\n",
        "    # Wait 10 seconds to avoid hitting rate limits\n",
        "    time.sleep(10)\n",
        "\n",
        "# Save the updated deduplicated DataFrame back to the original CSV\n",
        "print(\"Saving updated deduplicated IPs to CSV...\")\n",
        "deduplicated_df.to_csv(deduplicated_ips_file, index=False)\n",
        "\n",
        "# Save the processed IPs to the geolocated file\n",
        "if new_geolocated_data:\n",
        "    new_geolocated_df = pd.DataFrame(new_geolocated_data)\n",
        "    new_geolocated_df.to_csv(geolocated_ips_file, index=False)\n",
        "\n",
        "# Save the processed IPs to the geolocated file\n",
        "save_processed_ips(geolocated_ips_file, processed_ips)\n",
        "\n",
        "print(\"Geolocation data added and saved successfully.\")\n"
      ],
      "metadata": {
        "id": "t4mBrzp4CbnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To remove the time zone and UTC offset columns, if needed\n",
        "geolocated_ips.drop(columns=['local_time_zone', 'utc_offset'], inplace=True)"
      ],
      "metadata": {
        "id": "5YJG1LgNZ081"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from geopy.geocoders import Nominatim\n",
        "from timezonefinder import TimezoneFinder\n",
        "from pytz import timezone\n",
        "from datetime import datetime\n",
        "\n",
        "# Function to get timezone and UTC offset for a given city and country\n",
        "def get_timezone_and_offset(city, country, region=None):\n",
        "    geolocator = Nominatim(user_agent=\"timezone_lookup\")\n",
        "\n",
        "    # First attempt: geocode using city and country\n",
        "    location = geolocator.geocode(f\"{city}, {country}\", timeout=10)\n",
        "\n",
        "    if location:\n",
        "        latitude = location.latitude\n",
        "        longitude = location.longitude\n",
        "        tz_finder = TimezoneFinder()\n",
        "        tz_name = tz_finder.timezone_at(lng=longitude, lat=latitude)\n",
        "\n",
        "        if tz_name:\n",
        "            tz = timezone(tz_name)\n",
        "            utc_offset = tz.utcoffset(datetime.now()).total_seconds() / 3600  # in hours\n",
        "            return tz_name, utc_offset\n",
        "        else:\n",
        "            return 'Unknown', None\n",
        "\n",
        "    else:\n",
        "        # Second attempt: geocode using region if city-based lookup fails\n",
        "        if region:\n",
        "            location = geolocator.geocode(f\"{region}, {country}\", timeout=10)\n",
        "\n",
        "            if location:\n",
        "                latitude = location.latitude\n",
        "                longitude = location.longitude\n",
        "                tz_finder = TimezoneFinder()\n",
        "                tz_name = tz_finder.timezone_at(lng=longitude, lat=latitude)\n",
        "\n",
        "                if tz_name:\n",
        "                    tz = timezone(tz_name)\n",
        "                    utc_offset = tz.utcoffset(datetime.now()).total_seconds() / 3600  # in hours\n",
        "                    return tz_name, utc_offset\n",
        "                else:\n",
        "                    return 'Unknown', None\n",
        "        return 'Unknown', None  # Return 'Unknown' if both lookups fail\n",
        "\n",
        "# Apply the function to your DataFrame\n",
        "def add_time_zone_info(df):\n",
        "    df[['local_time_zone', 'utc_offset']] = df.apply(\n",
        "        lambda row: pd.Series(get_timezone_and_offset(row['city'], row['country'], row['region'])),\n",
        "        axis=1\n",
        "    )\n",
        "    return df\n",
        "\n",
        "# Load the data\n",
        "geolocated_ips = pd.read_csv('/content/spotify-listening-data/data/processed/ipads processing/geolocated_ips.csv')\n",
        "\n",
        "# Apply the function to add time zone information\n",
        "geolocated_ips = add_time_zone_info(geolocated_ips)\n",
        "\n",
        "# Export the updated file\n",
        "geolocated_ips.to_csv('/content/spotify-listening-data/data/processed/ipads processing/geolocated_ips.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "7iVw3f_XcsXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA ANALYSIS AND MANIPULATION**:"
      ],
      "metadata": {
        "id": "8uVMsLCyCxTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Spotify listening data\n",
        "spotify_data_path = '/content/spotify-listening-data/data/processed/spotify_data.csv'\n",
        "spotify_df = pd.read_csv(spotify_data_path)\n",
        "\n",
        "# Display the first few rows and summary of the data\n",
        "print(\"First few rows of the data:\")\n",
        "print(spotify_df.head())\n",
        "\n",
        "print(\"\\nData structure (columns and data types):\")\n",
        "print(spotify_df.info())\n",
        "\n",
        "print(\"\\nSummary statistics:\")\n",
        "print(spotify_df.describe(include='all'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OId5bfZtC-Mf",
        "outputId": "2a107074-73df-400d-eb19-f7948f826939"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the data:\n",
            "                              track_uri                      unique_id  \\\n",
            "0                                   NaN  2013-10-13T15:15:10Z_6cdee161   \n",
            "1  spotify:track:7p7chfq7OqmGmpwWWnPz0p  2013-10-13T15:15:26Z_082a86db   \n",
            "2  spotify:track:5cwAwML79FCaUyPEpEmrIh  2013-10-13T15:15:28Z_b922befa   \n",
            "3  spotify:track:2XjZtWBjcvYtVxcXPVoEHi  2013-10-13T15:15:29Z_955600b9   \n",
            "4  spotify:track:4AJY2aMfSaJip3o1JJCRCs  2013-10-13T15:15:35Z_88ef0788   \n",
            "\n",
            "           end_time_UTC             platform  ms_played connection_country  \\\n",
            "0  2013-10-13T15:15:10Z  OS X 10.8.4 [x86 4]      39381                 US   \n",
            "1  2013-10-13T15:15:26Z  OS X 10.8.4 [x86 4]      16427                 US   \n",
            "2  2013-10-13T15:15:28Z  OS X 10.8.4 [x86 4]       3204                 US   \n",
            "3  2013-10-13T15:15:29Z  OS X 10.8.4 [x86 4]       2136                 US   \n",
            "4  2013-10-13T15:15:35Z  OS X 10.8.4 [x86 4]       5313                 US   \n",
            "\n",
            "     ip_address      track_name         album_artist_name  \\\n",
            "0  173.79.66.78             NaN                       NaN   \n",
            "1  173.79.66.78  Lost and Found               sasakure.UK   \n",
            "2  173.79.66.78        Koukasen                ELLEGARDEN   \n",
            "3  173.79.66.78            ARIA                  Kalafina   \n",
            "4  173.79.66.78    Pray for you  SOSOSO (Tsukui Kazuhito)   \n",
            "\n",
            "                       album_name  ...   local_time_zone  utc_offset  \\\n",
            "0                             NaN  ...  America/New_York        -5.0   \n",
            "1  The Fantastic Reality of Aesop  ...  America/New_York        -5.0   \n",
            "2            Eleven Fire Crackers  ...  America/New_York        -5.0   \n",
            "3                  Seventh Heaven  ...  America/New_York        -5.0   \n",
            "4         GROW feat. Hatsune Miku  ...  America/New_York        -5.0   \n",
            "\n",
            "   song_duration_ms_x album_type_x explicit_x  available_markets_x  \\\n",
            "0                 NaN          NaN        NaN                  NaN   \n",
            "1            300920.0        album      False                   []   \n",
            "2            145053.0        album      False                   []   \n",
            "3            323946.0        album      False                   []   \n",
            "4            290000.0       single      False                   []   \n",
            "\n",
            "   song_duration_ms_y album_type_y explicit_y available_markets_y  \n",
            "0                 NaN          NaN        NaN                 NaN  \n",
            "1            300920.0        album      False                  []  \n",
            "2            145053.0        album      False                  []  \n",
            "3            323946.0        album      False                  []  \n",
            "4            290000.0       single      False                  []  \n",
            "\n",
            "[5 rows x 32 columns]\n",
            "\n",
            "Data structure (columns and data types):\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5106 entries, 0 to 5105\n",
            "Data columns (total 32 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   track_uri            5105 non-null   object \n",
            " 1   unique_id            5106 non-null   object \n",
            " 2   end_time_UTC         5106 non-null   object \n",
            " 3   platform             5106 non-null   object \n",
            " 4   ms_played            5106 non-null   int64  \n",
            " 5   connection_country   5106 non-null   object \n",
            " 6   ip_address           5106 non-null   object \n",
            " 7   track_name           5105 non-null   object \n",
            " 8   album_artist_name    5105 non-null   object \n",
            " 9   album_name           5105 non-null   object \n",
            " 10  episode_name         0 non-null      float64\n",
            " 11  episode_show_name    0 non-null      float64\n",
            " 12  episode_uri          0 non-null      float64\n",
            " 13  start_reason         5106 non-null   object \n",
            " 14  end_reason           5106 non-null   object \n",
            " 15  shuffle              5106 non-null   bool   \n",
            " 16  skipped              5088 non-null   float64\n",
            " 17  city                 4045 non-null   object \n",
            " 18  region               4045 non-null   object \n",
            " 19  country              4045 non-null   object \n",
            " 20  latitude             1934 non-null   float64\n",
            " 21  longitude            1934 non-null   float64\n",
            " 22  local_time_zone      4045 non-null   object \n",
            " 23  utc_offset           4045 non-null   float64\n",
            " 24  song_duration_ms_x   5105 non-null   float64\n",
            " 25  album_type_x         5105 non-null   object \n",
            " 26  explicit_x           5105 non-null   object \n",
            " 27  available_markets_x  5105 non-null   object \n",
            " 28  song_duration_ms_y   5105 non-null   float64\n",
            " 29  album_type_y         5105 non-null   object \n",
            " 30  explicit_y           5105 non-null   object \n",
            " 31  available_markets_y  5105 non-null   object \n",
            "dtypes: bool(1), float64(9), int64(1), object(21)\n",
            "memory usage: 1.2+ MB\n",
            "None\n",
            "\n",
            "Summary statistics:\n",
            "                                   track_uri                      unique_id  \\\n",
            "count                                   5105                           5106   \n",
            "unique                                  5105                           5106   \n",
            "top     spotify:track:7p7chfq7OqmGmpwWWnPz0p  2013-10-13T15:15:10Z_6cdee161   \n",
            "freq                                       1                              1   \n",
            "mean                                     NaN                            NaN   \n",
            "std                                      NaN                            NaN   \n",
            "min                                      NaN                            NaN   \n",
            "25%                                      NaN                            NaN   \n",
            "50%                                      NaN                            NaN   \n",
            "75%                                      NaN                            NaN   \n",
            "max                                      NaN                            NaN   \n",
            "\n",
            "                end_time_UTC platform      ms_played connection_country  \\\n",
            "count                   5106     5106    5106.000000               5106   \n",
            "unique                  5029       12            NaN                  2   \n",
            "top     2024-06-10T17:41:05Z      ios            NaN                 US   \n",
            "freq                       6     4362            NaN               5104   \n",
            "mean                     NaN      NaN   72396.831767                NaN   \n",
            "std                      NaN      NaN   92204.540763                NaN   \n",
            "min                      NaN      NaN       0.000000                NaN   \n",
            "25%                      NaN      NaN    2661.500000                NaN   \n",
            "50%                      NaN      NaN   16561.000000                NaN   \n",
            "75%                      NaN      NaN  141048.000000                NaN   \n",
            "max                      NaN      NaN  559000.000000                NaN   \n",
            "\n",
            "             ip_address track_name         album_artist_name  \\\n",
            "count              5106       5105                      5105   \n",
            "unique              162       4686                      1805   \n",
            "top     199.195.178.243      事件発生！  Crazy Ex-Girlfriend Cast   \n",
            "freq               1289          6                       242   \n",
            "mean                NaN        NaN                       NaN   \n",
            "std                 NaN        NaN                       NaN   \n",
            "min                 NaN        NaN                       NaN   \n",
            "25%                 NaN        NaN                       NaN   \n",
            "50%                 NaN        NaN                       NaN   \n",
            "75%                 NaN        NaN                       NaN   \n",
            "max                 NaN        NaN                       NaN   \n",
            "\n",
            "                                               album_name  ...  \\\n",
            "count                                                5105  ...   \n",
            "unique                                               2682  ...   \n",
            "top     Crazy Ex-Girlfriend: Season 1 (Original Televi...  ...   \n",
            "freq                                                  101  ...   \n",
            "mean                                                  NaN  ...   \n",
            "std                                                   NaN  ...   \n",
            "min                                                   NaN  ...   \n",
            "25%                                                   NaN  ...   \n",
            "50%                                                   NaN  ...   \n",
            "75%                                                   NaN  ...   \n",
            "max                                                   NaN  ...   \n",
            "\n",
            "        local_time_zone   utc_offset  song_duration_ms_x album_type_x  \\\n",
            "count              4045  4045.000000        5.105000e+03         5105   \n",
            "unique                5          NaN                 NaN            3   \n",
            "top     America/Chicago          NaN                 NaN        album   \n",
            "freq               1693          NaN                 NaN         3205   \n",
            "mean                NaN    -6.411619        1.974907e+05          NaN   \n",
            "std                 NaN     1.185518        7.909187e+04          NaN   \n",
            "min                 NaN    -8.000000        0.000000e+00          NaN   \n",
            "25%                 NaN    -8.000000        1.536930e+05          NaN   \n",
            "50%                 NaN    -6.000000        2.009460e+05          NaN   \n",
            "75%                 NaN    -5.000000        2.365810e+05          NaN   \n",
            "max                 NaN    -5.000000        1.199093e+06          NaN   \n",
            "\n",
            "       explicit_x                                available_markets_x  \\\n",
            "count        5105                                               5105   \n",
            "unique          2                                                289   \n",
            "top         False  ['AR', 'AU', 'AT', 'BE', 'BO', 'BR', 'BG', 'CA...   \n",
            "freq         4531                                               1941   \n",
            "mean          NaN                                                NaN   \n",
            "std           NaN                                                NaN   \n",
            "min           NaN                                                NaN   \n",
            "25%           NaN                                                NaN   \n",
            "50%           NaN                                                NaN   \n",
            "75%           NaN                                                NaN   \n",
            "max           NaN                                                NaN   \n",
            "\n",
            "        song_duration_ms_y album_type_y explicit_y  \\\n",
            "count         5.105000e+03         5105       5105   \n",
            "unique                 NaN            3          2   \n",
            "top                    NaN        album      False   \n",
            "freq                   NaN         3205       4531   \n",
            "mean          1.974907e+05          NaN        NaN   \n",
            "std           7.909187e+04          NaN        NaN   \n",
            "min           0.000000e+00          NaN        NaN   \n",
            "25%           1.536930e+05          NaN        NaN   \n",
            "50%           2.009460e+05          NaN        NaN   \n",
            "75%           2.365810e+05          NaN        NaN   \n",
            "max           1.199093e+06          NaN        NaN   \n",
            "\n",
            "                                      available_markets_y  \n",
            "count                                                5105  \n",
            "unique                                                289  \n",
            "top     ['AR', 'AU', 'AT', 'BE', 'BO', 'BR', 'BG', 'CA...  \n",
            "freq                                                 1941  \n",
            "mean                                                  NaN  \n",
            "std                                                   NaN  \n",
            "min                                                   NaN  \n",
            "25%                                                   NaN  \n",
            "50%                                                   NaN  \n",
            "75%                                                   NaN  \n",
            "max                                                   NaN  \n",
            "\n",
            "[11 rows x 32 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Listening Trends Over Time:**\n",
        "Analyze Spotify listening trends over different time periods (monthly, daily, and weekly).\n",
        "\n",
        "## Data Preparation:\n",
        "### Step 1: Load and Parse Date-Time Data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dc47RgjjFsKd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "ArSYZeU9TmWE",
        "outputId": "75e3d61b-1661-405e-be25-99661470f7df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-fccfce62decb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mspotify_data_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/spotify-listening-data/data/processed/spotify_data.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspotify_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspotify_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# List of columns to drop (those that have '_x' and '_y')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcolumns_to_drop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspotify_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'_x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pytz\n",
        "from datetime import timedelta\n",
        "\n",
        "# Step 1: Load the data (replace with actual data loading process)\n",
        "# Assuming the data is loaded from a CSV or another source\n",
        "new_data = pd.read_csv('/content/spotify-listening-data/data/processed/spotify_data.csv')\n",
        "\n",
        "# Step 2: Convert 'end_time_UTC' to datetime\n",
        "new_data['end_time_UTC'] = pd.to_datetime(new_data['end_time_UTC'])\n",
        "\n",
        "# Step 3: Convert UTC to Local Time\n",
        "# We'll adjust the 'end_time_UTC' column to local time using the 'utc_offset' column.\n",
        "\n",
        "# Function to convert from UTC to local time\n",
        "def convert_to_local_time(row):\n",
        "    # If 'utc_offset' is NaN, return the original UTC time (or handle as appropriate)\n",
        "    if pd.isna(row['utc_offset']):\n",
        "        return row['end_time_UTC']\n",
        "\n",
        "    utc_time = row['end_time_UTC']\n",
        "    utc_offset = row['utc_offset']\n",
        "\n",
        "    # Apply UTC offset to convert to local time\n",
        "    local_time = utc_time + timedelta(hours=utc_offset)\n",
        "    return local_time\n",
        "\n",
        "# Apply the function to create a new 'end_time_local' column\n",
        "new_data['end_time_local'] = new_data.apply(convert_to_local_time, axis=1)\n",
        "\n",
        "# Step 4: Apply the function to create a new 'end_time_local' column\n",
        "new_data['end_time_local'] = new_data.apply(convert_to_local_time, axis=1)\n",
        "\n",
        "# Step 5: Extract Time Features from 'end_time_local' (if not NaN)\n",
        "# Now that we have 'end_time_local', we'll extract day of the week, hour, and month.\n",
        "\n",
        "# Make sure to handle cases where 'end_time_local' might be NaN (if there were missing UTC offsets)\n",
        "# We can use `.dt` accessor to extract day of the week, hour, and month\n",
        "\n",
        "# First, we need to check for non-NaT values in 'end_time_local'\n",
        "new_data = new_data[new_data['end_time_local'].notna()]\n",
        "\n",
        "# Extract 'day_of_week', 'hour', and 'month' from 'end_time_local'\n",
        "new_data['day_of_week'] = new_data['end_time_local'].dt.weekday  # 0=Monday, 6=Sunday\n",
        "new_data['hour'] = new_data['end_time_local'].dt.hour  # Extract the hour\n",
        "new_data['month'] = new_data['end_time_local'].dt.month  # Extract the month\n",
        "new_data['year'] = new_data['end_time_local'].dt.year  # Extract the year\n",
        "\n",
        "# Step 6: Check the new DataFrame to see the extracted time features\n",
        "new_data.columns"
      ],
      "metadata": {
        "id": "2vsgcBueGneK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4c2b41b-4cd5-43c6-d75e-a11b6c5325fe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['track_uri', 'unique_id', 'end_time_UTC', 'platform', 'ms_played',\n",
              "       'connection_country', 'ip_address', 'track_name', 'album_artist_name',\n",
              "       'album_name', 'episode_name', 'episode_show_name', 'episode_uri',\n",
              "       'start_reason', 'end_reason', 'shuffle', 'skipped', 'city', 'region',\n",
              "       'country', 'latitude', 'longitude', 'local_time_zone', 'utc_offset',\n",
              "       'song_duration_ms_x', 'album_type_x', 'explicit_x',\n",
              "       'available_markets_x', 'song_duration_ms_y', 'album_type_y',\n",
              "       'explicit_y', 'available_markets_y', 'end_time_local', 'day_of_week',\n",
              "       'hour', 'month', 'year'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}