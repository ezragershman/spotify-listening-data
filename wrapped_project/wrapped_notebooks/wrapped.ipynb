{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asttokens==3.0.0 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: colorama==0.4.6 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: comm==0.2.2 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 3)) (0.2.2)\n",
      "Requirement already satisfied: contourpy==1.3.1 in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: cycler==0.12.1 in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: debugpy==1.8.9 in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 6)) (1.8.9)\n",
      "Requirement already satisfied: decorator==5.1.1 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 7)) (5.1.1)\n",
      "Requirement already satisfied: executing==2.1.0 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: fonttools==4.55.2 in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 9)) (4.55.2)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 10)) (6.29.5)\n",
      "Requirement already satisfied: ipython==8.30.0 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 11)) (8.30.0)\n",
      "Requirement already satisfied: jedi==0.19.2 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 12)) (0.19.2)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 13)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 14)) (5.7.2)\n",
      "Requirement already satisfied: kiwisolver==1.4.7 in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 15)) (1.4.7)\n",
      "Requirement already satisfied: matplotlib==3.9.3 in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 16)) (3.9.3)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 17)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 18)) (1.6.0)\n",
      "Requirement already satisfied: numpy==2.2.0 in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 19)) (2.2.0)\n",
      "Requirement already satisfied: packaging==24.2 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 20)) (24.2)\n",
      "Requirement already satisfied: pandas==2.2.3 in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 21)) (2.2.3)\n",
      "Requirement already satisfied: parso==0.8.4 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 22)) (0.8.4)\n",
      "Requirement already satisfied: pillow==11.0.0 in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 23)) (11.0.0)\n",
      "Requirement already satisfied: platformdirs==4.3.6 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 24)) (4.3.6)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.48 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 25)) (3.0.48)\n",
      "Requirement already satisfied: psutil==6.1.0 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 26)) (6.1.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 27)) (0.2.3)\n",
      "Requirement already satisfied: Pygments==2.18.0 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 28)) (2.18.0)\n",
      "Requirement already satisfied: pyparsing==3.2.0 in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 29)) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 30)) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 31)) (1.0.1)\n",
      "Requirement already satisfied: pytz==2024.2 in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 32)) (2024.2)\n",
      "Requirement already satisfied: pywin32==308 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 33)) (308)\n",
      "Requirement already satisfied: pyzmq==26.2.0 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 34)) (26.2.0)\n",
      "Requirement already satisfied: requests==2.26.0 in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 35)) (2.26.0)\n",
      "Requirement already satisfied: setuptools==75.6.0 in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 36)) (75.6.0)\n",
      "Requirement already satisfied: six==1.17.0 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 37)) (1.17.0)\n",
      "Requirement already satisfied: stack-data==0.6.3 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 38)) (0.6.3)\n",
      "Requirement already satisfied: tornado==6.4.2 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 39)) (6.4.2)\n",
      "Requirement already satisfied: traitlets==5.14.3 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 40)) (5.14.3)\n",
      "Requirement already satisfied: tzdata==2024.2 in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 41)) (2024.2)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in c:\\users\\ezrag\\appdata\\roaming\\python\\python313\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 42)) (0.2.13)\n",
      "Requirement already satisfied: wheel==0.45.1 in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 43)) (0.45.1)\n",
      "Requirement already satisfied: noise in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 44)) (1.2.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests==2.26.0->-r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 35)) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests==2.26.0->-r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 35)) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests==2.26.0->-r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 35)) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ezrag\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests==2.26.0->-r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt (line 35)) (3.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "SPOTIFY_CLIENT_ID = os.getenv('SPOTIFY_CLIENT_ID')\n",
    "SPOTIFY_CLIENT_SECRET = os.getenv('SPOTIFY_CLIENT_SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import threading\n",
    "import queue\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get user ID from input\n",
    "def get_user_id():\n",
    "    user_id = input(\"Enter the user's ID: \").lower()\n",
    "    return user_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the number of data chunks from input\n",
    "def get_num_chunks():\n",
    "    num_chunks = int(input(\"Enter the number of chunks: \"))\n",
    "    return num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and process data from multiple JSON files\n",
    "def read_and_process_data(user_id, num_chunks, base_path='wrapped_files/'):\n",
    "    all_data = []\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        json_file = os.path.join(base_path, f'{user_id}_music_{i}.json')\n",
    "        print(f\"Checking for file: {json_file}\")\n",
    "        \n",
    "        if not os.path.exists(json_file):\n",
    "            print(f\"File not found: {json_file}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Reading data from {json_file}\")\n",
    "        with open(json_file, 'r', encoding='utf-8') as file:\n",
    "            data_list = json.load(file)\n",
    "            all_data.extend(data_list)\n",
    "    \n",
    "    if not all_data:\n",
    "        raise ValueError(\"No data files were found or all were empty.\")\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    df['user_id'] = user_id\n",
    "    df['endTime'] = pd.to_datetime(df['endTime'])\n",
    "    \n",
    "    print(f\"Data read successfully for {len(df)} records.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to export data to a CSV file\n",
    "def export_to_csv(df, user_id):\n",
    "    csv_file = f'{user_id}_listening_data.csv'\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Data exported to {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to track and save unique songs to a CSV file\n",
    "def track_unique_songs(df, unique_songs_file):\n",
    "    # Drop duplicates within the current DataFrame\n",
    "    new_unique_songs = df[['trackName', 'artistName', 'external_urls']].drop_duplicates()\n",
    "    print(f\"Tracking {len(new_unique_songs)} unique songs.\")\n",
    "    \n",
    "    try:\n",
    "        # Attempt to load existing unique songs from the CSV file\n",
    "        existing_songs = pd.read_csv(unique_songs_file)\n",
    "        \n",
    "        # Ensure no duplicate songs are added\n",
    "        combined_songs = pd.concat([existing_songs, new_unique_songs]).drop_duplicates(subset=['trackName', 'artistName', 'external_urls'])\n",
    "        \n",
    "        # Identify new songs that are not in the existing unique songs\n",
    "        updated_songs = combined_songs[~combined_songs['external_urls'].isin(existing_songs['external_urls'])]\n",
    "        \n",
    "        print(f\"Existing unique songs loaded, total unique songs now {len(combined_songs)}.\")\n",
    "    except FileNotFoundError:\n",
    "        updated_songs = new_unique_songs\n",
    "        print(\"Unique songs file not found. Creating a new one.\")\n",
    "    \n",
    "    # Save the updated unique songs list to the CSV file\n",
    "    updated_songs.to_csv(unique_songs_file, index=False)\n",
    "    print(f\"Unique songs tracked and saved to {unique_songs_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get Spotify access token using client credentials\n",
    "def get_spotify_access_token(client_id, client_secret):\n",
    "    auth_url = 'https://accounts.spotify.com/api/token'\n",
    "    auth_response = requests.post(auth_url, {\n",
    "        'grant_type': 'client_credentials',\n",
    "        'client_id': client_id,\n",
    "        'client_secret': client_secret,\n",
    "    })\n",
    "    \n",
    "    # Parse the authentication response and extract access token\n",
    "    auth_response_data = auth_response.json()\n",
    "    return auth_response_data['access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get song details from Spotify API using search query\n",
    "def get_song_details(artist_name, track_name, access_token):\n",
    "    search_url = 'https://api.spotify.com/v1/search'\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {access_token}'\n",
    "    }\n",
    "    params = {\n",
    "        'q': f'artist:{artist_name} track:{track_name}',\n",
    "        'type': 'track',\n",
    "        'limit': 1\n",
    "    }\n",
    "    \n",
    "    # Send request to Spotify API to search for the track\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    response_data = response.json()\n",
    "    \n",
    "    if response_data['tracks']['items']:\n",
    "        track_info = response_data['tracks']['items'][0]\n",
    "        song_details = {\n",
    "            'spotify_id': track_info['id'],\n",
    "            'album': track_info['album']['name'],\n",
    "            'release_date': track_info['album']['release_date'],\n",
    "            'popularity': track_info['popularity'],\n",
    "            'duration_ms': track_info['duration_ms'],\n",
    "            'track_number': track_info['track_number'],\n",
    "            'album_artwork': track_info['album']['images'][0]['url'] if track_info['album']['images'] else None,\n",
    "            'external_urls': track_info['external_urls']['spotify'],\n",
    "            'artists_involved': \", \".join(artist['name'] for artist in track_info['artists'])\n",
    "        }\n",
    "        return song_details\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worker function to process each song in the queue\n",
    "def worker_thread(queue, unique_songs, access_token, export_interval, lock, start_time):\n",
    "    while not queue.empty():\n",
    "        index, row = queue.get()\n",
    "        if pd.notna(row['spotify_id']):\n",
    "            print(f\"Skipping already updated song at index {index}.\")\n",
    "            queue.task_done()\n",
    "            continue\n",
    "        \n",
    "        artist_name = row['artistName']\n",
    "        track_name = row['trackName']\n",
    "        song_details = get_song_details(artist_name, track_name, access_token)\n",
    "        \n",
    "        if song_details:\n",
    "            with lock:\n",
    "                unique_songs.at[index, 'spotify_id'] = song_details['spotify_id']\n",
    "                unique_songs.at[index, 'album'] = song_details['album']\n",
    "                unique_songs.at[index, 'release_date'] = song_details['release_date']\n",
    "                unique_songs.at[index, 'popularity'] = song_details['popularity']\n",
    "                unique_songs.at[index, 'duration_ms'] = song_details['duration_ms']\n",
    "                unique_songs.at[index, 'track_number'] = song_details['track_number']\n",
    "                unique_songs.at[index, 'album_artwork'] = song_details['album_artwork']\n",
    "                unique_songs.at[index, 'external_urls'] = song_details['external_urls']\n",
    "                unique_songs.at[index, 'artists_involved'] = song_details['artists_involved']\n",
    "        \n",
    "        if (index + 1) % export_interval == 0:\n",
    "            with lock:\n",
    "                print(f\"Exporting data at index {index}. Elapsed time: {time.time() - start_time:.2f} seconds.\")\n",
    "                unique_songs.to_csv(unique_songs_file, index=False)\n",
    "        \n",
    "        queue.task_done()\n",
    "        print(f\"Processed index {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to update unique songs table with Spotify info using threading\n",
    "def update_unique_songs(unique_songs_file='unique_songs.csv', export_interval=50):\n",
    "    # Load unique songs data from CSV file\n",
    "    unique_songs = pd.read_csv(unique_songs_file)\n",
    "    \n",
    "    # Check if the columns already exist, if not, create them\n",
    "    if 'spotify_id' not in unique_songs.columns:\n",
    "        unique_songs['spotify_id'] = None\n",
    "    if 'album' not in unique_songs.columns:\n",
    "        unique_songs['album'] = None\n",
    "    if 'release_date' not in unique_songs.columns:\n",
    "        unique_songs['popularity'] = None\n",
    "    if 'duration_ms' not in unique_songs.columns:\n",
    "        unique_songs['duration_ms'] = None\n",
    "    if 'track_number' not in unique_songs.columns:\n",
    "        unique_songs['track_number'] = None\n",
    "    if 'album_artwork' not in unique_songs.columns:\n",
    "        unique_songs['album_artwork'] = None\n",
    "    if 'external_urls' not in unique_songs.columns:\n",
    "        unique_songs['external_urls'] = None\n",
    "    if 'artists_involved' not in unique_songs.columns:\n",
    "        unique_songs['artists_involved'] = None\n",
    "\n",
    "    # Get Spotify access token\n",
    "    access_token = get_spotify_access_token(SPOTIFY_CLIENT_ID, SPOTIFY_CLIENT_SECRET)\n",
    "    \n",
    "    # Create a queue and add songs to be processed\n",
    "    q = queue.Queue()\n",
    "    for index, row in unique_songs.iterrows():\n",
    "        q.put((index, row))\n",
    "\n",
    "    # Create a lock for thread-safe operations\n",
    "    lock = threading.Lock()\n",
    "    start_time = time.time()\n",
    "    threads = []\n",
    "    for _ in range(10):  # Adjust number of threads as needed\n",
    "        thread = threading.Thread(target=worker_thread, args=(q, unique_songs, access_token, export_interval, lock, start_time))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "    \n",
    "    # Wait for all threads to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    # Remove duplicates based on 'external_urls'\n",
    "    unique_songs = drop_duplicates_by_external_urls(unique_songs)\n",
    "    \n",
    "    # Final export\n",
    "    print(f\"Final export. Total time taken: {time.time() - start_time:.2f} seconds.\")\n",
    "    unique_songs.to_csv(unique_songs_file, index=False)\n",
    "    print(f\"Unique songs table updated with Spotify info and saved to {unique_songs_file}.\")\n",
    "\n",
    "# Function to drop duplicates based on external_urls\n",
    "def drop_duplicates_by_external_urls(data):\n",
    "    \"\"\"\n",
    "    This function drops duplicate rows based on the 'external_urls' column.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): DataFrame containing the song data with 'external_urls' column.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with duplicates removed based on 'external_urls'.\n",
    "    \"\"\"\n",
    "    data = data.drop_duplicates(subset=['external_urls'])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fill in song info from unique songs database\n",
    "def fill_song_info(listening_data, unique_songs):\n",
    "    # Filter out rows where artistName is 'unknown'\n",
    "    listening_data_filtered = listening_data[~listening_data['artistName'].str.lower().isin(['unknown', 'unknown artist'])]\n",
    "    # Merge listening data with unique songs data on 'artistName' and 'trackName'\n",
    "    filled_data = pd.merge(listening_data_filtered, unique_songs, on=['artistName', 'trackName'], how='left')\n",
    "    return filled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read processed listening data\n",
    "def read_processed_data(user_id):\n",
    "    csv_file = f'{user_id}_listening_data.csv'  # Example file path, adjust as needed\n",
    "    listening_data = pd.read_csv(csv_file)\n",
    "    return listening_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to export filled listening data to a CSV file\n",
    "def export_filled_data(filled_data, user_id):\n",
    "    filled_csv_file = f'{user_id}_listening_data.csv'\n",
    "    filled_data.to_csv(filled_csv_file, index=False)\n",
    "    print(f\"Filled listening data exported to {filled_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import threading \n",
    "import time \n",
    "import requests \n",
    "from queue import Queue\n",
    "\n",
    "\n",
    "def get_album_details(artist_name, album_name, access_token, retry_count=3):\n",
    "    url = f\"https://api.spotify.com/v1/search?q=album:{album_name} artist:{artist_name}&type=album\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\"\n",
    "    }\n",
    "    for attempt in range(retry_count):\n",
    "        response = requests.get(url, headers=headers)\n",
    "        print(f\"API Response Status Code: {response.status_code}\")  # Debugging\n",
    "        if response.status_code == 200:\n",
    "            albums = response.json().get('albums', {}).get('items', [])\n",
    "            print(f\"API Response Data: {albums}\")  # Debugging\n",
    "            if albums:\n",
    "                return albums[0]  # Return the first matching album\n",
    "        elif response.status_code == 429:\n",
    "            # Wait before retrying\n",
    "            retry_after = int(response.headers.get('Retry-After', 1))\n",
    "            print(f\"Rate limited. Retrying after {retry_after} seconds.\")\n",
    "            time.sleep(retry_after)\n",
    "        else:\n",
    "            print(f\"Failed to fetch data: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# Worker thread function to process albums and update dataframe\n",
    "def worker_thread_album(q, access_token, unique_songs, album_info_cache, lock, export_interval):\n",
    "    processed_count = 0\n",
    "    while not q.empty():\n",
    "        index, row = q.get()\n",
    "        artist_name = row['artistName']\n",
    "        album_name = row['album']\n",
    "        \n",
    "        print(f\"Processing: {artist_name} - {album_name}\")\n",
    "\n",
    "        if (artist_name, album_name) in album_info_cache:\n",
    "            album_details = album_info_cache[(artist_name, album_name)]\n",
    "            print(f\"Found in cache: {artist_name} - {album_name}\")\n",
    "        else:\n",
    "            album_details = get_album_details(artist_name, album_name, access_token)\n",
    "            if album_details:\n",
    "                with lock:\n",
    "                    album_info_cache[(artist_name, album_name)] = album_details\n",
    "                print(f\"Fetched from Spotify: {artist_name} - {album_name}\")\n",
    "\n",
    "        if album_details:\n",
    "            with lock:\n",
    "                unique_songs.at[index, 'genre'] = ', '.join(album_details.get('genres', []))\n",
    "                unique_songs.at[index, 'label'] = album_details.get('label')\n",
    "                unique_songs.at[index, 'release_date'] = album_details.get('release_date')\n",
    "                unique_songs.at[index, 'album_type'] = album_details.get('album_type')\n",
    "                print(f\"Updated song: {artist_name} - {album_name} with details: {album_details}\")\n",
    "\n",
    "        processed_count += 1\n",
    "        \n",
    "        # Periodic export based on processed count\n",
    "        if processed_count >= export_interval:\n",
    "            with lock:\n",
    "                print(f\"Periodic export after processing {processed_count} songs.\")\n",
    "                unique_songs.to_csv('unique_songs_temp.csv', index=False)\n",
    "                processed_count = 0  # Reset the counter\n",
    "\n",
    "        q.task_done()\n",
    "\n",
    "# Function to update unique songs with album information\n",
    "def update_unique_songs_with_album_info(unique_songs_file='unique_songs.csv', export_interval=50):\n",
    "    # Load unique songs data from CSV file\n",
    "    unique_songs = pd.read_csv(unique_songs_file)\n",
    "    \n",
    "    # Check if the necessary columns already exist, if not, create them\n",
    "    for column in ['genre', 'label', 'release_date', 'album_type']:\n",
    "        if column not in unique_songs.columns:\n",
    "            unique_songs[column] = None\n",
    "    \n",
    "    # Get Spotify access token\n",
    "    access_token = get_spotify_access_token(SPOTIFY_CLIENT_ID, SPOTIFY_CLIENT_SECRET)\n",
    "    \n",
    "    # Create a queue and add albums to be processed\n",
    "    q = Queue()\n",
    "    for index, row in unique_songs.iterrows():\n",
    "        if pd.notna(row['album']) and pd.notna(row['artistName']):\n",
    "            q.put((index, row))\n",
    "    \n",
    "    # Create a lock and album info cache for thread-safe operations\n",
    "    lock = threading.Lock()\n",
    "    album_info_cache = {}\n",
    "    threads = []\n",
    "    for _ in range(10):  # Adjust number of threads as needed\n",
    "        thread = threading.Thread(target=worker_thread_album, args=(q, access_token, unique_songs, album_info_cache, lock, export_interval))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "    \n",
    "    # Wait for all threads to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    # Final export\n",
    "    unique_songs.to_csv(unique_songs_file, index=False)\n",
    "    print(f\"Unique songs table updated with album info and saved to {unique_songs_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Garfunkel and Oates - All Over Your Face\n",
      "Processing: Leith Ross - I'd Have to Think About It\n",
      "Processing: Tomohisa Sako - 僕から君へ\n",
      "Processing: ЯeaL - ライトアップアンビバレンツ\n",
      "Processing: Okazaki Taiiku - 「劇場版ポケットモンスター ココ」テーマソング集\n",
      "Processing: Shoko Nakagawa - Kaze To Issho Ni\n",
      "Processing: 西川くんとキリショー - 1・2・3\n",
      "Processing: Yusuke - あの・・旅の途中なんですケド。コンプリートパック\n",
      "Processing: Shoko Nakagawa - RGB - True Color\n",
      "Processing: Okazaki Taiiku - 「劇場版ポケットモンスター ココ」テーマソング集\n",
      "API Response Status Code: 429\n",
      "Rate limited. Retrying after 5285 seconds.\n",
      "API Response Status Code: 429\n",
      "Rate limited. Retrying after 5285 seconds.\n",
      "API Response Status Code: 429\n",
      "Rate limited. Retrying after 5285 seconds.\n",
      "API Response Status Code: 429\n",
      "Rate limited. Retrying after 5285 seconds.\n",
      "API Response Status Code: 429\n",
      "Rate limited. Retrying after 5285 seconds.\n",
      "API Response Status Code: 429\n",
      "Rate limited. Retrying after 5285 seconds.\n",
      "API Response Status Code: 429\n",
      "Rate limited. Retrying after 5285 seconds.\n",
      "API Response Status Code: 429\n",
      "Rate limited. Retrying after 5285 seconds.\n",
      "API Response Status Code: 429API Response Status Code: 429\n",
      "Rate limited. Retrying after 5285 seconds.\n",
      "\n",
      "Rate limited. Retrying after 5285 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the function\n",
    "update_unique_songs_with_album_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Billie Eilish - WHEN WE ALL FALL ASLEEP, WHERE DO WE GO?\n",
      "Processing: Led Zeppelin - Led Zeppelin IV\n",
      "Processing: Pink Floyd - The Wall\n",
      "API Response Status Code: 200\n",
      "API Response Data: [{'album_type': 'album', 'total_tracks': 16, 'available_markets': ['AR', 'AU', 'AT', 'BE', 'BO', 'BR', 'BG', 'CA', 'CL', 'CO', 'CR', 'CY', 'CZ', 'DK', 'DO', 'DE', 'EC', 'EE', 'SV', 'FI', 'FR', 'GR', 'GT', 'HN', 'HK', 'HU', 'IS', 'IE', 'IT', 'LV', 'LT', 'LU', 'MY', 'MT', 'MX', 'NL', 'NZ', 'NI', 'NO', 'PA', 'PY', 'PE', 'PH', 'PL', 'PT', 'SG', 'SK', 'ES', 'SE', 'CH', 'TW', 'TR', 'UY', 'US', 'GB', 'AD', 'LI', 'MC', 'ID', 'JP', 'TH', 'VN', 'RO', 'IL', 'ZA', 'SA', 'AE', 'BH', 'QA', 'OM', 'KW', 'EG', 'MA', 'DZ', 'TN', 'LB', 'JO', 'PS', 'IN', 'BY', 'KZ', 'MD', 'UA', 'AL', 'BA', 'HR', 'ME', 'MK', 'RS', 'SI', 'KR', 'BD', 'PK', 'LK', 'GH', 'KE', 'NG', 'TZ', 'UG', 'AG', 'AM', 'BS', 'BB', 'BZ', 'BT', 'BW', 'BF', 'CV', 'CW', 'DM', 'FJ', 'GM', 'GE', 'GD', 'GW', 'GY', 'HT', 'JM', 'KI', 'LS', 'LR', 'MW', 'MV', 'ML', 'MH', 'FM', 'NA', 'NR', 'NE', 'PW', 'PG', 'PR', 'WS', 'SM', 'ST', 'SN', 'SC', 'SL', 'SB', 'KN', 'LC', 'VC', 'SR', 'TL', 'TO', 'TT', 'TV', 'VU', 'AZ', 'BN', 'BI', 'KH', 'CM', 'TD', 'KM', 'GQ', 'SZ', 'GA', 'GN', 'KG', 'LA', 'MO', 'MR', 'MN', 'NP', 'RW', 'TG', 'UZ', 'ZW', 'BJ', 'MG', 'MU', 'MZ', 'AO', 'CI', 'DJ', 'ZM', 'CD', 'CG', 'IQ', 'LY', 'TJ', 'VE', 'ET', 'XK'], 'external_urls': {'spotify': 'https://open.spotify.com/album/44Ig8dzqOkvkGDzaUof9lK'}, 'href': 'https://api.spotify.com/v1/albums/44Ig8dzqOkvkGDzaUof9lK', 'id': '44Ig8dzqOkvkGDzaUof9lK', 'images': [{'height': 640, 'url': 'https://i.scdn.co/image/ab67616d0000b273c8a11e48c91a982d086afc69', 'width': 640}, {'height': 300, 'url': 'https://i.scdn.co/image/ab67616d00001e02c8a11e48c91a982d086afc69', 'width': 300}, {'height': 64, 'url': 'https://i.scdn.co/image/ab67616d00004851c8a11e48c91a982d086afc69', 'width': 64}], 'name': 'Led Zeppelin IV (Deluxe Edition)', 'release_date': '1971-11-08', 'release_date_precision': 'day', 'type': 'album', 'uri': 'spotify:album:44Ig8dzqOkvkGDzaUof9lK', 'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/36QJpDe2go2KgaRleHCDTp'}, 'href': 'https://api.spotify.com/v1/artists/36QJpDe2go2KgaRleHCDTp', 'id': '36QJpDe2go2KgaRleHCDTp', 'name': 'Led Zeppelin', 'type': 'artist', 'uri': 'spotify:artist:36QJpDe2go2KgaRleHCDTp'}]}, {'album_type': 'album', 'total_tracks': 8, 'available_markets': ['AR', 'AU', 'AT', 'BE', 'BO', 'BR', 'BG', 'CA', 'CL', 'CO', 'CR', 'CY', 'CZ', 'DK', 'DO', 'DE', 'EC', 'EE', 'SV', 'FI', 'FR', 'GR', 'GT', 'HN', 'HK', 'HU', 'IS', 'IE', 'IT', 'LV', 'LT', 'LU', 'MY', 'MT', 'MX', 'NL', 'NZ', 'NI', 'NO', 'PA', 'PY', 'PE', 'PH', 'PL', 'PT', 'SG', 'SK', 'ES', 'SE', 'CH', 'TW', 'TR', 'UY', 'US', 'GB', 'AD', 'LI', 'MC', 'ID', 'JP', 'TH', 'VN', 'RO', 'IL', 'ZA', 'SA', 'AE', 'BH', 'QA', 'OM', 'KW', 'EG', 'MA', 'DZ', 'TN', 'LB', 'JO', 'PS', 'IN', 'BY', 'KZ', 'MD', 'UA', 'AL', 'BA', 'HR', 'ME', 'MK', 'RS', 'SI', 'KR', 'BD', 'PK', 'LK', 'GH', 'KE', 'NG', 'TZ', 'UG', 'AG', 'AM', 'BS', 'BB', 'BZ', 'BW', 'BF', 'CV', 'CW', 'DM', 'FJ', 'GM', 'GD', 'GW', 'HT', 'JM', 'LS', 'LR', 'MW', 'ML', 'FM', 'NA', 'NE', 'PG', 'PR', 'SM', 'ST', 'SN', 'SC', 'SL', 'KN', 'LC', 'VC', 'TL', 'TT', 'AZ', 'BN', 'BI', 'KH', 'CM', 'TD', 'KM', 'GQ', 'SZ', 'GA', 'GN', 'KG', 'LA', 'MO', 'MR', 'MN', 'NP', 'RW', 'TG', 'UZ', 'ZW', 'BJ', 'MG', 'MU', 'MZ', 'AO', 'CI', 'DJ', 'ZM', 'CD', 'CG', 'IQ', 'LY', 'TJ', 'VE', 'ET', 'XK'], 'external_urls': {'spotify': 'https://open.spotify.com/album/5EyIDBAqhnlkAHqvPRwdbX'}, 'href': 'https://api.spotify.com/v1/albums/5EyIDBAqhnlkAHqvPRwdbX', 'id': '5EyIDBAqhnlkAHqvPRwdbX', 'images': [{'height': 640, 'url': 'https://i.scdn.co/image/ab67616d0000b2734509204d0860cc0cc67e83dc', 'width': 640}, {'height': 300, 'url': 'https://i.scdn.co/image/ab67616d00001e024509204d0860cc0cc67e83dc', 'width': 300}, {'height': 64, 'url': 'https://i.scdn.co/image/ab67616d000048514509204d0860cc0cc67e83dc', 'width': 64}], 'name': 'Led Zeppelin IV (Remaster)', 'release_date': '1971-11-08', 'release_date_precision': 'day', 'type': 'album', 'uri': 'spotify:album:5EyIDBAqhnlkAHqvPRwdbX', 'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/36QJpDe2go2KgaRleHCDTp'}, 'href': 'https://api.spotify.com/v1/artists/36QJpDe2go2KgaRleHCDTp', 'id': '36QJpDe2go2KgaRleHCDTp', 'name': 'Led Zeppelin', 'type': 'artist', 'uri': 'spotify:artist:36QJpDe2go2KgaRleHCDTp'}]}]\n",
      "Fetched from Spotify: Led Zeppelin - Led Zeppelin IV\n",
      "Updated song: Led Zeppelin - Led Zeppelin IV with details: {'album_type': 'album', 'total_tracks': 16, 'available_markets': ['AR', 'AU', 'AT', 'BE', 'BO', 'BR', 'BG', 'CA', 'CL', 'CO', 'CR', 'CY', 'CZ', 'DK', 'DO', 'DE', 'EC', 'EE', 'SV', 'FI', 'FR', 'GR', 'GT', 'HN', 'HK', 'HU', 'IS', 'IE', 'IT', 'LV', 'LT', 'LU', 'MY', 'MT', 'MX', 'NL', 'NZ', 'NI', 'NO', 'PA', 'PY', 'PE', 'PH', 'PL', 'PT', 'SG', 'SK', 'ES', 'SE', 'CH', 'TW', 'TR', 'UY', 'US', 'GB', 'AD', 'LI', 'MC', 'ID', 'JP', 'TH', 'VN', 'RO', 'IL', 'ZA', 'SA', 'AE', 'BH', 'QA', 'OM', 'KW', 'EG', 'MA', 'DZ', 'TN', 'LB', 'JO', 'PS', 'IN', 'BY', 'KZ', 'MD', 'UA', 'AL', 'BA', 'HR', 'ME', 'MK', 'RS', 'SI', 'KR', 'BD', 'PK', 'LK', 'GH', 'KE', 'NG', 'TZ', 'UG', 'AG', 'AM', 'BS', 'BB', 'BZ', 'BT', 'BW', 'BF', 'CV', 'CW', 'DM', 'FJ', 'GM', 'GE', 'GD', 'GW', 'GY', 'HT', 'JM', 'KI', 'LS', 'LR', 'MW', 'MV', 'ML', 'MH', 'FM', 'NA', 'NR', 'NE', 'PW', 'PG', 'PR', 'WS', 'SM', 'ST', 'SN', 'SC', 'SL', 'SB', 'KN', 'LC', 'VC', 'SR', 'TL', 'TO', 'TT', 'TV', 'VU', 'AZ', 'BN', 'BI', 'KH', 'CM', 'TD', 'KM', 'GQ', 'SZ', 'GA', 'GN', 'KG', 'LA', 'MO', 'MR', 'MN', 'NP', 'RW', 'TG', 'UZ', 'ZW', 'BJ', 'MG', 'MU', 'MZ', 'AO', 'CI', 'DJ', 'ZM', 'CD', 'CG', 'IQ', 'LY', 'TJ', 'VE', 'ET', 'XK'], 'external_urls': {'spotify': 'https://open.spotify.com/album/44Ig8dzqOkvkGDzaUof9lK'}, 'href': 'https://api.spotify.com/v1/albums/44Ig8dzqOkvkGDzaUof9lK', 'id': '44Ig8dzqOkvkGDzaUof9lK', 'images': [{'height': 640, 'url': 'https://i.scdn.co/image/ab67616d0000b273c8a11e48c91a982d086afc69', 'width': 640}, {'height': 300, 'url': 'https://i.scdn.co/image/ab67616d00001e02c8a11e48c91a982d086afc69', 'width': 300}, {'height': 64, 'url': 'https://i.scdn.co/image/ab67616d00004851c8a11e48c91a982d086afc69', 'width': 64}], 'name': 'Led Zeppelin IV (Deluxe Edition)', 'release_date': '1971-11-08', 'release_date_precision': 'day', 'type': 'album', 'uri': 'spotify:album:44Ig8dzqOkvkGDzaUof9lK', 'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/36QJpDe2go2KgaRleHCDTp'}, 'href': 'https://api.spotify.com/v1/artists/36QJpDe2go2KgaRleHCDTp', 'id': '36QJpDe2go2KgaRleHCDTp', 'name': 'Led Zeppelin', 'type': 'artist', 'uri': 'spotify:artist:36QJpDe2go2KgaRleHCDTp'}]}\n",
      "Periodic export after processing 1 songs.\n",
      "API Response Status Code: 200\n",
      "API Response Data: [{'album_type': 'album', 'total_tracks': 26, 'available_markets': ['AD', 'AE', 'AG', 'AL', 'AO', 'AR', 'AU', 'BA', 'BB', 'BD', 'BF', 'BG', 'BH', 'BI', 'BJ', 'BN', 'BO', 'BR', 'BS', 'BT', 'BW', 'BZ', 'CA', 'CD', 'CG', 'CI', 'CL', 'CM', 'CO', 'CR', 'CV', 'CW', 'DJ', 'DM', 'DO', 'DZ', 'EC', 'EG', 'ET', 'FJ', 'FM', 'FR', 'GA', 'GD', 'GH', 'GM', 'GN', 'GQ', 'GT', 'GW', 'GY', 'HK', 'HN', 'HR', 'HT', 'ID', 'IL', 'IN', 'IQ', 'JM', 'JO', 'JP', 'KE', 'KH', 'KI', 'KM', 'KN', 'KR', 'KW', 'LA', 'LB', 'LC', 'LK', 'LR', 'LS', 'LY', 'MA', 'MC', 'ME', 'MG', 'MH', 'MK', 'ML', 'MO', 'MR', 'MU', 'MV', 'MW', 'MX', 'MY', 'MZ', 'NA', 'NE', 'NG', 'NI', 'NL', 'NP', 'NR', 'NZ', 'OM', 'PA', 'PE', 'PG', 'PH', 'PK', 'PL', 'PR', 'PS', 'PW', 'PY', 'QA', 'RO', 'RS', 'RW', 'SA', 'SB', 'SC', 'SG', 'SI', 'SL', 'SN', 'SR', 'ST', 'SV', 'SZ', 'TD', 'TG', 'TH', 'TL', 'TN', 'TO', 'TT', 'TV', 'TW', 'TZ', 'UG', 'US', 'UY', 'VC', 'VE', 'VN', 'VU', 'WS', 'XK', 'ZA', 'ZM', 'ZW'], 'external_urls': {'spotify': 'https://open.spotify.com/album/5Dbax7G8SWrP9xyzkOvy2F'}, 'href': 'https://api.spotify.com/v1/albums/5Dbax7G8SWrP9xyzkOvy2F', 'id': '5Dbax7G8SWrP9xyzkOvy2F', 'images': [{'height': 640, 'url': 'https://i.scdn.co/image/ab67616d0000b2735d48e2f56d691f9a4e4b0bdf', 'width': 640}, {'height': 300, 'url': 'https://i.scdn.co/image/ab67616d00001e025d48e2f56d691f9a4e4b0bdf', 'width': 300}, {'height': 64, 'url': 'https://i.scdn.co/image/ab67616d000048515d48e2f56d691f9a4e4b0bdf', 'width': 64}], 'name': 'The Wall', 'release_date': '1979-11-30', 'release_date_precision': 'day', 'type': 'album', 'uri': 'spotify:album:5Dbax7G8SWrP9xyzkOvy2F', 'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/0k17h0D3J5VfsdmQ1iZtE9'}, 'href': 'https://api.spotify.com/v1/artists/0k17h0D3J5VfsdmQ1iZtE9', 'id': '0k17h0D3J5VfsdmQ1iZtE9', 'name': 'Pink Floyd', 'type': 'artist', 'uri': 'spotify:artist:0k17h0D3J5VfsdmQ1iZtE9'}]}, {'album_type': 'single', 'total_tracks': 1, 'available_markets': ['AD', 'AE', 'AG', 'AL', 'AO', 'AR', 'AU', 'BA', 'BB', 'BD', 'BF', 'BG', 'BH', 'BI', 'BJ', 'BN', 'BO', 'BR', 'BS', 'BT', 'BW', 'BZ', 'CA', 'CD', 'CG', 'CI', 'CL', 'CM', 'CO', 'CR', 'CV', 'CW', 'DJ', 'DM', 'DO', 'DZ', 'EC', 'EG', 'ET', 'FJ', 'FM', 'FR', 'GA', 'GD', 'GH', 'GM', 'GN', 'GQ', 'GT', 'GW', 'GY', 'HK', 'HN', 'HR', 'HT', 'ID', 'IL', 'IN', 'IQ', 'JM', 'JO', 'JP', 'KE', 'KH', 'KI', 'KM', 'KN', 'KR', 'KW', 'LA', 'LB', 'LC', 'LK', 'LR', 'LS', 'LY', 'MA', 'MC', 'ME', 'MG', 'MH', 'MK', 'ML', 'MO', 'MR', 'MU', 'MV', 'MW', 'MX', 'MY', 'MZ', 'NA', 'NE', 'NG', 'NI', 'NL', 'NP', 'NR', 'NZ', 'OM', 'PA', 'PE', 'PG', 'PH', 'PK', 'PL', 'PR', 'PS', 'PW', 'PY', 'QA', 'RO', 'RS', 'RW', 'SA', 'SB', 'SC', 'SG', 'SI', 'SL', 'SN', 'SR', 'ST', 'SV', 'SZ', 'TD', 'TG', 'TH', 'TL', 'TN', 'TO', 'TT', 'TV', 'TW', 'TZ', 'UG', 'US', 'UY', 'VC', 'VE', 'VN', 'VU', 'WS', 'XK', 'ZA', 'ZM', 'ZW'], 'external_urls': {'spotify': 'https://open.spotify.com/album/32d478S4XY41gb3H7DgOyM'}, 'href': 'https://api.spotify.com/v1/albums/32d478S4XY41gb3H7DgOyM', 'id': '32d478S4XY41gb3H7DgOyM', 'images': [{'height': 640, 'url': 'https://i.scdn.co/image/ab67616d0000b273394a5add67c488985d470fac', 'width': 640}, {'height': 300, 'url': 'https://i.scdn.co/image/ab67616d00001e02394a5add67c488985d470fac', 'width': 300}, {'height': 64, 'url': 'https://i.scdn.co/image/ab67616d00004851394a5add67c488985d470fac', 'width': 64}], 'name': 'The Doctor [(Comfortably Numb) [The Wall Work In Progress, Pt. 2, 1979] [Programme 1] [Band Demo] [2011 Remastered Version]]', 'release_date': '2020-07-07', 'release_date_precision': 'day', 'type': 'album', 'uri': 'spotify:album:32d478S4XY41gb3H7DgOyM', 'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/0k17h0D3J5VfsdmQ1iZtE9'}, 'href': 'https://api.spotify.com/v1/artists/0k17h0D3J5VfsdmQ1iZtE9', 'id': '0k17h0D3J5VfsdmQ1iZtE9', 'name': 'Pink Floyd', 'type': 'artist', 'uri': 'spotify:artist:0k17h0D3J5VfsdmQ1iZtE9'}]}, {'album_type': 'single', 'total_tracks': 1, 'available_markets': ['AD', 'AE', 'AG', 'AL', 'AO', 'AR', 'AU', 'BA', 'BB', 'BD', 'BF', 'BG', 'BH', 'BI', 'BJ', 'BN', 'BO', 'BR', 'BS', 'BT', 'BW', 'BZ', 'CA', 'CD', 'CG', 'CI', 'CL', 'CM', 'CO', 'CR', 'CV', 'CW', 'DJ', 'DM', 'DO', 'DZ', 'EC', 'EG', 'ET', 'FJ', 'FM', 'FR', 'GA', 'GD', 'GH', 'GM', 'GN', 'GQ', 'GT', 'GW', 'GY', 'HK', 'HN', 'HR', 'HT', 'ID', 'IL', 'IN', 'IQ', 'JM', 'JO', 'JP', 'KE', 'KH', 'KI', 'KM', 'KN', 'KR', 'KW', 'LA', 'LB', 'LC', 'LK', 'LR', 'LS', 'LY', 'MA', 'MC', 'ME', 'MG', 'MH', 'MK', 'ML', 'MO', 'MR', 'MU', 'MV', 'MW', 'MX', 'MY', 'MZ', 'NA', 'NE', 'NG', 'NI', 'NL', 'NP', 'NR', 'NZ', 'OM', 'PA', 'PE', 'PG', 'PH', 'PK', 'PL', 'PR', 'PS', 'PW', 'PY', 'QA', 'RO', 'RS', 'RW', 'SA', 'SB', 'SC', 'SG', 'SI', 'SL', 'SN', 'SR', 'ST', 'SV', 'SZ', 'TD', 'TG', 'TH', 'TL', 'TN', 'TO', 'TT', 'TV', 'TW', 'TZ', 'UG', 'US', 'UY', 'VC', 'VE', 'VN', 'VU', 'WS', 'XK', 'ZA', 'ZM', 'ZW'], 'external_urls': {'spotify': 'https://open.spotify.com/album/4XJwNchONBIImInCbhBuTI'}, 'href': 'https://api.spotify.com/v1/albums/4XJwNchONBIImInCbhBuTI', 'id': '4XJwNchONBIImInCbhBuTI', 'images': [{'height': 640, 'url': 'https://i.scdn.co/image/ab67616d0000b273c4df46fbb2b1699d72637b68', 'width': 640}, {'height': 300, 'url': 'https://i.scdn.co/image/ab67616d00001e02c4df46fbb2b1699d72637b68', 'width': 300}, {'height': 64, 'url': 'https://i.scdn.co/image/ab67616d00004851c4df46fbb2b1699d72637b68', 'width': 64}], 'name': 'Run Like Hell [The Wall Work In Progress, Pt. 2, 1979 (Programme 1) [Band Demo] [2011 Remastered Version]]', 'release_date': '2020-06-12', 'release_date_precision': 'day', 'type': 'album', 'uri': 'spotify:album:4XJwNchONBIImInCbhBuTI', 'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/0k17h0D3J5VfsdmQ1iZtE9'}, 'href': 'https://api.spotify.com/v1/artists/0k17h0D3J5VfsdmQ1iZtE9', 'id': '0k17h0D3J5VfsdmQ1iZtE9', 'name': 'Pink Floyd', 'type': 'artist', 'uri': 'spotify:artist:0k17h0D3J5VfsdmQ1iZtE9'}]}, {'album_type': 'album', 'total_tracks': 10, 'available_markets': ['AR', 'AU', 'AT', 'BE', 'BO', 'BR', 'BG', 'CA', 'CL', 'CO', 'CR', 'CY', 'CZ', 'DK', 'DO', 'DE', 'EC', 'EE', 'SV', 'FI', 'FR', 'GR', 'GT', 'HN', 'HK', 'HU', 'IS', 'IE', 'IT', 'LV', 'LT', 'LU', 'MY', 'MT', 'MX', 'NL', 'NZ', 'NI', 'NO', 'PA', 'PY', 'PE', 'PH', 'PL', 'PT', 'SG', 'SK', 'ES', 'SE', 'CH', 'TW', 'TR', 'UY', 'US', 'GB', 'AD', 'LI', 'MC', 'ID', 'JP', 'TH', 'VN', 'RO', 'IL', 'ZA', 'SA', 'AE', 'BH', 'QA', 'OM', 'KW', 'EG', 'MA', 'DZ', 'TN', 'LB', 'JO', 'PS', 'IN', 'BY', 'KZ', 'MD', 'UA', 'AL', 'BA', 'HR', 'ME', 'MK', 'RS', 'SI', 'KR', 'BD', 'PK', 'LK', 'GH', 'KE', 'NG', 'TZ', 'UG', 'AG', 'AM', 'BS', 'BB', 'BZ', 'BT', 'BW', 'BF', 'CV', 'CW', 'DM', 'FJ', 'GM', 'GE', 'GD', 'GW', 'GY', 'HT', 'JM', 'KI', 'LS', 'LR', 'MW', 'MV', 'ML', 'MH', 'FM', 'NA', 'NR', 'NE', 'PW', 'PG', 'PR', 'WS', 'SM', 'ST', 'SN', 'SC', 'SL', 'SB', 'KN', 'LC', 'VC', 'SR', 'TL', 'TO', 'TT', 'TV', 'VU', 'AZ', 'BN', 'BI', 'KH', 'CM', 'TD', 'KM', 'GQ', 'SZ', 'GA', 'GN', 'KG', 'LA', 'MO', 'MR', 'MN', 'NP', 'RW', 'TG', 'UZ', 'ZW', 'BJ', 'MG', 'MU', 'MZ', 'AO', 'CI', 'DJ', 'ZM', 'CD', 'CG', 'IQ', 'LY', 'TJ', 'VE', 'ET', 'XK'], 'external_urls': {'spotify': 'https://open.spotify.com/album/3JNBCGuQTRXWu2nHF2ufbI'}, 'href': 'https://api.spotify.com/v1/albums/3JNBCGuQTRXWu2nHF2ufbI', 'id': '3JNBCGuQTRXWu2nHF2ufbI', 'images': [{'height': 640, 'url': 'https://i.scdn.co/image/ab67616d0000b273fecc4b36a7114fe909809bf6', 'width': 640}, {'height': 300, 'url': 'https://i.scdn.co/image/ab67616d00001e02fecc4b36a7114fe909809bf6', 'width': 300}, {'height': 64, 'url': 'https://i.scdn.co/image/ab67616d00004851fecc4b36a7114fe909809bf6', 'width': 64}], 'name': 'Tearing Down the Wall: Live 2015', 'release_date': '2016-12-05', 'release_date_precision': 'day', 'type': 'album', 'uri': 'spotify:album:3JNBCGuQTRXWu2nHF2ufbI', 'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/5KlZxm5ILJ9tuGRG74b6vw'}, 'href': 'https://api.spotify.com/v1/artists/5KlZxm5ILJ9tuGRG74b6vw', 'id': '5KlZxm5ILJ9tuGRG74b6vw', 'name': 'Atom Pink Floyd Tribute', 'type': 'artist', 'uri': 'spotify:artist:5KlZxm5ILJ9tuGRG74b6vw'}]}]\n",
      "Fetched from Spotify: Pink Floyd - The Wall\n",
      "Updated song: Pink Floyd - The Wall with details: {'album_type': 'album', 'total_tracks': 26, 'available_markets': ['AD', 'AE', 'AG', 'AL', 'AO', 'AR', 'AU', 'BA', 'BB', 'BD', 'BF', 'BG', 'BH', 'BI', 'BJ', 'BN', 'BO', 'BR', 'BS', 'BT', 'BW', 'BZ', 'CA', 'CD', 'CG', 'CI', 'CL', 'CM', 'CO', 'CR', 'CV', 'CW', 'DJ', 'DM', 'DO', 'DZ', 'EC', 'EG', 'ET', 'FJ', 'FM', 'FR', 'GA', 'GD', 'GH', 'GM', 'GN', 'GQ', 'GT', 'GW', 'GY', 'HK', 'HN', 'HR', 'HT', 'ID', 'IL', 'IN', 'IQ', 'JM', 'JO', 'JP', 'KE', 'KH', 'KI', 'KM', 'KN', 'KR', 'KW', 'LA', 'LB', 'LC', 'LK', 'LR', 'LS', 'LY', 'MA', 'MC', 'ME', 'MG', 'MH', 'MK', 'ML', 'MO', 'MR', 'MU', 'MV', 'MW', 'MX', 'MY', 'MZ', 'NA', 'NE', 'NG', 'NI', 'NL', 'NP', 'NR', 'NZ', 'OM', 'PA', 'PE', 'PG', 'PH', 'PK', 'PL', 'PR', 'PS', 'PW', 'PY', 'QA', 'RO', 'RS', 'RW', 'SA', 'SB', 'SC', 'SG', 'SI', 'SL', 'SN', 'SR', 'ST', 'SV', 'SZ', 'TD', 'TG', 'TH', 'TL', 'TN', 'TO', 'TT', 'TV', 'TW', 'TZ', 'UG', 'US', 'UY', 'VC', 'VE', 'VN', 'VU', 'WS', 'XK', 'ZA', 'ZM', 'ZW'], 'external_urls': {'spotify': 'https://open.spotify.com/album/5Dbax7G8SWrP9xyzkOvy2F'}, 'href': 'https://api.spotify.com/v1/albums/5Dbax7G8SWrP9xyzkOvy2F', 'id': '5Dbax7G8SWrP9xyzkOvy2F', 'images': [{'height': 640, 'url': 'https://i.scdn.co/image/ab67616d0000b2735d48e2f56d691f9a4e4b0bdf', 'width': 640}, {'height': 300, 'url': 'https://i.scdn.co/image/ab67616d00001e025d48e2f56d691f9a4e4b0bdf', 'width': 300}, {'height': 64, 'url': 'https://i.scdn.co/image/ab67616d000048515d48e2f56d691f9a4e4b0bdf', 'width': 64}], 'name': 'The Wall', 'release_date': '1979-11-30', 'release_date_precision': 'day', 'type': 'album', 'uri': 'spotify:album:5Dbax7G8SWrP9xyzkOvy2F', 'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/0k17h0D3J5VfsdmQ1iZtE9'}, 'href': 'https://api.spotify.com/v1/artists/0k17h0D3J5VfsdmQ1iZtE9', 'id': '0k17h0D3J5VfsdmQ1iZtE9', 'name': 'Pink Floyd', 'type': 'artist', 'uri': 'spotify:artist:0k17h0D3J5VfsdmQ1iZtE9'}]}\n",
      "Periodic export after processing 1 songs.\n",
      "API Response Status Code: 200\n",
      "API Response Data: [{'album_type': 'album', 'total_tracks': 14, 'available_markets': ['AR', 'AU', 'AT', 'BE', 'BO', 'BR', 'BG', 'CA', 'CL', 'CO', 'CR', 'CY', 'CZ', 'DK', 'DO', 'DE', 'EC', 'EE', 'SV', 'FI', 'FR', 'GR', 'GT', 'HN', 'HK', 'HU', 'IS', 'IE', 'IT', 'LV', 'LT', 'LU', 'MY', 'MT', 'MX', 'NL', 'NZ', 'NI', 'NO', 'PA', 'PY', 'PE', 'PH', 'PL', 'PT', 'SG', 'SK', 'ES', 'SE', 'CH', 'TW', 'TR', 'UY', 'US', 'GB', 'AD', 'LI', 'MC', 'ID', 'JP', 'TH', 'VN', 'RO', 'IL', 'ZA', 'SA', 'AE', 'BH', 'QA', 'OM', 'KW', 'EG', 'MA', 'DZ', 'TN', 'LB', 'JO', 'PS', 'IN', 'BY', 'KZ', 'MD', 'UA', 'AL', 'BA', 'HR', 'ME', 'MK', 'RS', 'SI', 'KR', 'BD', 'PK', 'LK', 'GH', 'KE', 'NG', 'TZ', 'UG', 'AG', 'AM', 'BS', 'BB', 'BZ', 'BT', 'BW', 'BF', 'CV', 'CW', 'DM', 'FJ', 'GM', 'GE', 'GD', 'GW', 'GY', 'HT', 'JM', 'KI', 'LS', 'LR', 'MW', 'MV', 'ML', 'MH', 'FM', 'NA', 'NR', 'NE', 'PW', 'PG', 'WS', 'SM', 'ST', 'SN', 'SC', 'SL', 'SB', 'KN', 'LC', 'VC', 'SR', 'TL', 'TO', 'TT', 'TV', 'VU', 'AZ', 'BN', 'BI', 'KH', 'CM', 'TD', 'KM', 'GQ', 'SZ', 'GA', 'GN', 'KG', 'LA', 'MO', 'MR', 'MN', 'NP', 'RW', 'TG', 'UZ', 'ZW', 'BJ', 'MG', 'MU', 'MZ', 'AO', 'CI', 'DJ', 'ZM', 'CD', 'CG', 'IQ', 'LY', 'TJ', 'VE', 'ET', 'XK'], 'external_urls': {'spotify': 'https://open.spotify.com/album/0S0KGZnfBGSIssfF54WSJh'}, 'href': 'https://api.spotify.com/v1/albums/0S0KGZnfBGSIssfF54WSJh', 'id': '0S0KGZnfBGSIssfF54WSJh', 'images': [{'height': 640, 'url': 'https://i.scdn.co/image/ab67616d0000b27350a3147b4edd7701a876c6ce', 'width': 640}, {'height': 300, 'url': 'https://i.scdn.co/image/ab67616d00001e0250a3147b4edd7701a876c6ce', 'width': 300}, {'height': 64, 'url': 'https://i.scdn.co/image/ab67616d0000485150a3147b4edd7701a876c6ce', 'width': 64}], 'name': 'WHEN WE ALL FALL ASLEEP, WHERE DO WE GO?', 'release_date': '2019-03-29', 'release_date_precision': 'day', 'type': 'album', 'uri': 'spotify:album:0S0KGZnfBGSIssfF54WSJh', 'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/6qqNVTkY8uBg9cP3Jd7DAH'}, 'href': 'https://api.spotify.com/v1/artists/6qqNVTkY8uBg9cP3Jd7DAH', 'id': '6qqNVTkY8uBg9cP3Jd7DAH', 'name': 'Billie Eilish', 'type': 'artist', 'uri': 'spotify:artist:6qqNVTkY8uBg9cP3Jd7DAH'}]}]\n",
      "Fetched from Spotify: Billie Eilish - WHEN WE ALL FALL ASLEEP, WHERE DO WE GO?\n",
      "Updated song: Billie Eilish - WHEN WE ALL FALL ASLEEP, WHERE DO WE GO? with details: {'album_type': 'album', 'total_tracks': 14, 'available_markets': ['AR', 'AU', 'AT', 'BE', 'BO', 'BR', 'BG', 'CA', 'CL', 'CO', 'CR', 'CY', 'CZ', 'DK', 'DO', 'DE', 'EC', 'EE', 'SV', 'FI', 'FR', 'GR', 'GT', 'HN', 'HK', 'HU', 'IS', 'IE', 'IT', 'LV', 'LT', 'LU', 'MY', 'MT', 'MX', 'NL', 'NZ', 'NI', 'NO', 'PA', 'PY', 'PE', 'PH', 'PL', 'PT', 'SG', 'SK', 'ES', 'SE', 'CH', 'TW', 'TR', 'UY', 'US', 'GB', 'AD', 'LI', 'MC', 'ID', 'JP', 'TH', 'VN', 'RO', 'IL', 'ZA', 'SA', 'AE', 'BH', 'QA', 'OM', 'KW', 'EG', 'MA', 'DZ', 'TN', 'LB', 'JO', 'PS', 'IN', 'BY', 'KZ', 'MD', 'UA', 'AL', 'BA', 'HR', 'ME', 'MK', 'RS', 'SI', 'KR', 'BD', 'PK', 'LK', 'GH', 'KE', 'NG', 'TZ', 'UG', 'AG', 'AM', 'BS', 'BB', 'BZ', 'BT', 'BW', 'BF', 'CV', 'CW', 'DM', 'FJ', 'GM', 'GE', 'GD', 'GW', 'GY', 'HT', 'JM', 'KI', 'LS', 'LR', 'MW', 'MV', 'ML', 'MH', 'FM', 'NA', 'NR', 'NE', 'PW', 'PG', 'WS', 'SM', 'ST', 'SN', 'SC', 'SL', 'SB', 'KN', 'LC', 'VC', 'SR', 'TL', 'TO', 'TT', 'TV', 'VU', 'AZ', 'BN', 'BI', 'KH', 'CM', 'TD', 'KM', 'GQ', 'SZ', 'GA', 'GN', 'KG', 'LA', 'MO', 'MR', 'MN', 'NP', 'RW', 'TG', 'UZ', 'ZW', 'BJ', 'MG', 'MU', 'MZ', 'AO', 'CI', 'DJ', 'ZM', 'CD', 'CG', 'IQ', 'LY', 'TJ', 'VE', 'ET', 'XK'], 'external_urls': {'spotify': 'https://open.spotify.com/album/0S0KGZnfBGSIssfF54WSJh'}, 'href': 'https://api.spotify.com/v1/albums/0S0KGZnfBGSIssfF54WSJh', 'id': '0S0KGZnfBGSIssfF54WSJh', 'images': [{'height': 640, 'url': 'https://i.scdn.co/image/ab67616d0000b27350a3147b4edd7701a876c6ce', 'width': 640}, {'height': 300, 'url': 'https://i.scdn.co/image/ab67616d00001e0250a3147b4edd7701a876c6ce', 'width': 300}, {'height': 64, 'url': 'https://i.scdn.co/image/ab67616d0000485150a3147b4edd7701a876c6ce', 'width': 64}], 'name': 'WHEN WE ALL FALL ASLEEP, WHERE DO WE GO?', 'release_date': '2019-03-29', 'release_date_precision': 'day', 'type': 'album', 'uri': 'spotify:album:0S0KGZnfBGSIssfF54WSJh', 'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/6qqNVTkY8uBg9cP3Jd7DAH'}, 'href': 'https://api.spotify.com/v1/artists/6qqNVTkY8uBg9cP3Jd7DAH', 'id': '6qqNVTkY8uBg9cP3Jd7DAH', 'name': 'Billie Eilish', 'type': 'artist', 'uri': 'spotify:artist:6qqNVTkY8uBg9cP3Jd7DAH'}]}\n",
      "Periodic export after processing 1 songs.\n",
      "Unique songs table updated with album info and saved to test_unique_songs.csv.\n",
      "      artistName           trackName  \\\n",
      "0  Billie Eilish             bad guy   \n",
      "1   Led Zeppelin  Stairway to Heaven   \n",
      "2     Pink Floyd    Comfortably Numb   \n",
      "\n",
      "                                      album  spotify_id  genre  label  \\\n",
      "0  WHEN WE ALL FALL ASLEEP, WHERE DO WE GO?         NaN    NaN    NaN   \n",
      "1                           Led Zeppelin IV         NaN    NaN    NaN   \n",
      "2                                  The Wall         NaN    NaN    NaN   \n",
      "\n",
      "  release_date album_type  \n",
      "0   2019-03-29      album  \n",
      "1   1971-11-08      album  \n",
      "2   1979-11-30      album  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ezrag\\AppData\\Local\\Temp\\ipykernel_22252\\2307597989.py:53: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  unique_songs.at[index, 'genre'] = ', '.join(album_details.get('genres', []))\n",
      "C:\\Users\\ezrag\\AppData\\Local\\Temp\\ipykernel_22252\\2307597989.py:55: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1971-11-08' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  unique_songs.at[index, 'release_date'] = album_details.get('release_date')\n",
      "C:\\Users\\ezrag\\AppData\\Local\\Temp\\ipykernel_22252\\2307597989.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'album' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  unique_songs.at[index, 'album_type'] = album_details.get('album_type')\n"
     ]
    }
   ],
   "source": [
    "def test_update_unique_songs_with_album_info():\n",
    "    # Sample data for testing\n",
    "    test_data = {\n",
    "        'artistName': ['Billie Eilish', 'Led Zeppelin', 'Pink Floyd'],\n",
    "        'trackName': ['bad guy', 'Stairway to Heaven', 'Comfortably Numb'],\n",
    "        'album': ['WHEN WE ALL FALL ASLEEP, WHERE DO WE GO?', 'Led Zeppelin IV', 'The Wall'],\n",
    "        'spotify_id': [None, None, None],\n",
    "        'genre': [None, None, None],\n",
    "        'label': [None, None, None],\n",
    "        'release_date': [None, None, None],\n",
    "        'album_type': [None, None, None]\n",
    "    }\n",
    "    test_df = pd.DataFrame(test_data)\n",
    "    \n",
    "    # Save test data to CSV\n",
    "    test_file = 'test_unique_songs.csv'\n",
    "    test_df.to_csv(test_file, index=False)\n",
    "    \n",
    "    # Run the update function\n",
    "    update_unique_songs_with_album_info(test_file, export_interval=1)\n",
    "    \n",
    "    # Load updated data and print it\n",
    "    updated_df = pd.read_csv(test_file)\n",
    "    print(updated_df)\n",
    "\n",
    "def get_audio_features(track_ids, access_token):\n",
    "    url = f\"https://api.spotify.com/v1/audio-features?ids={','.join(track_ids)}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.json()['audio_features']\n",
    "\n",
    "\n",
    "\n",
    "# Example usage of the test function\n",
    "test_update_unique_songs_with_album_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "access_token = get_spotify_access_token()\n",
    "\n",
    "# Example list of track IDs (replace with your own)\n",
    "track_ids = [\n",
    "    '7qiZfU4dY1lWllzX7mPBI3',  # Shape of You by Ed Sheeran\n",
    "    '4uLU6hMCjMI75M1A2tKUQC'   # Uptown Funk by Mark Ronson ft. Bruno Mars\n",
    "]\n",
    "\n",
    "audio_features = get_audio_features(track_ids, access_token)\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "df = pd.DataFrame(audio_features)\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the main steps to read data, export to CSV, and track unique songs\n",
    "user_id = get_user_id()\n",
    "num_chunks = get_num_chunks()\n",
    "base_path = '../wrapped_files/'  # Adjusting the relative path based on the notebook location\n",
    "unique_songs_file = 'unique_songs.csv'\n",
    "\n",
    "try:\n",
    "    df = read_and_process_data(user_id, num_chunks, base_path)\n",
    "    export_to_csv(df, user_id)\n",
    "    track_unique_songs(df, unique_songs_file)\n",
    "\n",
    "    print(\"Data processing complete!\")\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "unique_songs_file = 'unique_songs.csv'\n",
    "# Execute the function to update unique songs table with Spotify info\n",
    "update_unique_songs('unique_songs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the unique songs database\n",
    "unique_songs_file = 'unique_songs.csv'\n",
    "unique_songs = pd.read_csv(unique_songs_file)\n",
    "\n",
    "# Sort the database by artistName\n",
    "sorted_unique_songs = unique_songs.sort_values(by='artistName')\n",
    "\n",
    "# Save the sorted database to a new CSV file\n",
    "sorted_unique_songs_file = 'sorted_unique_songs.csv'\n",
    "sorted_unique_songs.to_csv(sorted_unique_songs_file, index=False)\n",
    "\n",
    "print(f\"Sorted unique songs database saved to {sorted_unique_songs_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unique songs data\n",
    "unique_songs_file = 'unique_songs.csv'\n",
    "unique_songs = pd.read_csv(unique_songs_file)\n",
    "\n",
    "# Get user ID and read processed listening data\n",
    "user_id = get_user_id()\n",
    "try:\n",
    "    listening_data = read_processed_data(user_id)\n",
    "    \n",
    "    # Fill in song info from unique songs database\n",
    "    filled_listening_data = fill_song_info(listening_data, unique_songs)\n",
    "    \n",
    "    # Export the filled listening data to a new CSV file\n",
    "    export_filled_data(filled_listening_data, user_id)\n",
    "\n",
    "    print(\"Data processing complete!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Processed data file not found for user ID: {user_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Analysis of Filled Listening Data</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Imports and Setup**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import noise\n",
    "\n",
    "# Ensure all necessary packages are installed\n",
    "# If not, you can install them using pip\n",
    "# pip install pandas requests matplotlib Pillow noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to read filled listening data\n",
    "def read_filled_listening_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Color Generation Functions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a random color\n",
    "def generate_random_color():\n",
    "    color = (random.randint(100, 255), random.randint(100, 255), random.randint(100, 255))\n",
    "    print(f\"Generated random color: {color}\")\n",
    "    return color\n",
    "\n",
    "# Function to generate a color close to a given color\n",
    "def generate_similar_color(color, variance=50):\n",
    "    r = min(max(color[0] + random.randint(-variance, variance), 0), 255)\n",
    "    g = min(max(color[1] + random.randint(-variance, variance), 0), 255)\n",
    "    b = min(max(color[2] + random.randint(-variance, variance), 0), 255)\n",
    "    similar_color = (r, g, b)\n",
    "    print(f\"Generated color similar to {color}: {similar_color}\")\n",
    "    return similar_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Abstract Background Generation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate an abstract background with dynamic colors\n",
    "def generate_abstract_background(width=1080, height=1920):\n",
    "    print(f\"Generating abstract background of size {width}x{height}\")\n",
    "    start_color = generate_random_color()\n",
    "    end_color = generate_similar_color(start_color)\n",
    "    \n",
    "    # Create a gradient based on the generated colors\n",
    "    gradient = np.linspace(start_color, end_color, width).astype(int)\n",
    "    gradient_cmap = plt.cm.colors.ListedColormap(gradient / 255.0)\n",
    "\n",
    "    x = np.linspace(-5, 5, width)\n",
    "    y = np.linspace(-5, 5, height)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    Z = np.sin(X**2 + Y**2) * np.cos(Y**2 - X**2)\n",
    "    \n",
    "    plt.figure(figsize=(width / 100, height / 100), dpi=100)\n",
    "    plt.imshow(Z, cmap=gradient_cmap, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('abstract_background.png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    background = Image.open('abstract_background.png')\n",
    "    background = background.resize((width, height))\n",
    "    print(\"Abstract background generated and saved as 'abstract_background.png'\")\n",
    "    return background\n",
    "\n",
    "# Function to generate Perlin noise\n",
    "def generate_perlin_noise(width, height, scale=100, seed=random.randint(0,500)):\n",
    "    print(f\"Generating Perlin noise of size {width}x{height} with scale {scale} and seed {seed}\")\n",
    "    shape = (width, height)\n",
    "    world = np.zeros(shape)\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            world[i][j] = noise.pnoise2(i / scale, j / scale, octaves=6, persistence=0.5, lacunarity=2.0, repeatx=1024, repeaty=1024, base=seed)\n",
    "    \n",
    "    norm_world = (world - np.min(world)) / (np.max(world) - np.min(world))\n",
    "    print(\"Perlin noise generated\")\n",
    "    return norm_world\n",
    "\n",
    "# Function to generate an abstract background with Perlin noise\n",
    "def generate_abstract_background_with_noise(width=1080, height=1920):\n",
    "    print(f\"Generating abstract background with Perlin noise of size {width}x{height}\")\n",
    "    noise_pattern = generate_perlin_noise(width, height)\n",
    "    \n",
    "    start_color = generate_random_color()\n",
    "    end_color = generate_similar_color(start_color)\n",
    "    gradient = np.linspace(start_color, end_color, width).astype(int)\n",
    "    gradient_cmap = plt.cm.colors.ListedColormap(gradient / 255.0)\n",
    "\n",
    "    plt.figure(figsize=(width / 100, height / 100), dpi=100)\n",
    "    plt.imshow(noise_pattern, cmap=gradient_cmap, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('abstract_background_with_noise.png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    background = Image.open('abstract_background_with_noise.png')\n",
    "    background = background.resize((width, height))\n",
    "    print(\"Abstract background with Perlin noise generated and saved as 'abstract_background_with_noise.png'\")\n",
    "    return background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Text Drawing Function**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw wrapped text\n",
    "def draw_wrapped_text(draw, text, position, font, max_width, fill):\n",
    "    print(f\"Drawing wrapped text: {text}\")\n",
    "    lines = []\n",
    "    words = text.split()\n",
    "    while words:\n",
    "        line = ''\n",
    "        while words and font.getbbox(line + words[0])[2] <= max_width:\n",
    "            line += (words.pop(0) + ' ')\n",
    "        lines.append(line)\n",
    "    y_offset = position[1]\n",
    "    for line in lines:\n",
    "        draw.text((position[0], y_offset), line, font=font, fill=fill)\n",
    "        y_offset += font.getbbox(line)[3]  # Use getbbox for line height\n",
    "    print(f\"Wrapped text drawn at position {position}\")\n",
    "    return y_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Album Art Download!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch and download all popular album art for each artist\n",
    "def download_all_album_art(df, top_artists):\n",
    "    print(\"Downloading album art for top artists\")\n",
    "    album_art = {}\n",
    "    for artist in top_artists:\n",
    "        artist_data = df[df['artistName'] == artist]\n",
    "        if artist_data.empty:\n",
    "            continue\n",
    "        \n",
    "        art_urls = artist_data['album_artwork'].value_counts().index.tolist()\n",
    "        downloaded = False\n",
    "        for art_url in art_urls:\n",
    "            try:\n",
    "                response = requests.get(art_url)\n",
    "                img = Image.open(BytesIO(response.content))\n",
    "                \n",
    "                img_path = os.path.join(\"albums\", f'{artist}_album_art.jpg')\n",
    "                img.save(img_path)\n",
    "                \n",
    "                album_art[artist] = img_path\n",
    "                downloaded = True\n",
    "                print(f\"Downloaded album art for {artist}: {img_path}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {art_url} for {artist}: {e}\")\n",
    "                continue\n",
    "        if not downloaded:\n",
    "            print(f\"Could not download album art for {artist}\")\n",
    "    return album_art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Function to create and save layout images\n",
    "def create_layout_image(title, top_artists, album_art, file_name, user_id, background):\n",
    "    print(f\"Creating layout image: {file_name}\")\n",
    "    width, height = background.size\n",
    "    image = background.copy()\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Define fonts\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 40)\n",
    "        title_font = ImageFont.truetype(\"arial.ttf\", 60)\n",
    "        user_id_font = ImageFont.truetype(\"arial.ttf\", 30)\n",
    "    except IOError:\n",
    "        # In case the fonts are not available on the system\n",
    "        font = ImageFont.load_default()\n",
    "        title_font = ImageFont.load_default()\n",
    "        user_id_font = ImageFont.load_default()\n",
    "    \n",
    "    # Draw title and user ID\n",
    "    draw.text((width / 2, 50), title, font=title_font, fill=\"white\", anchor=\"mm\")\n",
    "    draw.text((width / 2, 150), f\"User: {user_id}\", font=user_id_font, fill=\"white\", anchor=\"mm\")\n",
    "\n",
    "    y_offset = 250\n",
    "    x_offset = 50\n",
    "\n",
    "    for rank, (artist, value) in enumerate(top_artists.items(), start=1):\n",
    "        if artist not in album_art:\n",
    "            continue\n",
    "        \n",
    "        art = Image.open(album_art[artist]).resize((100, 100))\n",
    "        image.paste(art, (x_offset, y_offset))\n",
    "        \n",
    "        text = f\"{rank}. {artist}: {value}\"\n",
    "        draw.text((x_offset + 120, y_offset + 30), text, font=font, fill=\"white\")\n",
    "        y_offset += 120\n",
    "\n",
    "    image.save(file_name)\n",
    "    print(f\"Layout image saved as {file_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**<h2>Data Processing Functions</h2>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate percentage listened for each track\n",
    "def calculate_percentage_listened(df):\n",
    "    print(\"Calculating percentage listened for each track\")\n",
    "    df['percentage_listened'] = df['msPlayed'] / df['duration_ms']\n",
    "    print(\"Percentage listened calculated\")\n",
    "    return df\n",
    "\n",
    "# Function to calculate top listened-to artists by listening time\n",
    "def top_artists_by_time(df, top_n=10):\n",
    "    print(f\"Calculating top {top_n} artists by listening time\")\n",
    "    artist_time = df.groupby('artistName')['msPlayed'].sum().sort_values(ascending=False).head(top_n)\n",
    "    artist_time_seconds = artist_time / 1000  # Convert milliseconds to seconds\n",
    "    print(\"Top artists by listening time calculated\")\n",
    "    return artist_time_seconds\n",
    "\n",
    "# Function to calculate top listened-to artists by count\n",
    "def top_artists_by_count(df, top_n=10):\n",
    "    print(f\"Calculating top {top_n} artists by count\")\n",
    "    artist_count = df['artistName'].value_counts().head(top_n)\n",
    "    print(\"Top artists by count calculated\")\n",
    "    return artist_count\n",
    "\n",
    "# Function to calculate top listened-to artists by weighted listening time\n",
    "def top_artists_by_weighted_time(df, top_n=10):\n",
    "    print(f\"Calculating top {top_n} artists by weighted listening time\")\n",
    "    artist_weighted_time = df.groupby('artistName')['percentage_listened'].sum().sort_values(ascending=False).head(top_n)\n",
    "    print(\"Top artists by weighted listening time calculated\")\n",
    "    return artist_weighted_time\n",
    "\n",
    "# Function to read filled listening data\n",
    "def read_filled_listening_data(file_path):\n",
    "    print(f\"Reading filled listening data from {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Filled listening data read successfully\")\n",
    "    return df\n",
    "\n",
    "# Function to calculate top listened-to songs by play count\n",
    "def top_songs_by_count(df, top_n=10):\n",
    "    song_count = df['trackName'].value_counts().head(top_n)\n",
    "    return song_count\n",
    "\n",
    "# Function to calculate total listening time per artist\n",
    "def total_listening_time_per_artist(df, top_n=10):\n",
    "    artist_time = df.groupby('artistName')['msPlayed'].sum().sort_values(ascending=False).head(top_n)\n",
    "    return artist_time\n",
    "\n",
    "# Function to calculate average listening time per song\n",
    "def average_listening_time_per_song(df, top_n=10):\n",
    "    song_time = df.groupby('trackName')['msPlayed'].mean().sort_values(ascending=False).head(top_n)\n",
    "    return song_time\n",
    "\n",
    "# Function to calculate total listening time per album\n",
    "def total_listening_time_per_album(df, top_n=10):\n",
    "    album_time = df.groupby('album')['msPlayed'].sum().sort_values(ascending=False).head(top_n)\n",
    "    return album_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print column names\n",
    "def print_column_names(df):\n",
    "    print(\"Column names in the filled listening data:\")\n",
    "    for column in df.columns:\n",
    "        print(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Generate Instagram story-sized abstract background with Perlin noise and custom colormap\n",
    "    background = generate_abstract_background_with_noise(1080, 1920)\n",
    "\n",
    "    # Load unique songs data\n",
    "    unique_songs_file = 'unique_songs.csv'\n",
    "    unique_songs = pd.read_csv(unique_songs_file)\n",
    "\n",
    "    # Get user ID and construct the file path\n",
    "    user_name = get_user_id()\n",
    "    file_path = f'{user_name}_listening_data.csv'\n",
    "\n",
    "    try:\n",
    "        # Read the filled listening data\n",
    "        filled_listening_data = read_filled_listening_data(file_path)\n",
    "        \n",
    "        # Calculate percentage listened for each track\n",
    "        filled_listening_data = calculate_percentage_listened(filled_listening_data)\n",
    "        \n",
    "        # Calculate top listened-to artists\n",
    "        top_artists_count = top_artists_by_count(filled_listening_data).head(5)\n",
    "        top_artists_time = top_artists_by_time(filled_listening_data).head(5)\n",
    "        top_artists_weighted_time = top_artists_by_weighted_time(filled_listening_data).head(5)\n",
    "        \n",
    "        # Combine all top artists to ensure all album art is downloaded\n",
    "        all_top_artists = top_artists_count.index.union(top_artists_time.index).union(top_artists_weighted_time.index)\n",
    "        \n",
    "        # Download the most common album art for each artist\n",
    "        album_art = download_all_album_art(filled_listening_data, all_top_artists)\n",
    "        \n",
    "        # Create layout images with user ID in the file name and abstract background\n",
    "        create_layout_image(\"Top Artists by Count\", top_artists_count, album_art, f\"{user_name}_spotify_wrapped_top_artists_count.png\", user_name, background)\n",
    "        create_layout_image(\"Top Artists by Listening Time (minutes)\", {k: v / 60 for k, v in top_artists_time.items()}, album_art, f\"{user_name}_spotify_wrapped_top_artists_time.png\", user_name, background)\n",
    "        create_layout_image(\"Top Artists by Weighted Listening Time\", top_artists_weighted_time, album_art, f\"{user_name}_spotify_wrapped_top_artists_weighted_time.png\", user_name, background)\n",
    "        \n",
    "        print(\"Data processing and layout creation complete!\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
