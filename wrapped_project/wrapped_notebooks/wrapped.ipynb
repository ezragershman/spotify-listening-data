{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Environmental Setup</h1>\n",
    "\n",
    "<strong>Install Required Packages:</strong> Install necessary packages using pip.\n",
    "\n",
    "<strong>Load Environment Variables:</strong> Load environment variables for Spotify API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r C:\\Users\\ezrag\\OneDrive\\Documents\\GitHub\\spotify-listening-data\\requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv # type: ignore\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "SPOTIFY_CLIENT_ID = os.getenv('SPOTIFY_CLIENT_ID')\n",
    "SPOTIFY_CLIENT_SECRET = os.getenv('SPOTIFY_CLIENT_SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import queue\n",
    "import random\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import quote\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Utility Functions and Initialization</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>User Input Functions</h3>\n",
    "<strong>Get User ID:</strong> Function to get user ID from input.\n",
    "\n",
    "<strong>Get Number of Data Chunks:</strong> Function to get the number of data chunks from input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_id():\n",
    "    \"\"\"\n",
    "    Prompt the user to enter their ID and return it in lowercase.\n",
    "\n",
    "    This function prompts the user to enter their ID, converts it to lowercase,\n",
    "    and returns the result.\n",
    "\n",
    "    Returns:\n",
    "    str: The user's ID in lowercase.\n",
    "    \"\"\"\n",
    "    user_id = input(\"Enter the user's ID: \").lower()\n",
    "    return user_id\n",
    "\n",
    "def get_num_chunks():\n",
    "    \"\"\"\n",
    "    Prompt the user to enter the number of data chunks.\n",
    "\n",
    "    This function prompts the user to enter the number of data chunks,\n",
    "    converts the input to an integer, and returns the result.\n",
    "\n",
    "    Returns:\n",
    "    int: The number of data chunks entered by the user.\n",
    "    \"\"\"\n",
    "    num_chunks = int(input(\"Enter the number of chunks: \"))\n",
    "    return num_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Reading and Processing</h3>\n",
    "<strong>Read and Process Data:</strong> Function to read and process data from multiple JSON files.\n",
    "\n",
    "<strong>Export Data to CSV:</strong> Function to export processed data to a CSV file.\n",
    "\n",
    "<strong>Track Unique Songs:</strong> Function to track unique songs and update unique songs list.\n",
    "\n",
    "<strong>Safe Literal Eval:</strong> Function to safely evaluate literals from strings.\n",
    "\n",
    "<strong>Expand Artists Involved:</strong> Function to expand artists involved in each track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_process_data(user_id, num_chunks, base_path='wrapped_files/'):\n",
    "    \"\"\"\n",
    "    Read and process data from multiple JSON files.\n",
    "\n",
    "    This function reads data from multiple JSON files specified by the user ID\n",
    "    and number of chunks, processes the data, and returns it as a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    user_id (str): The user's ID.\n",
    "    num_chunks (int): The number of JSON files (chunks) to read.\n",
    "    base_path (str, optional): The base path to the directory containing the JSON files. \n",
    "                               Defaults to 'wrapped_files/'.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the processed data.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If no data files were found or all files were empty.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        json_file = os.path.join(base_path, f'{user_id}_music_{i}.json')\n",
    "        print(f\"Checking for file: {json_file}\")\n",
    "        \n",
    "        if not os.path.exists(json_file):\n",
    "            print(f\"File not found: {json_file}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Reading data from {json_file}\")\n",
    "        with open(json_file, 'r', encoding='utf-8') as file:\n",
    "            data_list = json.load(file)\n",
    "            all_data.extend(data_list)\n",
    "    \n",
    "    if not all_data:\n",
    "        raise ValueError(\"No data files were found or all were empty.\")\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    df['user_id'] = user_id\n",
    "    df['endTime'] = pd.to_datetime(df['endTime'])\n",
    "    \n",
    "    print(f\"Data read successfully for {len(df)} records.\")\n",
    "    return df\n",
    "\n",
    "def export_to_csv(df, user_id):\n",
    "    \"\"\"\n",
    "    Export data to a CSV file.\n",
    "\n",
    "    This function exports the provided DataFrame to a CSV file named with the user's ID.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the data to be exported.\n",
    "    user_id (str): The user's ID.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    csv_file = f'{user_id}_listening_data.csv'\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Data exported to {csv_file}\")\n",
    "\n",
    "def track_unique_songs(df, unique_songs_file):\n",
    "    \"\"\"\n",
    "    Track unique songs in the given DataFrame.\n",
    "\n",
    "    This function ensures the DataFrame includes the necessary columns,\n",
    "    drops duplicates within the current DataFrame, and combines new unique songs\n",
    "    with existing unique songs from a CSV file. The combined unique songs are then\n",
    "    saved back to the CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the data to be processed.\n",
    "    unique_songs_file (str): The file path to the CSV file where unique songs are stored.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Ensure DataFrame includes necessary columns\n",
    "    required_columns = ['trackName', 'artistName', 'external_urls']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    \n",
    "    # Drop duplicates within the current DataFrame\n",
    "    new_unique_songs = df[required_columns].drop_duplicates()\n",
    "    print(f\"Tracking {len(new_unique_songs)} unique songs.\")\n",
    "    \n",
    "    try:\n",
    "        # Attempt to load existing unique songs from the CSV file\n",
    "        existing_unique_songs = pd.read_csv(unique_songs_file)\n",
    "        print(f\"Loaded {len(existing_unique_songs)} existing unique songs.\")\n",
    "    except FileNotFoundError:\n",
    "        # If the file does not exist, start with an empty DataFrame\n",
    "        existing_unique_songs = pd.DataFrame(columns=required_columns)\n",
    "        print(\"No existing unique songs file found. Starting fresh.\")\n",
    "    \n",
    "    # Combine new and existing unique songs\n",
    "    combined_unique_songs = pd.concat([existing_unique_songs, new_unique_songs]).drop_duplicates()\n",
    "    \n",
    "    # Save the combined DataFrame to the CSV file\n",
    "    combined_unique_songs.to_csv(unique_songs_file, index=False)\n",
    "    print(f\"Updated unique songs saved to {unique_songs_file}.\")\n",
    "\n",
    "\n",
    "def safe_literal_eval(val):\n",
    "    \"\"\"\n",
    "    Safely evaluate a string containing a Python literal or container display.\n",
    "\n",
    "    This function attempts to safely evaluate a string containing a Python literal\n",
    "    or container display (e.g., list, dictionary). If the evaluation fails due to\n",
    "    a ValueError or SyntaxError, the original value is returned.\n",
    "\n",
    "    Parameters:\n",
    "    val (str): The string to be evaluated.\n",
    "\n",
    "    Returns:\n",
    "    object: The evaluated Python object, or the original value if evaluation fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return val\n",
    "\n",
    "def expand_artists_involved(df):\n",
    "    \"\"\"\n",
    "    Expand and standardize the artists involved in each track.\n",
    "\n",
    "    This function adds the main artist to the 'artists_involved' list if not already present,\n",
    "    standardizes the capitalization of artist names for counting purposes, and ensures that\n",
    "    'artists_involved' is processed correctly from a string representation of a list. It keeps\n",
    "    the original artist names for display purposes.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing listening data with 'artists_involved' as\n",
    "                           a string representation of a list.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: Expanded DataFrame with artists involved in each track.\n",
    "    \"\"\"\n",
    "    print(\"Expanding and standardizing artists involved in each track\")\n",
    "    \n",
    "    df['artists_involved'] = df['artists_involved'].replace({pd.NA: '[]'})\n",
    "    df['artists_involved'] = df['artists_involved'].apply(safe_literal_eval)\n",
    "    \n",
    "    def add_main_artist(row):\n",
    "        if isinstance(row['artists_involved'], list):\n",
    "            if row['artistName'] not in row['artists_involved']:\n",
    "                row['artists_involved'].append(row['artistName'])\n",
    "        else:\n",
    "            row['artists_involved'] = [row['artistName']]\n",
    "        return row\n",
    "\n",
    "    df = df.apply(add_main_artist, axis=1)\n",
    "    \n",
    "    df['standardized_artists'] = df['artists_involved'].apply(lambda x: [artist.lower() for artist in x])\n",
    "    df['standardized_artists_str'] = df['standardized_artists'].apply(lambda x: ', '.join(x))  # Convert lists to strings\n",
    "    df['original_artists'] = df['artists_involved']  # Preserve original names\n",
    "\n",
    "    print(\"Artists involved expanded and standardized successfully\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Spotify API Integration</h3>\n",
    "<strong>Get Spotify Access Token:</strong> Function to obtain Spotify access token using client credentials.\n",
    "\n",
    "<strong>Get Song Details:</strong> Function to retrieve song details from Spotify API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spotify_access_token(client_id, client_secret):\n",
    "    \"\"\"\n",
    "    Get Spotify access token using client credentials.\n",
    "\n",
    "    This function sends a request to the Spotify API to get an access token \n",
    "    using the client credentials (client ID and client secret). The token is \n",
    "    required for further API requests.\n",
    "\n",
    "    Parameters:\n",
    "    client_id (str): The Spotify client ID.\n",
    "    client_secret (str): The Spotify client secret.\n",
    "\n",
    "    Returns:\n",
    "    str: The access token used for further API requests.\n",
    "    \"\"\"\n",
    "    auth_url = 'https://accounts.spotify.com/api/token'\n",
    "    auth_response = requests.post(auth_url, {\n",
    "        'grant_type': 'client_credentials',\n",
    "        'client_id': client_id,\n",
    "        'client_secret': client_secret,\n",
    "    })\n",
    "    \n",
    "    # Parse the authentication response and extract access token\n",
    "    auth_response_data = auth_response.json()\n",
    "    return auth_response_data['access_token']\n",
    "\n",
    "def get_song_details(artist_name, track_name, access_token):\n",
    "    \"\"\"\n",
    "    Get song details from Spotify API using search query.\n",
    "\n",
    "    This function sends a search request to the Spotify API using the given \n",
    "    artist name and track name. It retrieves detailed information about the \n",
    "    song including album, release date, popularity, duration, track number, \n",
    "    album artwork, external URLs, artists involved, and genres.\n",
    "\n",
    "    Parameters:\n",
    "    artist_name (str): The name of the artist.\n",
    "    track_name (str): The name of the track.\n",
    "    access_token (str): The Spotify API access token.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing detailed information about the song. \n",
    "          None if no track is found.\n",
    "    \"\"\"\n",
    "    search_url = 'https://api.spotify.com/v1/search'\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {access_token}'\n",
    "    }\n",
    "    query = f'artist:{quote(artist_name)} track:{quote(track_name)}'\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'type': 'track',\n",
    "        'limit': 1\n",
    "    }\n",
    "    \n",
    "    # Send request to Spotify API to search for the track\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    response_data = response.json()\n",
    "    \n",
    "    print(f\"Search query: artist:{artist_name} track:{track_name}\")\n",
    "    print(\"Search response data:\", response_data)\n",
    "    \n",
    "    if 'tracks' in response_data and response_data['tracks']['items']:\n",
    "        track_info = response_data['tracks']['items'][0]\n",
    "        \n",
    "        # Get artist details to fetch genres in batches\n",
    "        artist_ids = [artist['id'] for artist in track_info['artists']]\n",
    "        artist_genres = []\n",
    "\n",
    "        batch_size = 50\n",
    "        for i in range(0, len(artist_ids), batch_size):\n",
    "            batch_ids = artist_ids[i:i + batch_size]\n",
    "            artist_url = f\"https://api.spotify.com/v1/artists?ids={','.join(batch_ids)}\"\n",
    "            retries = 5\n",
    "            delay = 1\n",
    "            \n",
    "            while retries > 0:\n",
    "                artist_response = requests.get(artist_url, headers=headers)\n",
    "                \n",
    "                if artist_response.status_code == 200:\n",
    "                    try:\n",
    "                        artist_data = artist_response.json()['artists']\n",
    "                        for artist in artist_data:\n",
    "                            if 'genres' in artist:\n",
    "                                artist_genres.extend(artist['genres'])\n",
    "                        break  # Exit the retry loop if successful\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Error decoding JSON for batch {batch_ids}: {e}\")\n",
    "                elif artist_response.status_code == 429:\n",
    "                    retry_after = int(artist_response.headers.get('Retry-After', delay))\n",
    "                    print(f\"Rate limited. Retrying after {retry_after} seconds.\")\n",
    "                    time.sleep(retry_after)\n",
    "                else:\n",
    "                    print(f\"Request failed with status code {artist_response.status_code}\")\n",
    "                \n",
    "                retries -= 1\n",
    "                time.sleep(delay)\n",
    "                delay *= 2  # Exponential backoff\n",
    "            \n",
    "            if retries == 0:\n",
    "                print(f\"Failed to fetch genres for batch {batch_ids} after {retries} attempts.\")\n",
    "        \n",
    "        # Ensure the genres list is ordered and unique\n",
    "        artist_genres = sorted(set(artist_genres))\n",
    "        \n",
    "        # Check if album images and external URLs are present\n",
    "        album_artwork = track_info['album']['images'][0]['url'] if 'images' in track_info['album'] and track_info['album']['images'] else None\n",
    "        external_urls = track_info['external_urls']['spotify'] if 'external_urls' in track_info else None\n",
    "        \n",
    "        song_details = {\n",
    "            'spotify_id': track_info['id'],\n",
    "            'album': track_info['album']['name'],\n",
    "            'release_date': track_info['album']['release_date'],\n",
    "            'popularity': track_info['popularity'],\n",
    "            'duration_ms': track_info['duration_ms'],\n",
    "            'track_number': track_info['track_number'],\n",
    "            'album_artwork': album_artwork,\n",
    "            'external_urls': external_urls,\n",
    "            'artists_involved': [artist['name'] for artist in track_info['artists']],\n",
    "            'genres': artist_genres\n",
    "        }\n",
    "        \n",
    "        print(\"Song details:\", song_details)\n",
    "        return song_details\n",
    "    else:\n",
    "        print(\"No tracks found for the given query.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Multi-Threading for Data Processing</h3>\n",
    "<strong>Worker Thread:</strong> Function for worker threads to process each song in the queue.\n",
    "\n",
    "<strong>Update Unique Songs:</strong> Function to update the unique songs table with Spotify information using threading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_thread(queue, unique_songs, unique_songs_file, access_token, export_interval, lock, start_time):\n",
    "    \"\"\"\n",
    "    Worker function to process each song in the queue.\n",
    "\n",
    "    This function processes each song in the queue by fetching song details \n",
    "    from the Spotify API and updating the unique songs DataFrame. It also \n",
    "    periodically exports the updated DataFrame to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    queue (Queue): The queue containing songs to be processed.\n",
    "    unique_songs (DataFrame): The DataFrame of unique songs.\n",
    "    unique_songs_file (str): The file path for the unique songs CSV.\n",
    "    access_token (str): The Spotify API access token.\n",
    "    export_interval (int): The interval at which the DataFrame is exported to the CSV file.\n",
    "    lock (Lock): The lock to ensure thread-safe operations.\n",
    "    start_time (float): The start time of the processing.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    while not queue.empty():\n",
    "        index, row = queue.get()\n",
    "        if pd.notna(row['spotify_id']):\n",
    "            queue.task_done()\n",
    "            continue\n",
    "        \n",
    "        artist_name = row['artistName']\n",
    "        track_name = row['trackName']\n",
    "        song_details = get_song_details(artist_name, track_name, access_token)\n",
    "        \n",
    "        if song_details:\n",
    "            with lock:\n",
    "                unique_songs.at[index, 'spotify_id'] = song_details['spotify_id']\n",
    "                unique_songs.at[index, 'album'] = song_details['album']\n",
    "                unique_songs.at[index, 'release_date'] = song_details['release_date']\n",
    "                unique_songs.at[index, 'popularity'] = song_details['popularity']\n",
    "                unique_songs.at[index, 'duration_ms'] = song_details['duration_ms']\n",
    "                unique_songs.at[index, 'track_number'] = song_details['track_number']\n",
    "                unique_songs.at[index, 'album_artwork'] = song_details['album_artwork']\n",
    "                unique_songs.at[index, 'external_urls'] = song_details['external_urls']\n",
    "                unique_songs.at[index, 'artists_involved'] = song_details['artists_involved']\n",
    "                unique_songs.at[index, 'genres'] = song_details['genres']\n",
    "        \n",
    "        if (index + 1) % export_interval == 0:\n",
    "            with lock:\n",
    "                print(f\"Exporting data at index {index}. Elapsed time: {time.time() - start_time:.2f} seconds.\")\n",
    "                unique_songs.to_csv(unique_songs_file, index=False)\n",
    "        \n",
    "        queue.task_done()\n",
    "\n",
    "def update_unique_songs(unique_songs_file='unique_songs.csv', export_interval=50):\n",
    "    \"\"\"\n",
    "    Main function to update unique songs table with Spotify info using threading.\n",
    "\n",
    "    This function loads the unique songs data, checks for missing columns, and \n",
    "    updates the table with Spotify information using multiple threads. The \n",
    "    updated table is periodically exported to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    unique_songs_file (str, optional): The file path for the unique songs CSV. Defaults to 'unique_songs.csv'.\n",
    "    export_interval (int, optional): The interval at which the DataFrame is exported to the CSV file. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Load unique songs data from CSV file\n",
    "    print(f\"Loading unique songs from {unique_songs_file}\")\n",
    "    unique_songs = pd.read_csv(unique_songs_file)\n",
    "    print(f\"Loaded {len(unique_songs)} unique songs\")\n",
    "\n",
    "    # Check if the columns already exist, if not, create them\n",
    "    columns = ['spotify_id', 'album', 'release_date', 'popularity', 'duration_ms', 'track_number', 'album_artwork', 'external_urls', 'artists_involved', 'genre']\n",
    "    for column in columns:\n",
    "        if column not in unique_songs.columns:\n",
    "            unique_songs[column] = None\n",
    "\n",
    "    # Get Spotify access token\n",
    "    access_token = get_spotify_access_token(SPOTIFY_CLIENT_ID, SPOTIFY_CLIENT_SECRET)\n",
    "    \n",
    "    # Create a queue and add songs to be processed\n",
    "    q = queue.Queue()\n",
    "    for index, row in unique_songs.iterrows():\n",
    "        q.put((index, row))\n",
    "\n",
    "    # Create a lock for thread-safe operations\n",
    "    lock = threading.Lock()\n",
    "    start_time = time.time()\n",
    "    threads = []\n",
    "    for _ in range(10):  # Adjust number of threads as needed\n",
    "        thread = threading.Thread(target=worker_thread, args=(q, unique_songs, unique_songs_file, access_token, export_interval, lock, start_time))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "    \n",
    "    # Wait for all threads to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    # Remove duplicates based on 'external_urls'\n",
    "    unique_songs = drop_duplicates_by_external_urls(unique_songs)\n",
    "    \n",
    "    # Final export\n",
    "    print(f\"Final export. Total time taken: {time.time() - start_time:.2f} seconds.\")\n",
    "    unique_songs.to_csv(unique_songs_file, index=False)\n",
    "    print(f\"Unique songs table updated with Spotify info and saved to {unique_songs_file}.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Deduplication and Filling</h3>\n",
    "<strong>Drop Duplicates:</strong> Function to drop duplicate songs based on external URLs.\n",
    "\n",
    "<strong>Fill Song Info:</strong> Function to fill in song information from the unique songs database.\n",
    "\n",
    "<strong>Read Processed Data:</strong> Function to read processed listening data.\n",
    "\n",
    "<strong>Export Filled Data:</strong> Function to export filled listening data to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop duplicates based on external_urls\n",
    "def drop_duplicates_by_external_urls(data):\n",
    "    \"\"\"\n",
    "    This function drops duplicate rows based on the 'external_urls' column, \n",
    "    but retains rows where 'external_urls' is blank.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing the song data with 'external_urls' column.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with duplicates removed based on 'external_urls'.\n",
    "    \"\"\"\n",
    "    # Identify rows where external_urls is not blank\n",
    "    non_blank_urls = data['external_urls'].notna()\n",
    "    \n",
    "    # Drop duplicates only where external_urls is not blank\n",
    "    data_non_blank = data[non_blank_urls].drop_duplicates(subset=['external_urls'])\n",
    "    \n",
    "    # Combine the non-duplicated rows with the rows where external_urls is blank\n",
    "    data_final = pd.concat([data_non_blank, data[~non_blank_urls]], ignore_index=True)\n",
    "    \n",
    "    return data_final\n",
    "\n",
    "def fill_song_info(listening_data, unique_songs):\n",
    "    \"\"\"\n",
    "    Fill in song information from the unique songs database.\n",
    "\n",
    "    This function filters out rows with 'unknown' artists in the listening data,\n",
    "    then merges the listening data with the unique songs database on 'artistName'\n",
    "    and 'trackName' to fill in additional song information.\n",
    "\n",
    "    Parameters:\n",
    "    listening_data (pandas.DataFrame): DataFrame containing the user's listening data.\n",
    "    unique_songs (pandas.DataFrame): DataFrame containing the unique songs database.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with filled in song information.\n",
    "    \"\"\"\n",
    "    # Filter out rows where artistName is 'unknown'\n",
    "    listening_data_filtered = listening_data[~listening_data['artistName'].str.lower().isin(['unknown', 'unknown artist'])]\n",
    "    # Merge listening data with unique songs data on 'artistName' and 'trackName'\n",
    "    filled_data = pd.merge(listening_data_filtered, unique_songs, on=['artistName', 'trackName'], how='left')\n",
    "    return filled_data\n",
    "\n",
    "def read_processed_data(user_id):\n",
    "    \"\"\"\n",
    "    Read processed listening data from a CSV file.\n",
    "\n",
    "    This function reads a CSV file containing the user's processed listening data\n",
    "    and returns it as a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    user_id (str): The user's ID.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing the user's processed listening data.\n",
    "    \"\"\"\n",
    "    csv_file = f'{user_id}_listening_data.csv'  # Example file path, adjust as needed\n",
    "    listening_data = pd.read_csv(csv_file)\n",
    "    return listening_data\n",
    "\n",
    "def export_filled_data(filled_data, user_id):\n",
    "    \"\"\"\n",
    "    Export filled listening data to a CSV file.\n",
    "\n",
    "    This function exports the provided DataFrame containing filled listening data\n",
    "    to a CSV file named with the user's ID.\n",
    "\n",
    "    Parameters:\n",
    "    filled_data (pandas.DataFrame): The DataFrame containing the filled listening data.\n",
    "    user_id (str): The user's ID.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    filled_csv_file = f'{user_id}_listening_data.csv'\n",
    "    filled_data.to_csv(filled_csv_file, index=False)\n",
    "    print(f\"Filled listening data exported to {filled_csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Analysis & Visualization</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Listening Time Analysis</h3>\n",
    "<strong>Total Listening Time per User:</strong> Function to calculate total listening time per user.\n",
    "\n",
    "<strong>Total Listening Time per Artist:</strong> Function to calculate total listening time per artist.\n",
    "\n",
    "<strong>Biggest Listening Date:</strong> Function to identify the biggest listening date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_listening_time_per_user(df):\n",
    "    \"\"\"\n",
    "    Calculate total listening time per user.\n",
    "\n",
    "    This function calculates the total listening time for each user by summing\n",
    "    the 'msPlayed' column grouped by 'user_id' and converting the time from\n",
    "    milliseconds to hours.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the listening data with 'user_id' and 'msPlayed' columns.\n",
    "\n",
    "    Returns:\n",
    "    pandas.Series: Series containing the total listening time per user in hours.\n",
    "    \"\"\"\n",
    "    print(\"Calculating total listening time per user\")\n",
    "    total_time = df.groupby('user_id')['msPlayed'].sum()\n",
    "    total_time_hours = total_time / (1000 * 60 * 60)  # Convert milliseconds to hours\n",
    "    print(\"Total listening time per user calculated\")\n",
    "    return total_time_hours\n",
    "\n",
    "def total_listening_time_per_artist(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Calculate total listening time per artist.\n",
    "\n",
    "    This function expands the artists involved in each track, calculates the total\n",
    "    listening time for each artist by summing the 'msPlayed' column grouped by 'artists_involved',\n",
    "    and returns the top N artists based on their total listening time.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the listening data with 'artists_involved' and 'msPlayed' columns.\n",
    "    top_n (int, optional): Number of top artists to return. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "    pandas.Series: Series containing the total listening time per artist, sorted in descending order.\n",
    "    \"\"\"\n",
    "    df_expanded = expand_artists_involved(df)\n",
    "    artist_time = df_expanded.groupby('artists_involved')['msPlayed'].sum().sort_values(ascending=False).head(top_n)\n",
    "    return artist_time\n",
    "\n",
    "def biggest_listening_date(df):\n",
    "    \"\"\"\n",
    "    Identify the biggest listening date.\n",
    "\n",
    "    This function converts the 'endTime' column to datetime, groups the data by date,\n",
    "    sums the listening time ('msPlayed') for each date, and identifies the date with\n",
    "    the highest total listening time.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the listening data with 'endTime' and 'msPlayed' columns.\n",
    "\n",
    "    Returns:\n",
    "    datetime.date: The date with the highest total listening time.\n",
    "    \"\"\"\n",
    "    print(\"Identifying the biggest listening date\")\n",
    "    df['endTime'] = pd.to_datetime(df['endTime'])\n",
    "    biggest_date = df.groupby(df['endTime'].dt.date)['msPlayed'].sum().idxmax()\n",
    "    print(\"Biggest listening date identified\")\n",
    "    return biggest_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Artist Analysis</h3>\n",
    "\n",
    "<strong>Top Artists by Time:</strong> Function to calculate top artists by listening time.\n",
    "\n",
    "<strong>Top Artists by Count:</strong> Function to calculate top artists by count.\n",
    "\n",
    "<strong>Top Artists by Weighted Time:</strong> Function to calculate top artists by weighted listening time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_artists_by_time(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Calculate top listened-to artists by listening time.\n",
    "\n",
    "    This function expands the artists involved in each track, calculates the total\n",
    "    listening time for each artist by summing the 'msPlayed' column grouped by \n",
    "    'standardized_artists', and returns the top N artists based on their total \n",
    "    listening time, converted to seconds.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the listening data with 'standardized_artists' and 'msPlayed' columns.\n",
    "    top_n (int, optional): Number of top artists to return. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "    pandas.Series: Series containing the total listening time per artist in seconds, sorted in descending order.\n",
    "    \"\"\"\n",
    "    print(f\"Calculating top {top_n} artists by listening time\")\n",
    "    artist_time = df.explode('standardized_artists').groupby('standardized_artists')['msPlayed'].sum().sort_values(ascending=False).head(top_n)\n",
    "    artist_time_seconds = artist_time / 1000  # Convert milliseconds to seconds\n",
    "    print(\"Top artists by listening time calculated\")\n",
    "    return artist_time_seconds\n",
    "\n",
    "def top_artists_by_count(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Calculate top listened-to artists by count.\n",
    "\n",
    "    This function expands the artists involved in each track, calculates the count\n",
    "    of occurrences for each artist using the 'standardized_artists' column, and returns\n",
    "    the top N artists based on their count.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the listening data with 'standardized_artists' column.\n",
    "    top_n (int, optional): Number of top artists to return. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "    pandas.Series: Series containing the count of occurrences per artist, sorted in descending order.\n",
    "    \"\"\"\n",
    "    print(f\"Calculating top {top_n} artists by count\")\n",
    "    artist_count = df.explode('standardized_artists')['standardized_artists'].value_counts().head(top_n)\n",
    "    print(\"Top artists by count calculated\")\n",
    "    return artist_count\n",
    "\n",
    "\n",
    "def top_artists_by_weighted_listens(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Calculate top listened-to artists by weighted listening time.\n",
    "\n",
    "    This function expands the artists involved in each track, calculates the weighted\n",
    "    listening time for each artist, and returns the top N artists based on their\n",
    "    weighted listening time.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the listening data with 'artistName',\n",
    "                           'artists_involved', and 'percentage_listened' columns.\n",
    "    top_n (int, optional): Number of top artists to return. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing the top N artists and their weighted listening times.\n",
    "    \"\"\"\n",
    "    print(f\"Calculating top {top_n} artists by weighted listening time\")\n",
    "    df_expanded = df.explode('standardized_artists')\n",
    "    \n",
    "    weighted_times = {}\n",
    "\n",
    "    for index, row in df_expanded.iterrows():\n",
    "        artist_name = row['artistName'].lower()\n",
    "        artist = row['standardized_artists']\n",
    "        percentage_listened = row['percentage_listened']\n",
    "        \n",
    "        # Skip if percentage_listened is NaN\n",
    "        if pd.isna(percentage_listened):\n",
    "            continue\n",
    "        \n",
    "        # Add check for empty or suspicious artist names\n",
    "        if not artist or len(artist.strip()) <= 1:\n",
    "            continue\n",
    "        \n",
    "        if artist_name == artist:\n",
    "            # If the artist is the main artist, assign the full weight\n",
    "            if artist not in weighted_times:\n",
    "                weighted_times[artist] = 0\n",
    "            weighted_times[artist] += percentage_listened\n",
    "        else:\n",
    "            # Distribute the weight evenly\n",
    "            equal_weight = percentage_listened / 2\n",
    "            if artist_name not in weighted_times:\n",
    "                weighted_times[artist_name] = 0\n",
    "            weighted_times[artist_name] += equal_weight\n",
    "            \n",
    "            if artist not in weighted_times:\n",
    "                weighted_times[artist] = 0\n",
    "            weighted_times[artist] += equal_weight\n",
    "    \n",
    "    # Convert to a DataFrame for easy sorting and selection\n",
    "    weighted_times_df = pd.DataFrame.from_dict(weighted_times, orient='index', columns=['weighted_time'])\n",
    "    top_artists_weighted_time = weighted_times_df.sort_values(by='weighted_time', ascending=False).head(top_n)\n",
    "    \n",
    "    print(\"Top artists by weighted listens calculated\")\n",
    "    return top_artists_weighted_time\n",
    "\n",
    "\n",
    "def top_artists_by_weighted_time(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Calculate top listened-to artists by weighted listening time.\n",
    "\n",
    "    This function expands the artists involved in each track, calculates the weighted\n",
    "    listening time for each artist, and returns the top N artists based on their\n",
    "    weighted listening time.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the listening data with 'artistName',\n",
    "                           'artists_involved', 'standardized_artists', and 'percentage_listened' columns.\n",
    "    top_n (int, optional): Number of top artists to return. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing the top N artists and their weighted listening times.\n",
    "    \"\"\"\n",
    "    print(f\"Calculating top {top_n} artists by weighted listening time\")\n",
    "    df_expanded = df.explode('standardized_artists')\n",
    "    \n",
    "    weighted_times = {}\n",
    "\n",
    "    for index, row in df_expanded.iterrows():\n",
    "        artist_name = row['artistName'].lower()\n",
    "        artist = row['standardized_artists']\n",
    "        percentage_listened = row['percentage_listened']\n",
    "        duration_ms = row['duration_ms']\n",
    "        \n",
    "        # Skip if percentage_listened or duration_ms is NaN\n",
    "        if pd.isna(percentage_listened) or pd.isna(duration_ms):\n",
    "            continue\n",
    "        \n",
    "        # Calculate actual listened time\n",
    "        listened_time = (percentage_listened / 100) * duration_ms\n",
    "        \n",
    "        if artist_name == artist:\n",
    "            # If the artist is the main artist, assign the full listened time\n",
    "            if artist not in weighted_times:\n",
    "                weighted_times[artist] = 0\n",
    "            weighted_times[artist] += listened_time\n",
    "        else:\n",
    "            # Distribute the listened time evenly\n",
    "            equal_weight = listened_time / 2\n",
    "            if artist_name not in weighted_times:\n",
    "                weighted_times[artist_name] = 0\n",
    "            weighted_times[artist_name] += equal_weight\n",
    "            \n",
    "            if artist not in weighted_times:\n",
    "                weighted_times[artist] = 0\n",
    "            weighted_times[artist] += equal_weight\n",
    "    \n",
    "    # Convert to a DataFrame for easy sorting and selection\n",
    "    weighted_times_df = pd.DataFrame.from_dict(weighted_times, orient='index', columns=['weighted_time'])\n",
    "    top_artists_weighted_time = weighted_times_df.sort_values(by='weighted_time', ascending=False).head(top_n)\n",
    "    \n",
    "    print(\"Top artists by weighted listening time calculated\")\n",
    "    return top_artists_weighted_time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Music Taste Analysis</h3>\n",
    "\n",
    "<strong>Music Taste per Month:</strong> Function to analyze top music tastes per month.\n",
    "\n",
    "<strong>Common Listening Days and Times:</strong> Function to determine the most common listening days and times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def music_taste_per_month(df):\n",
    "    \"\"\"\n",
    "    Analyze top 5 music tastes per month.\n",
    "\n",
    "    This function converts the 'endTime' column to datetime, ensures 'artists_involved' is processed correctly,\n",
    "    expands the DataFrame to have one row per artist involved in each track, groups by month and artist,\n",
    "    sums the listening time ('msPlayed') for each artist, and identifies the top 5 artists per month.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the listening data with 'endTime', 'artistName', 'artists_involved', and 'msPlayed' columns.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary with months as keys and lists of the top 5 artists for each month.\n",
    "    \"\"\"\n",
    "    print(\"Analyzing music taste per month\")\n",
    "    df['endTime'] = pd.to_datetime(df['endTime'])\n",
    "    df['artists_involved'] = df['artists_involved'].replace({pd.NA: '[]'})\n",
    "    df['artists_involved'] = df['artists_involved'].apply(ast.literal_eval)\n",
    "    \n",
    "    # Add the artist from 'artistName' to 'artists_involved' if they are not already in the list\n",
    "    def add_main_artist(row):\n",
    "        if row['artistName'] not in row['artists_involved']:\n",
    "            row['artists_involved'].append(row['artistName'])\n",
    "        return row\n",
    "    \n",
    "    df = df.apply(add_main_artist, axis=1)\n",
    "    df_expanded = df.explode('artists_involved')\n",
    "    \n",
    "    # Group by month and artist, and sum the msPlayed\n",
    "    music_taste = df_expanded.groupby([df_expanded['endTime'].dt.month, 'artists_involved'])['msPlayed'].sum().unstack().fillna(0)\n",
    "    \n",
    "    # Get the top 5 artists per month\n",
    "    top_artists_per_month = {}\n",
    "    for month in music_taste.index:\n",
    "        top_artists = music_taste.loc[month].nlargest(5)\n",
    "        top_artists_per_month[month] = top_artists.index.tolist()\n",
    "    \n",
    "    print(\"Music taste per month analyzed\")\n",
    "    return top_artists_per_month\n",
    "\n",
    "def common_listening_days(df):\n",
    "    \"\"\"\n",
    "    Determine the most common listening days and times.\n",
    "\n",
    "    This function converts the 'endTime' column to datetime, calculates the frequency\n",
    "    of each day of the week in the listening data, and returns the counts of the most\n",
    "    common listening days.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the listening data with 'endTime' column.\n",
    "\n",
    "    Returns:\n",
    "    pandas.Series: Series containing the counts of the most common listening days.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Determining the most common listening days and times\")\n",
    "    df['endTime'] = pd.to_datetime(df['endTime'])\n",
    "    common_days = df['endTime'].dt.day_name().value_counts()\n",
    "    print(\"Most common listening days determined\")\n",
    "    return common_days\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><h1>Main Functions</h1></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Process and Track Songs:</strong> Function to process raw listening data, add songs to the unique_songs file, and save the new CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def process_and_track_songs(base_path='../wrapped_files/', unique_songs_file='unique_songs.csv'):\n",
    "    \"\"\"\n",
    "    Process the raw listening data, track unique songs, and save to CSV.\n",
    "\n",
    "    This function processes the raw listening data by reading and combining multiple chunks,\n",
    "    adds all unique songs into the unique_songs file, and saves the processed data to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    base_path (str, optional): The base path to the directory containing the raw listening data files. \n",
    "                               Defaults to '../wrapped_files/'.\n",
    "    unique_songs_file (str, optional): The file path to the CSV file where unique songs are stored. \n",
    "                                       Defaults to 'unique_songs.csv'.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    def on_process_button_click(b):\n",
    "        user_id = user_id_input.value\n",
    "        num_chunks = num_chunks_input.value\n",
    "\n",
    "        try:\n",
    "            df = read_and_process_data(user_id, num_chunks, base_path)\n",
    "            export_to_csv(df, user_id)\n",
    "            track_unique_songs(df, unique_songs_file)\n",
    "\n",
    "            print(\"Data processing complete!\")\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "\n",
    "    # Create text input widgets for user ID and number of chunks\n",
    "    user_id_input = widgets.Text(description=\"User ID:\")\n",
    "    num_chunks_input = widgets.IntText(description=\"Num Chunks:\")\n",
    "\n",
    "    # Create a button widget\n",
    "    process_button = widgets.Button(description=\"Process and Export Data\")\n",
    "\n",
    "    # Link the button to the nested function\n",
    "    process_button.on_click(on_process_button_click)\n",
    "\n",
    "    # Display the input fields and button\n",
    "    display(user_id_input, num_chunks_input, process_button)\n",
    "\n",
    "# Call the function to display the widgets and set up the processing\n",
    "process_and_track_songs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def update_unique_songs_data(unique_songs_file='unique_songs.csv'):\n",
    "    \"\"\"\n",
    "    Update the unique songs table with Spotify info.\n",
    "\n",
    "    This function updates the unique songs table with information from Spotify.\n",
    "\n",
    "    Parameters:\n",
    "    unique_songs_file (str, optional): The file path to the CSV file where unique songs are stored. \n",
    "                                       Defaults to 'unique_songs.csv'.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    def on_update_button_click(b):\n",
    "        update_unique_songs(unique_songs_file)\n",
    "        print(\"Unique songs table updated with Spotify info.\")\n",
    "\n",
    "    # Create a button widget for updating unique songs\n",
    "    update_button = widgets.Button(description=\"Update Unique Songs\")\n",
    "\n",
    "    # Link the button to the nested function\n",
    "    update_button.on_click(on_update_button_click)\n",
    "\n",
    "    # Display the button\n",
    "    display(update_button)\n",
    "\n",
    "# Call the function to set up the button for updating unique songs\n",
    "update_unique_songs_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the unique songs database\n",
    "unique_songs_file = 'unique_songs.csv'\n",
    "unique_songs = pd.read_csv(unique_songs_file)\n",
    "\n",
    "# Sort the database by artistName\n",
    "sorted_unique_songs = unique_songs.sort_values(by='artistName')\n",
    "\n",
    "# Save the sorted database to a new CSV file\n",
    "sorted_unique_songs_file = 'sorted_unique_songs.csv'\n",
    "sorted_unique_songs.to_csv(sorted_unique_songs_file, index=False)\n",
    "\n",
    "print(f\"Sorted unique songs database saved to {sorted_unique_songs_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "def process_filled_listening_data(unique_songs_file='unique_songs.csv'):\n",
    "    \"\"\"\n",
    "    Load unique songs data, get user ID, read processed listening data,\n",
    "    fill in song info, calculate percentage listened, remove empty genre column,\n",
    "    and export to CSV.\n",
    "\n",
    "    This function loads unique songs data, gets the user ID, reads the processed listening data,\n",
    "    fills in song info from the unique songs database, calculates the percentage listened for each track,\n",
    "    checks and removes the empty 'genre' column, and exports the filled data to a new CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    unique_songs_file (str, optional): The file path to the CSV file where unique songs are stored. \n",
    "                                       Defaults to 'unique_songs.csv'.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    def on_process_button_click(b):\n",
    "        unique_songs = pd.read_csv(unique_songs_file)\n",
    "\n",
    "        user_id = user_id_input.value\n",
    "        try:\n",
    "            listening_data = read_processed_data(user_id)\n",
    "            \n",
    "            filled_listening_data = fill_song_info(listening_data, unique_songs)\n",
    "\n",
    "            # Check if 'duration_ms' column is present\n",
    "            if 'duration_ms' not in filled_listening_data.columns:\n",
    "                print(\"Warning: 'duration_ms' column is missing in filled_listening_data.\")\n",
    "                return\n",
    "            \n",
    "            # Calculate percentage listened\n",
    "            filled_listening_data['percentage_listened'] = (filled_listening_data['msPlayed'] / filled_listening_data['duration_ms']) * 100\n",
    "            \n",
    "            # Check and remove the empty 'genre' column if it exists and is empty\n",
    "            if 'genre' in filled_listening_data.columns and filled_listening_data['genre'].isnull().all():\n",
    "                filled_listening_data = filled_listening_data.drop(columns=['genre'])\n",
    "            \n",
    "            export_filled_data(filled_listening_data, user_id)\n",
    "\n",
    "            print(\"Data processing complete!\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Processed data file not found for user ID: {user_id}\")\n",
    "\n",
    "    # Create text input widgets for user ID\n",
    "    user_id_input = widgets.Text(description=\"User ID:\")\n",
    "\n",
    "    # Create a button widget\n",
    "    process_button = widgets.Button(description=\"Process Filled Listening Data\")\n",
    "\n",
    "    # Link the button to the nested function\n",
    "    process_button.on_click(on_process_button_click)\n",
    "\n",
    "    # Display the input fields and button\n",
    "    display(user_id_input, process_button)\n",
    "\n",
    "# Call the function to display the input fields and button for processing filled listening data\n",
    "process_filled_listening_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def main():\n",
    "    # Set option to display all rows\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    \n",
    "    # Load unique songs data\n",
    "    unique_songs_file = 'unique_songs.csv'\n",
    "    unique_songs = pd.read_csv(unique_songs_file)\n",
    "    \n",
    "    # Set the file path for the user's listening data\n",
    "    file_path = 'ezra_listening_data.csv'\n",
    "    try:\n",
    "        listening_data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Ensure that 'percentage_listened' column exists\n",
    "        if 'percentage_listened' not in listening_data.columns:\n",
    "            listening_data['percentage_listened'] = 100  # Assuming 100% if not specified\n",
    "\n",
    "        # Expand and standardize artists involved\n",
    "        listening_data = expand_artists_involved(listening_data)\n",
    "        \n",
    "        # Test top artists by weighted listening time\n",
    "        print(\"Top Artists by Weighted Listening Time:\")\n",
    "        top_artists_weighted_time = top_artists_by_weighted_time(listening_data, top_n=100)\n",
    "        print(top_artists_weighted_time)\n",
    "\n",
    "        # Test top artists by time\n",
    "        print(\"\\n\\nTop Artists by Time:\")\n",
    "        top_artists_time = top_artists_by_time(listening_data, top_n=100)\n",
    "        print(top_artists_time)\n",
    "\n",
    "        # Test top artists by count\n",
    "        print(\"\\n\\nTop Artists by Count:\")\n",
    "        top_artists_count = top_artists_by_count(listening_data, top_n=100)\n",
    "        print(top_artists_count)\n",
    "        \n",
    "        print(\"Data processing complete!\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Processed data file not found: {file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from io import BytesIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import noise\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Ensure all necessary packages are installed\n",
    "# If not, you can install them using pip\n",
    "# pip install pandas requests matplotlib Pillow noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Color Generation Functions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a random color\n",
    "def generate_random_color():\n",
    "    color = (random.randint(100, 255), random.randint(100, 255), random.randint(100, 255))\n",
    "    print(f\"Generated random color: {color}\")\n",
    "    return color\n",
    "\n",
    "# Function to generate a color close to a given color\n",
    "def generate_similar_color(color, variance=50):\n",
    "    r = min(max(color[0] + random.randint(-variance, variance), 0), 255)\n",
    "    g = min(max(color[1] + random.randint(-variance, variance), 0), 255)\n",
    "    b = min(max(color[2] + random.randint(-variance, variance), 0), 255)\n",
    "    similar_color = (r, g, b)\n",
    "    print(f\"Generated color similar to {color}: {similar_color}\")\n",
    "    return similar_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Abstract Background Generation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate an abstract background with dynamic colors\n",
    "def generate_abstract_background(width=1080, height=1920):\n",
    "    print(f\"Generating abstract background of size {width}x{height}\")\n",
    "    start_color = generate_random_color()\n",
    "    end_color = generate_similar_color(start_color)\n",
    "    \n",
    "    # Create a gradient based on the generated colors\n",
    "    gradient = np.linspace(start_color, end_color, width).astype(int)\n",
    "    gradient_cmap = plt.cm.colors.ListedColormap(gradient / 255.0)\n",
    "\n",
    "    x = np.linspace(-5, 5, width)\n",
    "    y = np.linspace(-5, 5, height)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    Z = np.sin(X**2 + Y**2) * np.cos(Y**2 - X**2)\n",
    "    \n",
    "    plt.figure(figsize=(width / 100, height / 100), dpi=100)\n",
    "    plt.imshow(Z, cmap=gradient_cmap, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('abstract_background.png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    background = Image.open('abstract_background.png')\n",
    "    background = background.resize((width, height))\n",
    "    print(\"Abstract background generated and saved as 'abstract_background.png'\")\n",
    "    return background\n",
    "\n",
    "# Function to generate Perlin noise\n",
    "def generate_perlin_noise(width, height, scale=100, seed=random.randint(0,500)):\n",
    "    print(f\"Generating Perlin noise of size {width}x{height} with scale {scale} and seed {seed}\")\n",
    "    shape = (width, height)\n",
    "    world = np.zeros(shape)\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            world[i][j] = noise.pnoise2(i / scale, j / scale, octaves=6, persistence=0.5, lacunarity=2.0, repeatx=1024, repeaty=1024, base=seed)\n",
    "    \n",
    "    norm_world = (world - np.min(world)) / (np.max(world) - np.min(world))\n",
    "    print(\"Perlin noise generated\")\n",
    "    return norm_world\n",
    "\n",
    "# Function to generate an abstract background with Perlin noise\n",
    "def generate_abstract_background_with_noise(width=1080, height=1920):\n",
    "    print(f\"Generating abstract background with Perlin noise of size {width}x{height}\")\n",
    "    noise_pattern = generate_perlin_noise(width, height)\n",
    "    \n",
    "    start_color = generate_random_color()\n",
    "    end_color = generate_similar_color(start_color)\n",
    "    gradient = np.linspace(start_color, end_color, width).astype(int)\n",
    "    gradient_cmap = plt.cm.colors.ListedColormap(gradient / 255.0)\n",
    "\n",
    "    plt.figure(figsize=(width / 100, height / 100), dpi=100)\n",
    "    plt.imshow(noise_pattern, cmap=gradient_cmap, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('abstract_background_with_noise.png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    background = Image.open('abstract_background_with_noise.png')\n",
    "    background = background.resize((width, height))\n",
    "    print(\"Abstract background with Perlin noise generated and saved as 'abstract_background_with_noise.png'\")\n",
    "    return background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Text Drawing Function**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw wrapped text\n",
    "def draw_wrapped_text(draw, text, position, font, max_width, fill):\n",
    "    print(f\"Drawing wrapped text: {text}\")\n",
    "    lines = []\n",
    "    words = text.split()\n",
    "    while words:\n",
    "        line = ''\n",
    "        while words and font.getbbox(line + words[0])[2] <= max_width:\n",
    "            line += (words.pop(0) + ' ')\n",
    "        lines.append(line)\n",
    "    y_offset = position[1]\n",
    "    for line in lines:\n",
    "        draw.text((position[0], y_offset), line, font=font, fill=fill)\n",
    "        y_offset += font.getbbox(line)[3]  # Use getbbox for line height\n",
    "    print(f\"Wrapped text drawn at position {position}\")\n",
    "    return y_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Album Art Download!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch and download all popular album art for each artist\n",
    "def download_all_album_art(df, top_artists):\n",
    "    print(\"Downloading album art for top artists\")\n",
    "    album_art = {}\n",
    "    for artist in top_artists:\n",
    "        artist_data = df[df['artistName'] == artist]\n",
    "        if artist_data.empty:\n",
    "            continue\n",
    "        \n",
    "        art_urls = artist_data['album_artwork'].value_counts().index.tolist()\n",
    "        downloaded = False\n",
    "        for art_url in art_urls:\n",
    "            try:\n",
    "                response = requests.get(art_url)\n",
    "                img = Image.open(BytesIO(response.content))\n",
    "                \n",
    "                img_path = os.path.join(\"albums\", f'{artist}_album_art.jpg')\n",
    "                img.save(img_path)\n",
    "                \n",
    "                album_art[artist] = img_path\n",
    "                downloaded = True\n",
    "                print(f\"Downloaded album art for {artist}: {img_path}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {art_url} for {artist}: {e}\")\n",
    "                continue\n",
    "        if not downloaded:\n",
    "            print(f\"Could not download album art for {artist}\")\n",
    "    return album_art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Function to create and save layout images\n",
    "def create_layout_image(title, top_artists, album_art, file_name, user_id, background):\n",
    "    print(f\"Creating layout image: {file_name}\")\n",
    "    width, height = background.size\n",
    "    image = background.copy()\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Define fonts\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 40)\n",
    "        title_font = ImageFont.truetype(\"arial.ttf\", 60)\n",
    "        user_id_font = ImageFont.truetype(\"arial.ttf\", 30)\n",
    "    except IOError:\n",
    "        # In case the fonts are not available on the system\n",
    "        font = ImageFont.load_default()\n",
    "        title_font = ImageFont.load_default()\n",
    "        user_id_font = ImageFont.load_default()\n",
    "    \n",
    "    # Draw title and user ID\n",
    "    draw.text((width / 2, 50), title, font=title_font, fill=\"white\", anchor=\"mm\")\n",
    "    draw.text((width / 2, 150), f\"User: {user_id}\", font=user_id_font, fill=\"white\", anchor=\"mm\")\n",
    "\n",
    "    y_offset = 250\n",
    "    x_offset = 50\n",
    "\n",
    "    for rank, (artist, value) in enumerate(top_artists.items(), start=1):\n",
    "        if artist not in album_art:\n",
    "            continue\n",
    "        \n",
    "        art = Image.open(album_art[artist]).resize((100, 100))\n",
    "        image.paste(art, (x_offset, y_offset))\n",
    "        \n",
    "        text = f\"{rank}. {artist}: {value}\"\n",
    "        draw.text((x_offset + 120, y_offset + 30), text, font=font, fill=\"white\")\n",
    "        y_offset += 120\n",
    "\n",
    "    image.save(file_name)\n",
    "    print(f\"Layout image saved as {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_filled_listening_data('ezra_listening_data_with_percentage.csv')\n",
    "\n",
    "# Calculate total listening time per user\n",
    "total_time = total_listening_time_per_user(df)\n",
    "print(\"Total listening time per user (in hours):\")\n",
    "print(total_time, \"\\n\")\n",
    "\n",
    "# Identify the biggest listening date\n",
    "biggest_date = biggest_listening_date(df)\n",
    "print(\"Biggest listening date:\")\n",
    "print(biggest_date, \"\\n\")\n",
    "\n",
    "# Analyze top 5 music tastes per month\n",
    "taste_per_month = music_taste_per_month(df)\n",
    "print(\"Top 5 artists per month:\")\n",
    "for month, artists in taste_per_month.items():\n",
    "    print(f\"Month {month}: {artists}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Determine the most common listening days and times\n",
    "common_days = common_listening_days_and_times(df)\n",
    "print(\"Most common listening days:\")\n",
    "print(common_days, \"\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Album Analysis Functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Generate Instagram story-sized abstract background with Perlin noise and custom colormap\n",
    "    background = generate_abstract_background_with_noise(1080, 1920)\n",
    "\n",
    "    # Load unique songs data\n",
    "    unique_songs_file = 'unique_songs.csv'\n",
    "    unique_songs = pd.read_csv(unique_songs_file)\n",
    "\n",
    "    # Get user ID and construct the file path\n",
    "    user_name = get_user_id()\n",
    "    file_path = f'{user_name}_listening_data.csv'\n",
    "\n",
    "    try:\n",
    "        # Read the filled listening data\n",
    "        filled_listening_data = read_filled_listening_data(file_path)\n",
    "        \n",
    "        # Calculate percentage listened for each track\n",
    "        filled_listening_data = calculate_percentage_listened(filled_listening_data)\n",
    "        \n",
    "        # Calculate top listened-to artists\n",
    "        top_artists_count = top_artists_by_count(filled_listening_data).head(5)\n",
    "        top_artists_time = top_artists_by_time(filled_listening_data).head(5)\n",
    "        top_artists_weighted_time = top_artists_by_weighted_time(filled_listening_data).head(5)\n",
    "        \n",
    "        # Combine all top artists to ensure all album art is downloaded\n",
    "        all_top_artists = top_artists_count.index.union(top_artists_time.index).union(top_artists_weighted_time.index)\n",
    "        \n",
    "        # Download the most common album art for each artist\n",
    "        album_art = download_all_album_art(filled_listening_data, all_top_artists)\n",
    "        \n",
    "        # Create layout images with user ID in the file name and abstract background\n",
    "        create_layout_image(\"Top Artists by Count\", top_artists_count, album_art, f\"{user_name}_spotify_wrapped_top_artists_count.png\", user_name, background)\n",
    "        create_layout_image(\"Top Artists by Listening Time (minutes)\", {k: v / 60 for k, v in top_artists_time.items()}, album_art, f\"{user_name}_spotify_wrapped_top_artists_time.png\", user_name, background)\n",
    "        create_layout_image(\"Top Artists by Weighted Listening Time\", top_artists_weighted_time, album_art, f\"{user_name}_spotify_wrapped_top_artists_weighted_time.png\", user_name, background)\n",
    "        \n",
    "        print(\"Data processing and layout creation complete!\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
